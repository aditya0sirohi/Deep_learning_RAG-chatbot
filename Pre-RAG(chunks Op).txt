Total chunks: 1172

--- Document Chunk 1 ---
The Little Book
of
Deep Learning
FranÃ§ois Fleuret
Chunk length: 49

--- Document Chunk 2 ---
FranÃ§ois Fleuret is a professor of computer sci-
ence at the University of Geneva, Switzerland.
The cover illustration is a schematic of the
Neocognitron by Fukushima [1980], a key an-
cestor of deep neural networks.
Chunk length: 216

--- Document Chunk 3 ---
cestor of deep neural networks.
This ebook is formatted to fit on a phone screen.
Chunk length: 81

--- Document Chunk 4 ---
Contents
Contents 5
List of figures 7
Foreword 8
I Foundations 10
1 Machine Learning 11
1.1 Learning from data . . . . . . . 12
1.2 Basis function regression . . . . 14
1.3 Under and overfitting . . . . . . 16
1.4 Categories of models . . . . . . 18
Chunk length: 249

--- Document Chunk 5 ---
1.4 Categories of models . . . . . . 18
2 Efficient Computation 20
2.1 GPUs, TPUs, and batches . . . . 21
2.2 Tensors . . . . . . . . . . . . . . 23
3 Training 25
3.1 Losses . . . . . . . . . . . . . . 26
3.2 Autoregressive models . . . . . 30
Chunk length: 243

--- Document Chunk 6 ---
3.2 Autoregressive models . . . . . 30
3.3 Gradient descent . . . . . . . . 35
3
Chunk length: 80

--- Document Chunk 7 ---
3.4 Backpropagation . . . . . . . . 40
3.5 The value of depth . . . . . . . 45
3.6 Training protocols . . . . . . . 48
3.7 The benefits of scale . . . . . . 52
II Deep Models 57
4 Model Components 58
4.1 The notion of layer . . . . . . . 59
Chunk length: 240

--- Document Chunk 8 ---
4.1 The notion of layer . . . . . . . 59
4.2 Linear layers . . . . . . . . . . . 61
4.3 Activation functions . . . . . . 71
4.4 Pooling . . . . . . . . . . . . . . 74
4.5 Dropout . . . . . . . . . . . . . 77
4.6 Normalizing layers . . . . . . . 80
Chunk length: 247

--- Document Chunk 9 ---
4.6 Normalizing layers . . . . . . . 80
4.7 Skip connections . . . . . . . . 84
4.8 Attention layers . . . . . . . . . 87
4.9 Token embedding . . . . . . . . 95
4.10 Positional encoding . . . . . . . 96
5 Architectures 98
Chunk length: 221

--- Document Chunk 10 ---
5 Architectures 98
5.1 Multi-Layer Perceptrons . . . . 99
5.2 Convolutional networks . . . . 101
5.3 Attention models . . . . . . . . 108
III Applications 116
6 Prediction 117
6.1 Image denoising . . . . . . . . . 118
Chunk length: 217

--- Document Chunk 11 ---
6.1 Image denoising . . . . . . . . . 118
6.2 Image classification . . . . . . . 120
6.3 Object detection . . . . . . . . . 121
4
Chunk length: 129

--- Document Chunk 12 ---
6.4 Semantic segmentation . . . . . 126
6.5 Speech recognition . . . . . . . 129
6.6 Text-image representations . . . 131
6.7 Reinforcement learning . . . . . 134
7 Synthesis 138
7.1 Text generation . . . . . . . . . 139
Chunk length: 220

--- Document Chunk 13 ---
7.1 Text generation . . . . . . . . . 139
7.2 Image generation . . . . . . . . 142
8 The Compute Schism 146
8.1 Prompt Engineering . . . . . . 147
8.2 Quantization . . . . . . . . . . . 150
8.3 Adapters . . . . . . . . . . . . . 153
Chunk length: 232

--- Document Chunk 14 ---
8.3 Adapters . . . . . . . . . . . . . 153
8.4 Model merging . . . . . . . . . 156
The missing bits 158
Bibliography 164
Index 176
5
Chunk length: 132

--- Document Chunk 15 ---
List of Figures
1.1 Kernel regression . . . . . . . . . . 14
1.2 Overfitting of kernel regression . . 16
3.1 Causal autoregressive model . . . . 32
3.2 Gradient descent . . . . . . . . . . . 36
3.3 Backpropagation . . . . . . . . . . . 40
Chunk length: 238

--- Document Chunk 16 ---
3.3 Backpropagation . . . . . . . . . . . 40
3.4 Feature warping . . . . . . . . . . . 46
3.5 Training and validation losses . . . 49
3.6 Scaling laws . . . . . . . . . . . . . 53
3.7 Model training costs . . . . . . . . . 55
Chunk length: 225

--- Document Chunk 17 ---
3.7 Model training costs . . . . . . . . . 55
4.1 1D convolution . . . . . . . . . . . . 63
4.2 2D convolution . . . . . . . . . . . . 64
4.3 Stride, padding, and dilation . . . . 65
4.4 Receptive field . . . . . . . . . . . . 68
Chunk length: 229

--- Document Chunk 18 ---
4.4 Receptive field . . . . . . . . . . . . 68
4.5 Activation functions . . . . . . . . . 72
4.6 Max pooling . . . . . . . . . . . . . 75
4.7 Dropout . . . . . . . . . . . . . . . . 78
4.8 Dropout 2D . . . . . . . . . . . . . . 79
Chunk length: 230

--- Document Chunk 19 ---
4.8 Dropout 2D . . . . . . . . . . . . . . 79
4.9 Batch normalization . . . . . . . . . 81
4.10 Skip connections . . . . . . . . . . . 85
6
Chunk length: 139

--- Document Chunk 20 ---
4.11 Attention operator interpretation . 88
4.12 Complete attention operator . . . . 90
4.13 Multi-Head Attention layer . . . . . 92
5.1 Multi-Layer Perceptron . . . . . . . 99
5.2 LeNet-like convolutional model . . 102
Chunk length: 219

--- Document Chunk 21 ---
5.2 LeNet-like convolutional model . . 102
5.3 Residual block . . . . . . . . . . . . 103
5.4 Downscaling residual block . . . . . 104
5.5 ResNet-50 . . . . . . . . . . . . . . . 105
5.6 Transformer components . . . . . . 109
Chunk length: 225

--- Document Chunk 22 ---
5.6 Transformer components . . . . . . 109
5.7 Transformer . . . . . . . . . . . . . 110
5.8 GPT model . . . . . . . . . . . . . . 112
5.9 ViT model . . . . . . . . . . . . . . 114
6.1 Convolutional object detector . . . 122
Chunk length: 224

--- Document Chunk 23 ---
6.1 Convolutional object detector . . . 122
6.2 Object detection with SSD . . . . . 123
6.3 Semantic segmentation with PSP . . 127
6.4 CLIP zero-shot prediction . . . . . . 133
6.5 DQN state value evolution . . . . . 136
Chunk length: 220

--- Document Chunk 24 ---
6.5 DQN state value evolution . . . . . 136
7.1 Few-shot prediction with a GPT . . 140
7.2 Denoising diffusion . . . . . . . . . 143
8.1 Chain-of-thought . . . . . . . . . . 148
8.2 Quantization . . . . . . . . . . . . . 151
7
Chunk length: 226

--- Document Chunk 25 ---
Foreword
The current period of progress in artificial in-
telligence was triggered when Krizhevsky et al.
Chunk length: 105

--- Document Chunk 26 ---
[2012] demonstrated that anar ar ar ar ar ar ar ar ar ar arti ti ti ti ti ti ti ti ti ti tifi fi fi fi fi fi fi fi fi fi ficial cial cial cial cial cial cial cial cial cial cialneu neu neu neu neu neu neu neu neu neu neural ral ral ral ral ral ral
Chunk length: 247

--- Document Chunk 27 ---
neu neu neu neu neural ral ral ral ral ral ral ral ral ral ralnet net net net net net net net net net net-
Chunk length: 106

--- Document Chunk 28 ---
work work work work work work work work work work workdesigned twenty years earlier [LeCun et al.,
1989] could outperform complex state-of-the-
art image recognition methods by a huge mar-
gin, simply by being a hundred times larger and
Chunk length: 236

--- Document Chunk 29 ---
gin, simply by being a hundred times larger and
trained on a dataset similarly scaled up.
This breakthrough was made possible thanks to
Chunk length: 135

--- Document Chunk 30 ---
Graph Graph Graph Graph Graph Graph Graph Graph Graph Graph Graphi i i i i i i i i i ical cal cal cal cal cal cal cal cal cal calPro Pro Pro Pro Pro Pro Pro Pro Pro Pro Process cess cess cess cess cess cess cess cess cess cessing ing ing ing ing ing
Chunk length: 249

--- Document Chunk 31 ---
cess cess cess cess cessing ing ing ing ing ing ing ing ing ing ingUnits Units Units Units Units Units Units Units Units Units Units(GPUs GPUs GPUs GPUs GPUs GPUs GPUs GPUs GPUs GPUs GPUs), highly par-
Chunk length: 201

--- Document Chunk 32 ---
allel consumer-grade computing devices devel-
oped for real-time image synthesis and repur-
posed for artificial neural networks.
Since then, under the umbrella term of â€œ deep deep deep deep deep deep deep deep deep deep deep
Chunk length: 225

--- Document Chunk 33 ---
learn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ing, â€ innovations in the structures of these
networks, the strategies to train them, and ded-
icated hardware have allowed for an exponen-
Chunk length: 244

--- Document Chunk 34 ---
icated hardware have allowed for an exponen-
tial increase in both their size and the quantity
of training data they take advantage of [Sevilla
8
Chunk length: 145

--- Document Chunk 35 ---
et al., 2022]. This has resulted in a wave of suc-
cessful applications across technical domains,
from computer vision and robotics to speech
processing, and since 2020 in the development
of Large Language Models with general proto-
Chunk length: 232

--- Document Chunk 36 ---
of Large Language Models with general proto-
reasoning capabilities [Chowdhery et al., 2022].
Although the bulk of deep learning is not difficult
to understand, it combines diverse components
such as linear algebra, calculus, probabilities, op-
Chunk length: 244

--- Document Chunk 37 ---
timization, signal processing, programming, al-
gorithmics, and high-performance computing,
making it complicated to learn.
Instead of trying to be exhaustive, this little book
is limited to the background necessary to under-
Chunk length: 225

--- Document Chunk 38 ---
is limited to the background necessary to under-
stand a few important models. This proved to
be a popular approach, resulting in more than
500,000 downloads of the PDF file in the 12
months following its announcement on Twitter.
Chunk length: 229

--- Document Chunk 39 ---
months following its announcement on Twitter.
If you did not get this book from its official URL
https://fleuret.org/public/lbdl.pdf
please do so, to allow the estimation of the num-
ber of readers.
FranÃ§ois Fleuret,
May 19, 2024
9
Chunk length: 231

--- Document Chunk 40 ---
Part I
Foundations
10
Chunk length: 21

--- Document Chunk 41 ---
Chapter 1
Machine Learning
Deep Deep Deep Deep Deep Deep Deep Deep Deep Deep Deeplearn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ing belongs historically to the larger
Chunk length: 224

--- Document Chunk 42 ---
field of statistical ma ma ma ma ma ma ma ma ma ma machine chine chine chine chine chine chine chine chine chine chinelearn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ing, as it funda-
Chunk length: 240

--- Document Chunk 43 ---
mentally concerns methods that are able to learn
representations from data. The techniques in-
Chunk length: 94

--- Document Chunk 44 ---
volved come originally fromar ar ar ar ar ar ar ar ar ar arti ti ti ti ti ti ti ti ti ti tifi fi fi fi fi fi fi fi fi fi ficial cial cial cial cial cial cial cial cial cial cialneu neu neu neu neu neu neu neu neu neu neural ral ral ral ral ral ral
Chunk length: 247

--- Document Chunk 45 ---
neu neu neu neu neural ral ral ral ral ral ral ral ral ral ralnet net net net net net net net net net net-
Chunk length: 106

--- Document Chunk 46 ---
works works works works works works works works works works works, and the â€œdeepâ€ qualifier highlights that
models are long compositions of mappings, now
known to achieve greater performance.
The modularity, versatility, and scalability of
Chunk length: 239

--- Document Chunk 47 ---
The modularity, versatility, and scalability of
deep models have resulted in a plethora of spe-
cific mathematical methods and software devel-
opment tools, establishing deep learning as a
distinct and vast technical field.
11
Chunk length: 226

--- Document Chunk 48 ---
1.1 Learning from data
The simplest use case for a model trained from
data is when a signalx is accessible, for instance,
the picture of a license plate, from which one
wants to predict a quantity y, such as the string
Chunk length: 218

--- Document Chunk 49 ---
wants to predict a quantity y, such as the string
of characters written on the plate.
In many real-world situations where x is a high-
dimensional signal captured in an uncontrolled
environment, it is too complicated to come up
Chunk length: 227

--- Document Chunk 50 ---
environment, it is too complicated to come up
with an analytical recipe that relates x and y.
What one can do is to collect a large train train train train train train train train train train training ing ing ing ing ing ing ing ing ing ing
Chunk length: 240

--- Document Chunk 51 ---
set set set set set set set set set set setð’Ÿ of pairs (xn,yn), and devise a para para para para para para para para para para paramet met met met met met met met met met met-
Chunk length: 174

--- Document Chunk 52 ---
ric ric ric ric ric ric ric ric ric ric ricmodel model model model model model model model model model modelf. This is a piece of computer code
Chunk length: 143

--- Document Chunk 53 ---
that incorporates train train train train train train train train train train trainable able able able able able able able able able ablepa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e eters ters ters
Chunk length: 247

--- Document Chunk 54 ---
ram ram rame e e e e e e e e e eters ters ters ters ters ters ters ters ters ters tersw that
Chunk length: 92

--- Document Chunk 55 ---
modulate its behavior, and such that, with the
proper values wâˆ—, it is a good predictor. â€œGoodâ€
here means that if an x is given to this piece
of code, the value Ë†y = f(x;wâˆ—) it computes is
a good estimate of the y that would have been
Chunk length: 235

--- Document Chunk 56 ---
a good estimate of the y that would have been
associated with x in the training set had it been
there.
This notion of goodness is usually formalized
with a loss loss loss loss loss loss loss loss loss loss lossâ„’(w) which is small when f(Â·;w)
Chunk length: 241

--- Document Chunk 57 ---
is good on ð’Ÿ. Then, train train train train train train train train train train training ing ing ing ing ing ing ing ing ing ingthe model consists
of computing a value wâˆ— that minimizes â„’(wâˆ—).
12
Chunk length: 195

--- Document Chunk 58 ---
Most of the content of this book is about the defi-
nition of f, which, in realistic scenarios, is a com-
plex combination of pre-defined sub-modules.
The trainable parameters that compose w are of-
Chunk length: 198

--- Document Chunk 59 ---
The trainable parameters that compose w are of-
ten called weights weights weights weights weights weights weights weights weights weights weights, by analogy with the synaptic
weights of biological neural networks. In addi-
Chunk length: 224

--- Document Chunk 60 ---
weights of biological neural networks. In addi-
tion to these parameters, models usually depend
Chunk length: 95

--- Document Chunk 61 ---
on hy hy hy hy hy hy hy hy hy hy hyper per per per per per per per per per per-pa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e eters ters ters ters ters ters ters ters ters ters ters, which are set
Chunk length: 244

--- Document Chunk 62 ---
ters ters ters ters ters ters ters, which are set according
Chunk length: 59

--- Document Chunk 63 ---
to domain prior knowledge, best practices, or re-
source constraints. They may also be optimized
in some way, but with techniques different from
those used to optimize w.
13
Chunk length: 173

--- Document Chunk 64 ---
1.2 Basis function regression
We can illustrate the training of a model in a sim-
ple case where xn and yn are two real numbers,
Chunk length: 128

--- Document Chunk 65 ---
the loss is the mean mean mean mean mean mean mean mean mean mean meansquared squared squared squared squared squared squared squared squared squared squareder er er er er er er er er er error ror ror ror ror ror ror ror ror ror ror:
â„’(w) = 1
N
NX
Chunk length: 247

--- Document Chunk 66 ---
â„’(w) = 1
N
NX
n=1
(yn âˆ’f(xn;w))2, (1.1)
and f(Â·;w) is a linear combination of a pre-
defined basis of functions f1,...,f K, with w =
(w1,...,w K):
f(x;w) =
KX
k=1
wkfk(x).
Since f(xn;w) is linear with respect to the wks
Chunk length: 219

--- Document Chunk 67 ---
Since f(xn;w) is linear with respect to the wks
and â„’(w) is quadratic with respect to f(xn;w),
Figure 1.1: Given a basis of functions (blue curves)
and a training set (black dots), we can compute an
Chunk length: 198

--- Document Chunk 68 ---
optimal linear combination of the former (red curve)
to approximate the latter for the mean squared error.
14
Chunk length: 109

--- Document Chunk 69 ---
the loss â„’(w) is quadratic with respect to the
wks, and findingwâˆ— that minimizes it boils down
to solving a linear system. See Figure 1.1 for an
example with Gaussian kernels as fk.
15
Chunk length: 184

--- Document Chunk 70 ---
1.3 Under and overfitting
A key element is the interplay between the ca ca ca ca ca ca ca ca ca ca ca-
pac pac pac pac pac pac pac pac pac pac pacity ity ity ity ity ity ity ity ity ity ityof the model, that is its flexibility and
Chunk length: 230

--- Document Chunk 71 ---
ability to fit diverse data, and the amount and
quality of the training data. When the capacity
is insufficient, the model cannot fit the data, re-
sulting in a high error during training. This is
Chunk length: 196

--- Document Chunk 72 ---
sulting in a high error during training. This is
referred to as un un un un un un un un un un under der der der der der der der der der derfit fit fit fit fit fit fit fit fit fit fitting ting ting ting ting ting ting ting ting ting ting.
Chunk length: 237

--- Document Chunk 73 ---
On the contrary, when the amount of data is in-
sufficient, as illustrated in Figure 1.2, the model
will often learn characteristics specific to the
training examples, resulting in excellent perfor-
mance during training, at the cost of a worse
Chunk length: 244

--- Document Chunk 74 ---
mance during training, at the cost of a worse
Figure 1.2: If the amount of training data (black dots)
is small compared to the capacity of the model, the em-
pirical performance of the fitted model during training
Chunk length: 213

--- Document Chunk 75 ---
(red curve) reflects poorly its actual fit to the underly-
ing data structure (thin black curve), and consequently
its usefulness for prediction.
16
Chunk length: 148

--- Document Chunk 76 ---
fit to the global structure of the data, and poor
performance on new inputs. This phenomenon
Chunk length: 92

--- Document Chunk 77 ---
performance on new inputs. This phenomenon
is referred to as over over over over over over over over over over overfit fit fit fit fit fit fit fit fit fit fitting ting ting ting ting ting ting ting ting ting ting.
Chunk length: 213

--- Document Chunk 78 ---
So, a large part of the art of applied ma ma ma ma ma ma ma ma ma ma machine chine chine chine chine chine chine chine chine chine chine
Chunk length: 136

--- Document Chunk 79 ---
learn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ingis to design models that are not too
flexible yet still able to fit the data. This is done
Chunk length: 198

--- Document Chunk 80 ---
by crafting the right in in in in in in in in in in induc duc duc duc duc duc duc duc duc duc ductive tive tive tive tive tive tive tive tive tive tivebias bias bias bias bias bias bias bias bias bias biasin a model,
Chunk length: 216

--- Document Chunk 81 ---
which means that its structure corresponds to
the underlying structure of the data at hand.
Even though this classical perspective is relevant
for reasonably-sized deep models, things get con-
fusing with large ones that have a very large
Chunk length: 238

--- Document Chunk 82 ---
fusing with large ones that have a very large
number of trainable parameters and extreme ca-
pacity yet still perform well on prediction. We
will come back to this in Â§ 3.6 and Â§ 3.7.
17
Chunk length: 186

--- Document Chunk 83 ---
1.4 Categories of models
Chunk length: 24

--- Document Chunk 84 ---
We can organize the use of ma ma ma ma ma ma ma ma ma ma machine chine chine chine chine chine chine chine chine chine chinelearn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ing
Chunk length: 232

--- Document Chunk 85 ---
models into three broad categories:
â€¢ Re Re Re Re Re Re Re Re Re Re Regres gres gres gres gres gres gres gres gres gres gression sion sion sion sion sion sion sion sion sion sionconsists of predicting a
continuous-valued vector y âˆˆ RK, for instance,
Chunk length: 249

--- Document Chunk 86 ---
continuous-valued vector y âˆˆ RK, for instance,
a geometrical position of an object, given an
input signal X. This is a multi-dimensional
generalization of the setup we saw in Â§ 1.2. The
training set is composed of pairs of an input
Chunk length: 231

--- Document Chunk 87 ---
training set is composed of pairs of an input
signal and a ground ground ground ground ground ground ground ground ground ground ground-truth truth truth truth truth truth truth truth truth truth truth value.
Chunk length: 208

--- Document Chunk 88 ---
â€¢ Clas Clas Clas Clas Clas Clas Clas Clas Clas Clas Classi si si si si si si si si si sifi fi fi fi fi fi fi fi fi fi fica ca ca ca ca ca ca ca ca ca cation tion tion tion tion tion tion tion tion tion tionaims at predicting a value from
Chunk length: 237

--- Document Chunk 89 ---
a finite set{1,...,C }, for instance, the labelY of
an image X. As with regression, the training set
is composed of pairs of input signal, and ground-
truth quantity, here a label from that set. The
standard way of tackling this is to predict one
Chunk length: 246

--- Document Chunk 90 ---
standard way of tackling this is to predict one
score per potential class, such that the correct
class has the maximum score.
Chunk length: 125

--- Document Chunk 91 ---
â€¢ Den Den Den Den Den Den Den Den Den Den Density sity sity sity sity sity sity sity sity sity sitymod mod mod mod mod mod mod mod mod mod model el el el el el el el el el eling ing ing ing ing ing ing ing ing ing inghas as its objective to model
Chunk length: 246

--- Document Chunk 92 ---
the probability density function of the data ÂµX
itself, for instance, images. In that case, the train-
ing set is composed of values xn without associ-
ated quantities to predict, and the trained model
Chunk length: 201

--- Document Chunk 93 ---
ated quantities to predict, and the trained model
should allow for the evaluation of the probability
density function, or sampling from the distribu-
tion, or both.
18
Chunk length: 167

--- Document Chunk 94 ---
Both regression and classification are gener-
Chunk length: 45

--- Document Chunk 95 ---
ally referred to as su su su su su su su su su su super per per per per per per per per per pervised vised vised vised vised vised vised vised vised vised visedlearn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing
Chunk length: 248

--- Document Chunk 96 ---
learn learn learn learning ing ing ing ing ing ing ing ing ing ing, since the
Chunk length: 77

--- Document Chunk 97 ---
value to be predicted, which is required as a tar-
get during training, has to be provided, for in-
stance, by human experts. On the contrary, den-
Chunk length: 147

--- Document Chunk 98 ---
sity modeling is usually seen as un un un un un un un un un un unsu su su su su su su su su su super per per per per per per per per per pervised vised vised vised vised vised vised vised vised vised vised
Chunk length: 205

--- Document Chunk 99 ---
learn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ing, since it is sufficient to take existing
data without the need for producing an associ-
ated ground-truth.
Chunk length: 215

--- Document Chunk 100 ---
ated ground-truth.
These three categories are not disjoint; for in-
stance, classification can be cast as class-score
regression, or discrete sequence density model-
ing as iterated classification. Furthermore, they
Chunk length: 215

--- Document Chunk 101 ---
ing as iterated classification. Furthermore, they
do not cover all cases. One may want to predict
compounded quantities, or multiple classes, or
model a density conditional on a signal.
19
Chunk length: 188

--- Document Chunk 102 ---
Chapter 2
Efficient
Computation
From an implementation standpoint, deep learn-
ing is about executing heavy computations with
Chunk length: 125

--- Document Chunk 103 ---
large amounts of data
Chunk length: 21

--- Document Chunk 104 ---
. TheGraph Graph Graph Graph Graph Graph Graph Graph Graph Graph Graphi i i i i i i i i i ical cal cal cal cal cal cal cal cal cal calPro Pro Pro Pro Pro Pro Pro Pro Pro Pro Process cess cess cess cess cess cess cess cess cess cessing ing ing ing ing
Chunk length: 250

--- Document Chunk 105 ---
cess cess cess cess cess cessing ing ing ing ing ing ing ing ing ing ing
Chunk length: 72

--- Document Chunk 106 ---
Units Units Units Units Units Units Units Units Units Units Units(GPUs GPUs GPUs GPUs GPUs GPUs GPUs GPUs GPUs GPUs GPUs) have been instrumental in the suc-
cess of the field by allowing such computations
to be run on affordable hardware.
Chunk length: 238

--- Document Chunk 107 ---
to be run on affordable hardware.
The importance of their use, and the resulting
technical constraints on the computations that
can be done efficiently, force the research in the
field to constantly balance mathematical sound-
Chunk length: 226

--- Document Chunk 108 ---
field to constantly balance mathematical sound-
ness and implementability of novel methods.
20
Chunk length: 94

--- Document Chunk 109 ---
2.1 GPUs, TPUs, and batches
Graphical Processing Units were originally de-
signed for real-time image synthesis, which re-
quires highly parallel architectures that happen
to be well suited for deep models. As their usage
Chunk length: 221

--- Document Chunk 110 ---
to be well suited for deep models. As their usage
for AI has increased, GPUs have been equipped
Chunk length: 95

--- Document Chunk 111 ---
for AI has increased, GPUs have been equipped
with dedicated ten ten ten ten ten ten ten ten ten ten tensor sor sor sor sor sor sor sor sor sor sorcores cores cores cores cores cores cores cores cores cores cores, and deep-learning
Chunk length: 231

--- Document Chunk 112 ---
specialized chips such as Googleâ€™s Ten Ten Ten Ten Ten Ten Ten Ten Ten Ten Tensor sor sor sor sor sor sor sor sor sor sorPro Pro Pro Pro Pro Pro Pro Pro Pro Pro Pro-
Chunk length: 165

--- Document Chunk 113 ---
cess cess cess cess cess cess cess cess cess cess cessing ing ing ing ing ing ing ing ing ing ingUnits Units Units Units Units Units Units Units Units Units Units (TPUs TPUs TPUs TPUs TPUs TPUs TPUs TPUs TPUs TPUs TPUs) have been developed.
Chunk length: 240

--- Document Chunk 114 ---
A GPU possesses several thousand parallel units
and its own fast memory. The limiting factor
is usually not the number of computing units,
Chunk length: 138

--- Document Chunk 115 ---
but the read read read read read read read read read read read-write write write write write write write write write write writeop op op op op op op op op op oper er er er er er er er er er era a a a a a a a a a ations tions tions tions tions tions
Chunk length: 248

--- Document Chunk 116 ---
a a a a a a ations tions tions tions tions tions tions tions tions tions tionsto to to to to to to to to to tomem mem mem mem mem mem mem mem mem mem memory ory ory ory ory ory ory ory ory ory ory
Chunk length: 196

--- Document Chunk 117 ---
. The
Chunk length: 5

--- Document Chunk 118 ---
slowest link is between the CPU memory and
the GPU memory, and consequently one should
avoid copying data across devices. Moreover,
the structure of the GPU itself involves multiple
Chunk length: 181

--- Document Chunk 119 ---
the structure of the GPU itself involves multiple
levels of cache cache cache cache cache cache cache cache cache cache cachemem mem mem mem mem mem mem mem mem mem memory ory ory ory ory ory ory ory ory ory ory, which are smaller but
Chunk length: 234

--- Document Chunk 120 ---
faster, and computation should be organized to
avoid copies between these different caches.
This is achieved, in particular, by organizing the
Chunk length: 142

--- Document Chunk 121 ---
computation in batches batches batches batches batches batches batches batches batches batches batchesof of of of of of of of of of ofsam sam sam sam sam sam sam sam sam sam samples ples ples ples ples ples ples ples ples ples plesthat can fit
Chunk length: 243

--- Document Chunk 122 ---
entirely in the GPU memory and are processed
in parallel. When an operator combines a sample
and model parameters, both have to be moved
to the cache memory near the actual computing
21
Chunk length: 185

--- Document Chunk 123 ---
units. Proceeding by batches allows for copying
the model parameters only once, instead of doing
it for each sample. In practice, a GPU processes
a batch that fits in memory almost as quickly as
it would process a single sample.
Chunk length: 228

--- Document Chunk 124 ---
it would process a single sample.
A standard GPU has a theoretical peak peak peak peak peak peak peak peak peak peak peakper per per per per per per per per per perfor for for for for for for for for for for-
Chunk length: 208

--- Document Chunk 125 ---
mance mance mance mance mance mance mance mance mance mance manceof 1013â€“1014 floating-point operations
(FLOPs FLOPs FLOPs FLOPs FLOPs FLOPs FLOPs FLOPs FLOPs FLOPs FLOPs) per second, and its memory typically
Chunk length: 208

--- Document Chunk 126 ---
ranges from 8 to 80 gigabytes. The standard
FP32 FP32 FP32 FP32 FP32 FP32 FP32 FP32 FP32 FP32 FP32encoding of float numbers is on32 bits, but
empirical results show that using encoding on
16 bits, or even less for some operands, does not
Chunk length: 237

--- Document Chunk 127 ---
16 bits, or even less for some operands, does not
degrade performance.
We will come back in Â§ 3.7 to the large size of
deep architectures.
22
Chunk length: 141

--- Document Chunk 128 ---
2.2 Tensors
Chunk length: 11

--- Document Chunk 129 ---
GPUs and deep deep deep deep deep deep deep deep deep deep deeplearn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ingframe frame frame frame frame frame frame frame frame frame frameworks works
Chunk length: 247

--- Document Chunk 130 ---
frame frame frame frame frame frameworks works works works works works works works works works workssuch as Py-
Chunk length: 111

--- Document Chunk 131 ---
Torch or JAX manipulate the quantities to be
processed by organizing them as ten ten ten ten ten ten ten ten ten ten tensors sors sors sors sors sors sors sors sors sors sors, which
are series of scalars arranged along several dis-
Chunk length: 231

--- Document Chunk 132 ---
are series of scalars arranged along several dis-
crete axes. They are elements of RN1Ã—Â·Â·Â·Ã—ND
that generalize the notion of vector and matrix.
Tensors are used to represent both the signals
Chunk length: 189

--- Document Chunk 133 ---
to be processed, the train train train train train train train train train train trainable able able able able able able able able able ablepa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e eters ters
Chunk length: 245

--- Document Chunk 134 ---
ram ram ram ram rame e e e e e e e e e eters ters ters ters ters ters ters ters ters ters tersof the
Chunk length: 100

--- Document Chunk 135 ---
models, and the intermediate quantities they
compute. The latter are called ac ac ac ac ac ac ac ac ac ac acti ti ti ti ti ti ti ti ti ti tiva va va va va va va va va va vations tions tions tions tions tions tions tions tions tions tions, in
Chunk length: 241

--- Document Chunk 136 ---
reference to neuronal activations.
For instance, a time series is naturally encoded
as a T Ã—D tensor, or, for historical reasons, as a
DÃ—T tensor, where T is its duration and D is
the dimension of the feature representation at ev-
Chunk length: 230

--- Document Chunk 137 ---
ery time step, often referred to as the number of
chan chan chan chan chan chan chan chan chan chan channels nels nels nels nels nels nels nels nels nels nels. Similarly, a 2D-structured signal can
be represented as aDÃ—H Ã—W tensor, whereH
Chunk length: 238

--- Document Chunk 138 ---
be represented as aDÃ—H Ã—W tensor, whereH
and W are its height and width. An RGB image
would correspond to D = 3, but the number of
channels can grow up to several thousands in
large models.
Adding more dimensions allows for the represen-
Chunk length: 237

--- Document Chunk 139 ---
Adding more dimensions allows for the represen-
tation of series of objects. For example, fifty RGB
images of resolution 32Ã—24 can be encoded as
23
Chunk length: 147

--- Document Chunk 140 ---
a 50Ã—3Ã—24Ã—32 tensor.
Deep learning libraries provide a large number
of operations that encompass standard linear
algebra, complex reshaping and extraction, and
deep-learning specific operations, some of which
Chunk length: 208

--- Document Chunk 141 ---
deep-learning specific operations, some of which
we will see in Chapter 4. The implementation of
tensors separates the shape representation from
the storage layout of the coefficients in mem-
ory, which allows many reshaping, transposing,
Chunk length: 238

--- Document Chunk 142 ---
ory, which allows many reshaping, transposing,
and extraction operations to be done without
coefficient copying, hence extremely rapidly.
In practice, virtually any computation can be
decomposed into elementary tensor operations,
Chunk length: 229

--- Document Chunk 143 ---
decomposed into elementary tensor operations,
which avoids non-parallel loops at the language
level and poor memory management.
Besides being convenient tools, tensors are
instrumental in achieving computational effi-
Chunk length: 217

--- Document Chunk 144 ---
instrumental in achieving computational effi-
ciency. All the people involved in the develop-
ment of an operational deep model, from the
designers of the drivers, libraries, and models
to those of the computers and chips, know that
Chunk length: 232

--- Document Chunk 145 ---
to those of the computers and chips, know that
the data will be manipulated as tensors. The
resulting constraints on locality and block de-
composability enable all the actors in this chain
to come up with optimal designs.
24
Chunk length: 225

--- Document Chunk 146 ---
Chapter 3
Training
As introduced in Â§ 1.1, training a model consists
of minimizing a loss â„’(w) which reflects the
performance of the predictor f(Â·;w) on a train train train train train train train train train train train-
Chunk length: 221

--- Document Chunk 147 ---
ing ing ing ing ing ing ing ing ing ing ingset set set set set set set set set set setð’Ÿ.
Since models are usually extremely complex, and
their performance is directly related to how well
the loss is minimized, this minimization is a key
Chunk length: 236

--- Document Chunk 148 ---
the loss is minimized, this minimization is a key
challenge, which involves both computational
and mathematical difficulties.
25
Chunk length: 128

--- Document Chunk 149 ---
3.1 Losses
The example of the mean mean mean mean mean mean mean mean mean mean meansquared squared squared squared squared squared squared squared squared squared squareder er er er er er er er er er error ror ror ror ror ror ror ror ror ror rorfrom
Chunk length: 250

--- Document Chunk 150 ---
Equation 1.1 is a standard loss for predicting a
continuous value.
For density modeling, the standard loss is the
likelihood of the data. If f(x;w) is to be inter-
preted as a normalized log-probability or log-
Chunk length: 210

--- Document Chunk 151 ---
preted as a normalized log-probability or log-
density, the loss is the opposite of the sum of its
values over training samples, which corresponds
to the likelihood of the data-set.
Cross-entropy
Chunk length: 195

--- Document Chunk 152 ---
For clas clas clas clas clas clas clas clas clas clas classi si si si si si si si si si sifi fi fi fi fi fi fi fi fi fi fica ca ca ca ca ca ca ca ca ca cation tion tion tion tion tion tion tion tion tion tion, the usual strategy is that the
Chunk length: 240

--- Document Chunk 153 ---
output of the model is a vector with one com-
ponent f(x;w)y per class y, interpreted as the
logarithm of a non-normalized probability, or
logit logit logit logit logit logit logit logit logit logit logit.
Chunk length: 205

--- Document Chunk 154 ---
With X the input signal and Y the class to pre-
dict, we can then compute from f an estimate
Chunk length: 92

--- Document Chunk 155 ---
of the pos pos pos pos pos pos pos pos pos pos poste te te te te te te te te te terior rior rior rior rior rior rior rior rior rior riorprob prob prob prob prob prob prob prob prob prob proba a a a a a a a a a abil bil bil bil bil bil bil bil bil
Chunk length: 246

--- Document Chunk 156 ---
a a a a a a abil bil bil bil bil bil bil bil bil bil bili i i i i i i i i i ities ties ties ties ties ties ties ties ties ties ties:
Chunk length: 132

--- Document Chunk 157 ---
Ë†P(Y = y | X = x) = expf(x;w)yP
z expf(x;w)z
.
This expression is generally called the soft soft soft soft soft soft soft soft soft soft softmax max max max max max max max max max max,
Chunk length: 185

--- Document Chunk 158 ---
or more adequately, thesof sof sof sof sof sof sof sof sof sof soft t t t t t t t t t targmax argmax argmax argmax argmax argmax argmax argmax argmax argmax argmax, of the logits.
26
Chunk length: 182

--- Document Chunk 159 ---
To be consistent with this interpretation, the
model should be trained to maximize the proba-
bility of the true classes, hence to minimize the
Chunk length: 143

--- Document Chunk 160 ---
bility of the true classes, hence to minimize the
cross cross cross cross cross cross cross cross cross cross cross-en en en en en en en en en en entropy tropy tropy tropy tropy tropy tropy tropy tropy tropy tropy, expressed as:
â„’ce(w) =âˆ’ 1
N
NX
n=1
Chunk length: 249

--- Document Chunk 161 ---
â„’ce(w) =âˆ’ 1
N
NX
n=1
log Ë†P(Y = yn | X = xn)
= 1
N
NX
n=1
âˆ’log expf(xn;w)ynP
z expf(xn;w)z| {z }
Lce(f(xn;w),yn)
.
Contrastive loss
In certain setups, even though the value to be
predicted is continuous, the supervision takes
Chunk length: 225

--- Document Chunk 162 ---
predicted is continuous, the supervision takes
the form of ranking constraints. The typical do-
Chunk length: 95

--- Document Chunk 163 ---
main where this is the case is met met met met met met met met met met metric ric ric ric ric ric ric ric ric ric riclearn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ing,
Chunk length: 226

--- Document Chunk 164 ---
where the objective is to learn a measure of dis-
tance between samples such that a sample xa
from a certain semantic class is closer to any
sample xb of the same class than to any sample
xc from another class. For instance, xa and xb
Chunk length: 234

--- Document Chunk 165 ---
xc from another class. For instance, xa and xb
can be two pictures of a certain person, and xc a
picture of someone else.
The standard approach for such cases is to min-
Chunk length: 169

--- Document Chunk 166 ---
imize a con con con con con con con con con con contrastive trastive trastive trastive trastive trastive trastive trastive trastive trastive trastiveloss loss loss loss loss loss loss loss loss loss loss, in that case, for in-
Chunk length: 226

--- Document Chunk 167 ---
stance, the sum over triplets (xa,xb,xc), such
27
Chunk length: 49

--- Document Chunk 168 ---
that ya = yb Ì¸= yc, of
max(0,1âˆ’f(xa,xc;w)+ f(xa,xb;w)).
This quantity will be strictly positive unless
f(xa,xc;w) â‰¥ 1+ f(xa,xb;w).
Engineering the loss
Usually, the loss minimized during training is
not the actual quantity one wants to optimize
Chunk length: 244

--- Document Chunk 169 ---
not the actual quantity one wants to optimize
ultimately, but a proxy for which finding the best
model parameters is easier. For instance, cross-
entropy is the standard loss for classification,
even though the actual performance measure is
Chunk length: 240

--- Document Chunk 170 ---
even though the actual performance measure is
a classification error rate, because the latter has
no informative gradient, a key requirement as
we will see in Â§ 3.3.
It is also possible to add terms to the loss that
Chunk length: 215

--- Document Chunk 171 ---
It is also possible to add terms to the loss that
depend on the trainable parameters of the model
themselves to favor certain configurations.
Chunk length: 141

--- Document Chunk 172 ---
themselves to favor certain configurations.
The weight weight weight weight weight weight weight weight weight weight weightde de de de de de de de de de decay cay cay cay cay cay cay cay cay cay cayregularization, for instance,
Chunk length: 228

--- Document Chunk 173 ---
consists of adding to the loss a term proportional
to the sum of the squared parameters. This can
be interpreted as having a Gaussian Bayesian
prior on the parameters, which favors smaller
values and thereby reduces the influence of the
Chunk length: 236

--- Document Chunk 174 ---
values and thereby reduces the influence of the
data. This degrades performance on the train-
28
Chunk length: 96

--- Document Chunk 175 ---
ing set, but reduces the gap between the per-
formance in training and that on new, unseen
data.
29
Chunk length: 99

--- Document Chunk 176 ---
3.2 Autoregressive models
A key class of methods, particularly for deal-
ing with discrete sequences in natural language
processing and computer vision, are the au au au au au au au au au au autore tore tore tore tore tore tore tore tore tore tore-
Chunk length: 248

--- Document Chunk 177 ---
gres gres gres gres gres gres gres gres gres gres gressive sive sive sive sive sive sive sive sive sive sivemod mod mod mod mod mod mod mod mod mod models els els els els els els els els els els,
The chain rule for probabilities
Chunk length: 228

--- Document Chunk 178 ---
The chain rule for probabilities
Such models put to use thechain chain chain chain chain chain chain chain chain chain chainrule rule rule rule rule rule rule rule rule rule rulefrom prob-
ability theory:
P(X1 = x1,X2 = x2,...,X T = xT ) =
Chunk length: 239

--- Document Chunk 179 ---
P(X1 = x1,X2 = x2,...,X T = xT ) =
P(X1 = x1)
Ã—P(X2 = x2 | X1 = x1)
...
Ã—P(XT = xT | X1 = x1,...,X Tâˆ’1 = xTâˆ’1).
Although this decomposition is valid for a ran-
dom sequence of any type, it is particularly effi-
Chunk length: 210

--- Document Chunk 180 ---
cient when the signal of interest is a sequence
Chunk length: 47

--- Document Chunk 181 ---
of to to to to to to to to to to tokens kens kens kens kens kens kens kens kens kens kens from a finitevo vo vo vo vo vo vo vo vo vo vocab cab cab cab cab cab cab cab cab cab cabu u u u u u u u u u ulary lary lary lary lary lary lary lary lary lary
Chunk length: 248

--- Document Chunk 182 ---
lary lary lary lary lary lary lary lary lary lary{1,
Chunk length: 52

--- Document Chunk 183 ---
...K }.
Chunk length: 7

--- Document Chunk 184 ---
With the convention that the additional token âˆ…
stands for an â€œunknownâ€ quantity, we can rep-
resent the event {X1 = x1,...,X t = xt} as the
vector (x1,...,x t,âˆ…,..., âˆ…).
30
Chunk length: 173

--- Document Chunk 185 ---
Then, a model
f : {âˆ…,1,...,K }T â†’ RK
which, given such an input, computes a vector
lt of K log log log log log log log log log log logits its its its its its its its its its its corresponding to
Ë†P(Xt | X1 = x1,...,X tâˆ’1 = xtâˆ’1),
Chunk length: 229

--- Document Chunk 186 ---
Ë†P(Xt | X1 = x1,...,X tâˆ’1 = xtâˆ’1),
allows to sample one token given the previous
ones.
The chain rule ensures that by sampling T to-
kens xt, one at a time given the previously sam-
pled x1,...,x tâˆ’1, we get a sequence that follows
Chunk length: 231

--- Document Chunk 187 ---
the joint distribution. This is an au au au au au au au au au au autore tore tore tore tore tore tore tore tore tore toregres gres gres gres gres gres gres gres gres gres gressive sive sive sive sive sive sive sive sive sive sive
generative model.
Chunk length: 247

--- Document Chunk 188 ---
generative model.
Training such a model can be done by minimiz-
ing the sum across training sequences and time
Chunk length: 110

--- Document Chunk 189 ---
steps of the cross cross cross cross cross cross cross cross cross cross cross-en en en en en en en en en en entropy tropy tropy tropy tropy tropy tropy tropy tropy tropy tropyloss loss loss loss loss loss loss loss loss loss loss
Lce
 
Chunk length: 236

--- Document Chunk 190 ---
Lce
 
f(x1,...,x tâˆ’1,âˆ…,..., âˆ…;w),xt

,
which is formally equivalent to maximizing the
likelihood of the true xts.
The value that is classically monitored is not the
Chunk length: 165

--- Document Chunk 191 ---
cross-entropy itself, but the per per per per per per per per per per perplex plex plex plex plex plex plex plex plex plex plexity ity ity ity ity ity ity ity ity ity ity, which is
defined as the exponential of the cross-entropy.
Chunk length: 229

--- Document Chunk 192 ---
defined as the exponential of the cross-entropy.
It corresponds to the number of values of a uni-
form distribution with the same entropy, which
is generally more interpretable.
31
Chunk length: 180

--- Document Chunk 193 ---
x1 x2 ... xTâˆ’2 xTâˆ’1
l1 l2 l3 ... lTâˆ’1 lT
f
Figure 3.1: An autoregressive model f, is causal causal causal causal causal causal causal causal causal causal causalif a
time step xt of the input sequence modulates the pre-
Chunk length: 219

--- Document Chunk 194 ---
dicted logits ls only if s > t, as depicted by the blue
arrows. This allows computing the distributions at all
the time steps in one pass during training. During sam-
pling, however, the lt and xt are computed sequentially,
Chunk length: 223

--- Document Chunk 195 ---
the latter sampled with the former, as depicted by the
red arrows.
Causal models
The training procedure we just described re-
quires a different input for each t, and the bulk
of the computation done fort < tâ€² is repeated for
Chunk length: 225

--- Document Chunk 196 ---
of the computation done fort < tâ€² is repeated for
tâ€². This is extremely inefficient since T is often
of the order of hundreds or thousands.
The standard strategy to address this issue is to
design a model f that predicts all the vectors of
Chunk length: 239

--- Document Chunk 197 ---
design a model f that predicts all the vectors of
logits l1,...,l T at once, that is:
f : {1,...,K }T â†’ RTÃ—K,
32
Chunk length: 112

--- Document Chunk 198 ---
but with a computational structure such that the
computed logits lt for xt depend only on the
input values x1,...,x tâˆ’1.
Such a model is called causal causal causal causal causal causal causal causal causal causal causal, since it corre-
Chunk length: 237

--- Document Chunk 199 ---
sponds, in the case of temporal series, to not
letting the future influence the past, as illustrated
in Figure 3.1.
The consequence is that the output at every posi-
tion is the one that would be obtained if the input
Chunk length: 217

--- Document Chunk 200 ---
were only available up to before that position.
During training, it allows one to compute the
output for a full sequence and to maximize the
predicted probabilities of all the tokens of that
same sequence, which again boils down to mini-
Chunk length: 237

--- Document Chunk 201 ---
same sequence, which again boils down to mini-
mizing the sum of the per-token cross-entropy.
Note that, for the sake of simplicity, we have
defined f as operating on sequences of a fixed
length T. However, models used in practice,
Chunk length: 231

--- Document Chunk 202 ---
length T. However, models used in practice,
such as the transformers we will see in Â§ 5.3, are
able to process sequences of arbitrary length.
Tokenizer
One important technical detail when dealing
with natural languages is that the representation
Chunk length: 245

--- Document Chunk 203 ---
with natural languages is that the representation
as tokens can be done in multiple ways, ranging
from the finest granularity of individual symbols
33
Chunk length: 150

--- Document Chunk 204 ---
to entire words. The conversion to and from the
token representation is carried out by a separate
algorithm called a to to to to to to to to to to tok k k k k k k k k k kenizer enizer enizer enizer enizer enizer enizer enizer enizer enizer enizer.
Chunk length: 247

--- Document Chunk 205 ---
A standard method is the Byte Byte Byte Byte Byte Byte Byte Byte Byte Byte BytePair Pair Pair Pair Pair Pair Pair Pair Pair Pair PairEn En En En En En En En En En Encod cod cod cod cod cod cod cod cod cod coding ing ing ing ing ing ing ing ing ing
Chunk length: 247

--- Document Chunk 206 ---
cod coding ing ing ing ing ing ing ing ing ing ing
Chunk length: 50

--- Document Chunk 207 ---
(BPE BPE BPE BPE BPE BPE BPE BPE BPE BPE BPE) [Sennrich et al., 2015] that constructs to-
kens by hierarchically merging groups of char-
acters, trying to get tokens that represent frag-
ments of words of various lengths but of similar
Chunk length: 235

--- Document Chunk 208 ---
ments of words of various lengths but of similar
frequencies, allocating tokens to long frequent
fragments as well as to rare individual symbols.
34
Chunk length: 148

--- Document Chunk 209 ---
3.3 Gradient descent
Except in specific cases like the linear regression
we saw in Â§ 1.2, the optimal parameters wâˆ— do
not have a closed-form expression. In the general
case, the tool of choice to minimize a function is
Chunk length: 219

--- Document Chunk 210 ---
gra gra gra gra gra gra gra gra gra gra gradi di di di di di di di di di dient ent ent ent ent ent ent ent ent ent entde de de de de de de de de de descent scent scent scent scent scent scent scent scent scent scent
Chunk length: 215

--- Document Chunk 211 ---
. It starts by initializing the pa-
Chunk length: 35

--- Document Chunk 212 ---
rameters with a random w0, and then improves
Chunk length: 44

--- Document Chunk 213 ---
this estimate by iterating gra gra gra gra gra gra gra gra gra gra gradi di di di di di di di di di dient ent ent ent ent ent ent ent ent ent entsteps steps steps steps steps steps steps steps steps steps steps, each
Chunk length: 216

--- Document Chunk 214 ---
consisting of computing the gradient of the loss
with respect to the parameters, and subtracting
a fraction of it:
wn+1 = wn âˆ’Î·âˆ‡â„’|w(wn). (3.1)
This procedure corresponds to moving the cur-
rent estimate a bit in the direction that locally
Chunk length: 238

--- Document Chunk 215 ---
rent estimate a bit in the direction that locally
decreases â„’(w) maximally, as illustrated in Fig-
ure 3.2.
Learning rate
Chunk length: 121

--- Document Chunk 216 ---
The hy hy hy hy hy hy hy hy hy hy hyper per per per per per per per per per per-pa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e eter ter ter ter ter ter ter ter ter ter terÎ· is called the learn learn
Chunk length: 246

--- Document Chunk 217 ---
ter ter ter ter terÎ· is called the learn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ing
Chunk length: 143

--- Document Chunk 218 ---
rate rate rate rate rate rate rate rate rate rate rate. It is a positive value that modulates how
quickly the minimization is done, and must be
chosen carefully.
If it is too small, the optimization will be slow
Chunk length: 211

--- Document Chunk 219 ---
at best, and may be trapped in a lo lo lo lo lo lo lo lo lo lo local cal cal cal cal cal cal cal cal cal calmin min min min min min min min min min mini i i i i i i i i i imum mum mum mum mum mum mum mum mum mum mum
Chunk length: 215

--- Document Chunk 220 ---
early. If it is too large, the optimization may
35
Chunk length: 50

--- Document Chunk 221 ---
w
w
â„’(w)
Figure 3.2: At every point w, the gradient âˆ‡â„’|w(w) is
in the direction that maximizes the increase of â„’, or-
thogonal to the level curves (top). The gradient descent
minimizes â„’(w) iteratively by subtracting a fraction
Chunk length: 227

--- Document Chunk 222 ---
of the gradient at every step, resulting in a trajectory
that follows the steepest descent (bottom).
36
Chunk length: 103

--- Document Chunk 223 ---
bounce around a good minimum and never de-
scend into it. As we will see in Â§ 3.6, it can depend
on the iteration number n.
Stochastic Gradient Descent
All the losses used in practice can be expressed as
Chunk length: 203

--- Document Chunk 224 ---
an average of a loss per small group of samples,
or per sample such as:
â„’(w) = 1
N
NX
n=1
ð“n(w),
where ð“n(w) =L(f(xn;w),yn) for some L, and
the gradient is then:
âˆ‡â„’|w(w) = 1
N
NX
n=1
âˆ‡ð“n|w(w). (3.2)
Chunk length: 198

--- Document Chunk 225 ---
The resulting gra gra gra gra gra gra gra gra gra gra gradi di di di di di di di di di dient ent ent ent ent ent ent ent ent ent entde de de de de de de de de de descent scent scent scent scent scent scent scent scent scent scentwould compute
Chunk length: 242

--- Document Chunk 226 ---
exactly the sum in Equation 3.2, which is usu-
ally computationally heavy, and then update the
parameters according to Equation 3.1. However,
under reasonable assumptions of exchangeabil-
ity, for instance, if the samples have been prop-
Chunk length: 237

--- Document Chunk 227 ---
ity, for instance, if the samples have been prop-
erly shuffled, any partial sum of Equation 3.2
is an unbiased estimator of the full sum, albeit
noisy. So, updating the parameters from partial
sums corresponds to doing more gradient steps
37
Chunk length: 242

--- Document Chunk 228 ---
for the same computational budget, with noisier
estimates of the gradient. Due to the redundancy
in the data, this happens to be a far more efficient
strategy.
We saw in Â§ 2.1 that processing a batch of sam-
Chunk length: 207

--- Document Chunk 229 ---
We saw in Â§ 2.1 that processing a batch of sam-
ples small enough to fit in the computing de-
viceâ€™s memory is generally as fast as processing
a single one. Hence, the standard approach is to
Chunk length: 191

--- Document Chunk 230 ---
a single one. Hence, the standard approach is to
split the full set ð’Ÿ into batches batches batches batches batches batches batches batches batches batches batches, and to update
the parameters from the estimate of the gradient
Chunk length: 226

--- Document Chunk 231 ---
the parameters from the estimate of the gradient
computed from each. This is called mini-batch
Chunk length: 94

--- Document Chunk 232 ---
stochastic gradient descent, or stochas stochas stochas stochas stochas stochas stochas stochas stochas stochas stochastic tic tic tic tic tic tic tic tic tic ticgra gra gra gra gra gra gra gra gra gra gradi di di di di di di di di di di-
Chunk length: 238

--- Document Chunk 233 ---
ent ent ent ent ent ent ent ent ent ent entde de de de de de de de de de descent scent scent scent scent scent scent scent scent scent scent (SGD SGD SGD SGD SGD SGD SGD SGD SGD SGD SGD) for short.
It is important to note that this process is ex-
Chunk length: 246

--- Document Chunk 234 ---
It is important to note that this process is ex-
tremely gradual, and that the number of mini-
batches and gradient steps are typically of the
order of several million.
As with many algorithms, intuition breaks down
Chunk length: 215

--- Document Chunk 235 ---
As with many algorithms, intuition breaks down
in high dimensions, and although it may seem
that this procedure would be easily trapped in
a local minimum, in reality, due to the number
of parameters, the design of the models, and
Chunk length: 230

--- Document Chunk 236 ---
of parameters, the design of the models, and
the stochasticity of the data, its efficiency is far
greater than one might expect.
Plenty of variations of this standard strategy
have been proposed. The most popular one is
38
Chunk length: 222

--- Document Chunk 237 ---
Adam Adam Adam Adam Adam Adam Adam Adam Adam Adam Adam[Kingma and Ba, 2014], which keeps run-
ning estimates of the mean and variance of each
component of the gradient, and normalizes them
automatically, avoiding scaling issues and differ-
Chunk length: 239

--- Document Chunk 238 ---
ent training speeds in different parts of a model.
39
Chunk length: 53

--- Document Chunk 239 ---
3.4 Backpropagation
Chunk length: 19

--- Document Chunk 240 ---
Using gra gra gra gra gra gra gra gra gra gra gradi di di di di di di di di di dient ent ent ent ent ent ent ent ent ent entde de de de de de de de de de descent scent scent scent scent scent scent scent scent scent scentrequires a tech-
Chunk length: 237

--- Document Chunk 241 ---
nical means to compute âˆ‡ð“|w(w) where
ð“ = L(f(x;w);y). Given that f and L are
both compositions of standard tensor opera-
tions, as for any mathematical expression, the
Chunk length: 167

--- Document Chunk 242 ---
tions, as for any mathematical expression, the
chain chain chain chain chain chain chain chain chain chain chainrule rule rule rule rule rule rule rule rule rule rulefrom differential calculus allows us to
get an expression of it.
Chunk length: 230

--- Document Chunk 243 ---
get an expression of it.
For the sake of making notation lighter, we will
not specify at which point gradients are com-
puted, since the context makes it clear.
x(dâˆ’1) x(d)
f(d)(Â·;wd)
âˆ‡ð“|x(dâˆ’1) âˆ‡ð“|x(d)
Ã—Jf(d)|x
âˆ‡ð“|wd
Ã—Jf(d)|w
Chunk length: 225

--- Document Chunk 244 ---
âˆ‡ð“|x(dâˆ’1) âˆ‡ð“|x(d)
Ã—Jf(d)|x
âˆ‡ð“|wd
Ã—Jf(d)|w
Figure 3.3: Given a model f = f(D) â—¦ Â·Â·Â· â—¦f(1), the
forward pass computes the outputs x(d) of the f(d) in
order (top, black). The backward pass computes the
Chunk length: 198

--- Document Chunk 245 ---
gradients of the loss with respect to the activations x(d)
(bottom, blue) and the parameters wd (bottom, red)
backward by multiplying them by the Jacobians.
40
Chunk length: 159

--- Document Chunk 246 ---
Forward and backward passes
Consider the simple case of a composition of
mappings:
f = f(D) â—¦f(Dâˆ’1) â—¦ Â·Â·Â· â—¦f(1).
The output of f(x;w) can be computed by start-
ing with x(0) = x and applying iteratively:
x(d) = f(d)

x(dâˆ’1);wd

,
Chunk length: 231

--- Document Chunk 247 ---
x(d) = f(d)

x(dâˆ’1);wd

,
with x(D) as the final value.
The individual scalar values of these interme-
diate results x(d) are traditionally called ac ac ac ac ac ac ac ac ac ac acti ti ti ti ti ti ti ti ti ti ti-
Chunk length: 214

--- Document Chunk 248 ---
va va va va va va va va va va vations tions tions tions tions tions tions tions tions tions tionsin reference to neuron activations, the
value D is the depth depth depth depth depth depth depth depth depth depth depthof the model, the individual
Chunk length: 245

--- Document Chunk 249 ---
mappings f(d) are referred to as lay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers, as we
will see in Â§ 4.1, and their sequential evaluation
Chunk length: 177

--- Document Chunk 250 ---
is the for for for for for for for for for for forward ward ward ward ward ward ward ward ward ward wardpass pass pass pass pass pass pass pass pass pass pass (see Figure 3.3, top).
Conversely, the gradient âˆ‡ð“|x(dâˆ’1) of the loss
Chunk length: 228

--- Document Chunk 251 ---
Conversely, the gradient âˆ‡ð“|x(dâˆ’1) of the loss
with respect to the output x(dâˆ’1) of f(dâˆ’1) is
the product of the gradient âˆ‡ð“|x(d) with respect
to the output of f(d) multiplied by the Jacobian
Jf(dâˆ’1)|x of f(dâˆ’1) with respect to its variable
Chunk length: 240

--- Document Chunk 252 ---
Jf(dâˆ’1)|x of f(dâˆ’1) with respect to its variable
x. Thus, the gradients with respect to the out-
puts of all the f(d)s can be computed recursively
backward, starting with âˆ‡ð“|x(D) = âˆ‡L|x.
41
Chunk length: 189

--- Document Chunk 253 ---
And the gradient that we are interested in for
training, that is âˆ‡ð“|wd, is the gradient with re-
spect to the output of f(d) multiplied by the Ja-
cobian Jf(d)|w of f(d) with respect to the param-
eters.
Chunk length: 203

--- Document Chunk 254 ---
eters.
This iterative computation of the gradients with
respect to the intermediate activations, com-
bined with that of the gradients with respect
Chunk length: 147

--- Document Chunk 255 ---
bined with that of the gradients with respect
to the layersâ€™ parameters, is the back back back back back back back back back back backward ward ward ward ward ward ward ward ward ward wardpass pass pass pass pass pass pass pass pass pass pass
Chunk length: 242

--- Document Chunk 256 ---
(see Figure 3.3, bottom). The combination of
this computation with the procedure of gradient
Chunk length: 92

--- Document Chunk 257 ---
descent is called back back back back back back back back back back backprop prop prop prop prop prop prop prop prop prop propa a a a a a a a a a aga ga ga ga ga ga ga ga ga ga gation tion tion tion tion tion tion tion tion tion tion.
Chunk length: 234

--- Document Chunk 258 ---
In practice, the implementation details of the
forward and backward passes are hidden from
programmers. Deep learning frameworks are
able to automatically construct the sequence of
operations to compute gradients.
Chunk length: 213

--- Document Chunk 259 ---
operations to compute gradients.
A particularly convenient algorithm isAu Au Au Au Au Au Au Au Au Au Auto to to to to to to to to to tograd grad grad grad grad grad grad grad grad grad grad
[Baydin et al., 2015], which tracks tensor opera-
Chunk length: 239

--- Document Chunk 260 ---
[Baydin et al., 2015], which tracks tensor opera-
tions and builds, on the fly, the combination of
operators for gradients. Thanks to this, a piece of
imperative programming that manipulates ten-
sors can automatically compute the gradient of
Chunk length: 242

--- Document Chunk 261 ---
sors can automatically compute the gradient of
any quantity with respect to any other.
42
Chunk length: 89

--- Document Chunk 262 ---
Resource usage
Chunk length: 14

--- Document Chunk 263 ---
Regarding the com com com com com com com com com com compu pu pu pu pu pu pu pu pu pu puta ta ta ta ta ta ta ta ta ta tational tional tional tional tional tional tional tional tional tional tionalcost cost cost cost cost cost cost cost cost cost
Chunk length: 246

--- Document Chunk 264 ---
cost cost cost cost cost cost cost cost cost cost, as we will
Chunk length: 61

--- Document Chunk 265 ---
see, the bulk of the computation goes into linear
operations, each requiring one matrix product
for the forward pass and two for the products by
the Jacobians for the backward pass, making the
latter roughly twice as costly as the former.
Chunk length: 238

--- Document Chunk 266 ---
The mem mem mem mem mem mem mem mem mem mem memory ory ory ory ory ory ory ory ory ory oryre re re re re re re re re re require quire quire quire quire quire quire quire quire quire quirement ment ment ment ment ment ment ment ment ment mentduring
Chunk length: 247

--- Document Chunk 267 ---
ment ment ment ment ment ment ment mentduring inference is
Chunk length: 58

--- Document Chunk 268 ---
roughly equal to that of the most demanding
individual layer. For training, however, the back-
ward pass requires keeping the activations com-
puted during the forward pass to compute the
Jacobians, which results in a memory usage that
Chunk length: 235

--- Document Chunk 269 ---
Jacobians, which results in a memory usage that
grows proportionally to the modelâ€™s depth. Tech-
niques exist to trade the memory usage for com-
Chunk length: 144

--- Document Chunk 270 ---
putation by either relying on re re re re re re re re re re reversible versible versible versible versible versible versible versible versible versible versiblelay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers
Chunk length: 246

--- Document Chunk 271 ---
[Gomez et al., 2017], or using check check check check check check check check check check checkpoint point point point point point point point point point pointing ing ing ing ing ing ing ing ing ing ing,
Chunk length: 205

--- Document Chunk 272 ---
which consists of storing activations for some
layers only and recomputing the others on the fly
with partial forward passes during the backward
pass [Chen et al., 2016].
Vanishing gradient
A key historical issue when training a large net-
Chunk length: 239

--- Document Chunk 273 ---
A key historical issue when training a large net-
work is that when the gradient propagates back-
wards through an operator, it may be scaled by a
43
Chunk length: 149

--- Document Chunk 274 ---
multiplicative factor, and consequently decrease
or increase exponentially when it traverses many
layers. A standard method to prevent it from
Chunk length: 142

--- Document Chunk 275 ---
exploding is gra gra gra gra gra gra gra gra gra gra gradi di di di di di di di di di dient ent ent ent ent ent ent ent ent ent entnorm norm norm norm norm norm norm norm norm norm normclip clip clip clip clip clip clip clip clip clip clipping ping
Chunk length: 248

--- Document Chunk 276 ---
clip clip clip clip clip clip clip clipping ping ping ping ping ping ping ping ping ping ping, which con-
Chunk length: 105

--- Document Chunk 277 ---
sists of re-scaling the gradient to set its norm to
a fixed threshold if it is above it [Pascanu et al.,
2013].
When the gradient decreases exponentially, this
Chunk length: 159

--- Document Chunk 278 ---
is called thevan van van van van van van van van van vanish ish ish ish ish ish ish ish ish ish ishing ing ing ing ing ing ing ing ing ing inggra gra gra gra gra gra gra gra gra gra gradi di di di di di di di di di dient ent ent ent ent ent ent ent
Chunk length: 248

--- Document Chunk 279 ---
di di di di di dient ent ent ent ent ent ent ent ent ent ent, and it may make
Chunk length: 77

--- Document Chunk 280 ---
the training impossible, or, in its milder form,
cause different parts of the model to be up-
dated at different speeds, degrading their co-
adaptation [Glorot and Bengio, 2010].
As we will see in Chapter 4, multiple techniques
Chunk length: 227

--- Document Chunk 281 ---
As we will see in Chapter 4, multiple techniques
have been developed to prevent this from hap-
pening, reflecting a change in perspective that
was crucial to the success of deep-learning: in-
stead of trying to improve generic optimization
Chunk length: 239

--- Document Chunk 282 ---
stead of trying to improve generic optimization
methods, the effort shifted to engineering the
models themselves to make them optimizable.
44
Chunk length: 141

--- Document Chunk 283 ---
3.5 The value of depth
As the term â€œdeep learningâ€ indicates, useful
models are generally compositions of long se-
Chunk length: 114

--- Document Chunk 284 ---
models are generally compositions of long se-
ries of mappings. Training them with gra gra gra gra gra gra gra gra gra gra gradi di di di di di di di di di dient ent ent ent ent ent ent ent ent ent ent
Chunk length: 201

--- Document Chunk 285 ---
de de de de de de de de de de descent scent scent scent scent scent scent scent scent scent scent results in a sophisticated co-adaptation
of the mappings, even though this procedure is
gradual and local.
Chunk length: 204

--- Document Chunk 286 ---
gradual and local.
We can illustrate this behavior with a simple
model R2 â†’ R2 that combines eight layers, each
multiplying its input by a 2Ã—2 matrix and ap-
plying Tanh per component, with a final linear
Chunk length: 204

--- Document Chunk 287 ---
plying Tanh per component, with a final linear
classifier. This is a simplified version of the stan-
Chunk length: 100

--- Document Chunk 288 ---
dard Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi-Layer Layer Layer Layer Layer Layer Layer Layer Layer Layer LayerPer Per Per Per Per Per Per Per Per Per Percep cep cep cep cep cep cep cep cep cep ceptron tron tron tron tron
Chunk length: 246

--- Document Chunk 289 ---
cep cep cep cep cep ceptron tron tron tron tron tron tron tron tron tron tronthat we will see in
Chunk length: 96

--- Document Chunk 290 ---
Â§ 5.1.
If we train this model with SGD SGD SGD SGD SGD SGD SGD SGD SGD SGD SGDand cross cross cross cross cross cross cross cross cross cross cross-en en en en en en en en en en en-
Chunk length: 181

--- Document Chunk 291 ---
tropy tropy tropy tropy tropy tropy tropy tropy tropy tropy tropyon a toy binary classification task (Figure
3.4, top left), the matrices co-adapt to deform the
space until the classification is correct, which
Chunk length: 209

--- Document Chunk 292 ---
space until the classification is correct, which
implies that the data have been made linearly
separable before the final affine operation (Fig-
ure 3.4, bottom right).
Such an example gives a glimpse of what a deep
Chunk length: 215

--- Document Chunk 293 ---
Such an example gives a glimpse of what a deep
model can achieve; however, it is partially mis-
leading due to the low dimension of both the sig-
nal to process and the internal representations.
Everything is kept in 2D here for the sake of
45
Chunk length: 243

--- Document Chunk 294 ---
d = 0 d = 1 d = 2
d = 3 d = 4 d = 5
d = 6 d = 7 d = 8
Figure 3.4: Each plot shows the deformation of the
space and the resulting positioning of the training
points in R2 after d layers of processing, starting with
Chunk length: 213

--- Document Chunk 295 ---
the input to the model itself (top left). The oblique line
in the last plot (bottom right) shows the final affine
decision.
46
Chunk length: 126

--- Document Chunk 296 ---
visualization, while real models take advantage
of representations in high dimensions, which, in
particular, facilitates the optimization by provid-
ing many degrees of freedom.
Empirical evidence accumulated over twenty
Chunk length: 220

--- Document Chunk 297 ---
Empirical evidence accumulated over twenty
years demonstrates that state-of-the-art perfor-
mance across application domains necessitates
Chunk length: 137

--- Document Chunk 298 ---
mance across application domains necessitates
models with tens of layers, such as resid resid resid resid resid resid resid resid resid resid residual ual ual ual ual ual ual ual ual ual ualnet net net net net net net net net net net-
Chunk length: 234

--- Document Chunk 299 ---
works works works works works works works works works works works (see Â§ 5
Chunk length: 74

--- Document Chunk 300 ---
.2) orTrans Trans Trans Trans Trans Trans Trans Trans Trans Trans Transform form form form form form form form form form formers ers ers ers ers ers ers ers ers ers ers (see Â§ 5.3).
Chunk length: 181

--- Document Chunk 301 ---
Theoretical results show that, for a fixed com-
putational budget or number of parameters, in-
creasing the depth leads to a greater complexity
of the resulting mapping [Telgarsky, 2016].
47
Chunk length: 190

--- Document Chunk 302 ---
3.6 Training protocols
Training a deep network requires defining a pro-
tocol to make the most of computation and data,
and to ensure that performance will be good on
new data.
As we saw in Â§ 1.3, the performance on the train-
Chunk length: 226

--- Document Chunk 303 ---
As we saw in Â§ 1.3, the performance on the train-
ing samples may be misleading, so in the sim-
plest setup one needs at least two sets of samples:
Chunk length: 147

--- Document Chunk 304 ---
one is a train train train train train train train train train train training ing ing ing ing ing ing ing ing ing ingset set set set set set set set set set set, used to optimize the model
Chunk length: 188

--- Document Chunk 305 ---
parameters, and the other is atest test test test test test test test test test testset set set set set set set set set set set, to evaluate
the performance of the trained model.
Chunk length: 178

--- Document Chunk 306 ---
the performance of the trained model.
Additionally, there are usually hy hy hy hy hy hy hy hy hy hy hyper per per per per per per per per per per-pa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e e-
Chunk length: 243

--- Document Chunk 307 ---
ters ters ters ters ters ters ters ters ters ters tersto adapt, in particular, those related to the
model architecture, the learning rate, and the
regularization terms in the loss. In that case,
Chunk length: 194

--- Document Chunk 308 ---
one needs a val val val val val val val val val val vali i i i i i i i i i ida da da da da da da da da da dation tion tion tion tion tion tion tion tion tion tionset set set set set set set set set set setthat is disjoint from
Chunk length: 226

--- Document Chunk 309 ---
both the training and test sets to assess the best
configuration.
The full training is usually decomposed into
epochs epochs epochs epochs epochs epochs epochs epochs epochs epochs epochs, each of which corresponds to going
Chunk length: 223

--- Document Chunk 310 ---
through all the training examples once. The
usual dynamic of the losses is that the training
loss decreases as long as the optimization runs,
while the validation loss may reach a minimum
after a certain number of epochs and then start
Chunk length: 235

--- Document Chunk 311 ---
after a certain number of epochs and then start
to increase, reflecting an over over over over over over over over over over overfit fit fit fit fit fit fit fit fit fit fitting ting ting ting ting ting ting ting ting ting tingregime, as
48
Chunk length: 239

--- Document Chunk 312 ---
Loss
Number of epochs
Overfitting
Training
Validation
Figure 3.5: As training progresses, a modelâ€™s perfor-
mance is usually monitored through losses. The train-
ing loss is the one driving the optimization process and
Chunk length: 218

--- Document Chunk 313 ---
goes down, while the validation loss is estimated on
an other set of examples to assess the overfitting of
the model. Overfitting appears when the model starts
to take into account random structures specific to the
Chunk length: 214

--- Document Chunk 314 ---
training set at hand, resulting in the validation loss
starting to increase.
introduced in Â§ 1.3 and illustrated in Figure 3.5.
Paradoxically, although they should suffer from
severe overfitting due to their capacity, large
Chunk length: 223

--- Document Chunk 315 ---
severe overfitting due to their capacity, large
models usually continue to improve as training
Chunk length: 94

--- Document Chunk 316 ---
models usually continue to improve as training
progresses. This may be due to the in in in in in in in in in in induc duc duc duc duc duc duc duc duc duc ductive tive tive tive tive tive tive tive tive tive tive
Chunk length: 211

--- Document Chunk 317 ---
bias bias bias bias bias bias bias bias bias bias biasof the model becoming the main driver of
optimization when performance is near perfect
49
Chunk length: 143

--- Document Chunk 318 ---
on the training set [Belkin et al., 2018].
An important design choice is the learn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ingrate rate rate rate rate rate rate rate rate rate rate
Chunk length: 239

--- Document Chunk 319 ---
sched sched sched sched sched sched sched sched sched sched schedule ule ule ule ule ule ule ule ule ule uleduring training, that is, the specifica-
Chunk length: 148

--- Document Chunk 320 ---
tion of the value of thelearn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ingrate rate rate rate rate rate rate rate rate rate rateat each iter-
ation of the gradient descent. The general policy
Chunk length: 249

--- Document Chunk 321 ---
ation of the gradient descent. The general policy
is that the learning rate should be initially large
to avoid having the optimization being trapped
in a bad local minimum early, and that it should
get smaller so that the optimized parameter val-
Chunk length: 246

--- Document Chunk 322 ---
get smaller so that the optimized parameter val-
ues do not bounce around and reach a good min-
imum in a narrow valley of the loss landscape.
The training of very large models may take
months on thousands of powerful GPUs and
Chunk length: 226

--- Document Chunk 323 ---
months on thousands of powerful GPUs and
have a financial cost of several million dollars. At
this scale, the training may involve many man-
ual interventions, informed, in particular, by the
dynamics of the loss evolution.
Fine-tuning
Chunk length: 235

--- Document Chunk 324 ---
dynamics of the loss evolution.
Fine-tuning
It is often beneficial to adapt an already trained
Chunk length: 94

--- Document Chunk 325 ---
model to a new task, referred to as adown down down down down down down down down down downstream stream stream stream stream stream stream stream stream stream stream
task task task task task task task task task task task.
Chunk length: 223

--- Document Chunk 326 ---
It can be because the amount of data for the
original task is plentiful, while they are lim-
ited for the downstream task, and the two tasks
share enough similarities that statistical struc-
50
Chunk length: 193

--- Document Chunk 327 ---
tures learned for the first provide a good induc-
tive bias for the second. It can also be to limit the
training cost by reusing the patterns encoded in
an existing model.
Chunk length: 171

--- Document Chunk 328 ---
an existing model.
Adapting a pre pre pre pre pre pre pre pre pre pre pre-trained trained trained trained trained trained trained trained trained trained trainedmodel model model model model model model model model model modelto a specific task
Chunk length: 244

--- Document Chunk 329 ---
is achieved withfine fine fine fine fine fine fine fine fine fine fine-tun tun tun tun tun tun tun tun tun tun tuning ing ing ing ing ing ing ing ing ing ing, which is a standard
training procedure for the downstream task, but
Chunk length: 226

--- Document Chunk 330 ---
training procedure for the downstream task, but
which starts from the pre-trained model instead
of using a random initialization.
This is the main strategy for most computer vi-
sion applications which generally use a model
Chunk length: 223

--- Document Chunk 331 ---
sion applications which generally use a model
pre-trained for classification on ImageNet [Deng
et al., 2009] (see Â§ 6.3 and Â§ 6.4), and it is also
Chunk length: 146

--- Document Chunk 332 ---
how purely generative pre-trained Large Large Large Large Large Large Large Large Large Large LargeLan Lan Lan Lan Lan Lan Lan Lan Lan Lan Lan-
Chunk length: 143

--- Document Chunk 333 ---
guage guage guage guage guage guage guage guage guage guage guageMod Mod Mod Mod Mod Mod Mod Mod Mod Mod Models els els els els els els els els els elsare re-purposed as assistant-like
models, able to produce interactive dialogues
(see Â§ 7.1).
Chunk length: 243

--- Document Chunk 334 ---
(see Â§ 7.1).
We come back to techniques to cope with lim-
ited resources in inference and for fine-tuning
in Chapter 8.
51
Chunk length: 122

--- Document Chunk 335 ---
3.7 The benefits of scale
There is an accumulation of empirical results
showing that performance, for instance, esti-
mated through the loss on test data, improves
with the amount of data according to remarkable
Chunk length: 211

--- Document Chunk 336 ---
with the amount of data according to remarkable
scal scal scal scal scal scal scal scal scal scal scaling ing ing ing ing ing ing ing ing ing inglaws laws laws laws laws laws laws laws laws laws laws, as long as the model size increases
Chunk length: 236

--- Document Chunk 337 ---
correspondingly [Kaplan et al., 2020] (see Figure
3.6).
Benefiting from these scaling laws in the multi-
billion sample regime is possible in part thanks to
the structure of deep models which can be scaled
Chunk length: 205

--- Document Chunk 338 ---
the structure of deep models which can be scaled
up arbitrarily, as we will see, by increasing the
number of layers or feature dimensions. But it
is also made possible by the distributed nature
of the computation they implement, and by the
Chunk length: 239

--- Document Chunk 339 ---
stochas stochas stochas stochas stochas stochas stochas stochas stochas stochas stochastic tic tic tic tic tic tic tic tic tic ticgra gra gra gra gra gra gra gra gra gra gradi di di di di di di di di di dient ent ent ent ent ent ent ent ent ent
Chunk length: 244

--- Document Chunk 340 ---
di di dient ent ent ent ent ent ent ent ent ent entde de de de de de de de de de descent scent scent scent scent scent scent scent scent scent scent, which requires only
Chunk length: 169

--- Document Chunk 341 ---
a fraction of the data at a time and can operate
with datasets whose size is orders of magnitude
greater than that of the computing deviceâ€™s mem-
ory. This has resulted in an exponential growth
of the models, as illustrated in Figure 3.7.
Chunk length: 238

--- Document Chunk 342 ---
of the models, as illustrated in Figure 3.7.
Typical vision models have10â€“100 million train train train train train train train train train train train-
Chunk length: 152

--- Document Chunk 343 ---
able able able able able able able able able able ablepa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e eters ters ters ters ters ters ters ters ters ters tersand require 1018â€“1019 FLOPs
Chunk length: 231

--- Document Chunk 344 ---
for training [He et al., 2015; Sevilla et al., 2022].
Language models have from 100 million to hun-
dreds of billions of trainable parameters and re-
52
Chunk length: 152

--- Document Chunk 345 ---
Test lossTest lossTest loss
Compute (peta-FLOP/s-day)
Dataset size (tokens)
Number of parameters
Figure 3.6: Test loss of a language model vs. the amount
of computation in petaflop/s-day, the dataset size in
Chunk length: 207

--- Document Chunk 346 ---
tokens, that is fragments of words, and the model size
in parameters [Kaplan et al., 2020].
53
Chunk length: 94

--- Document Chunk 347 ---
Dataset Year Nb. of images Size
ImageNet 2012 1.2M 150Gb
Cityscape 2016 25K 60Gb
LAION-5B 2022 5.8B 240Tb
Dataset Year Nb. of books Size
WMT-18-de-en 2018 14M 8Gb
The Pile 2020 1.6B 825Gb
OSCAR 2020 12B 6Tb
Chunk length: 206

--- Document Chunk 348 ---
The Pile 2020 1.6B 825Gb
OSCAR 2020 12B 6Tb
Table 3.1: Some examples of publicly available datasets.
The equivalent number of books is an indicative esti-
mate for 250 pages of 2000 characters per book.
Chunk length: 202

--- Document Chunk 349 ---
mate for 250 pages of 2000 characters per book.
quire 1020â€“1023 FLOPs for training [Devlin et al.,
2018; Brown et al., 2020; Chowdhery et al., 2022;
Sevilla et al., 2022]. These latter models require
machines with multiple high-end GPUs.
Chunk length: 237

--- Document Chunk 350 ---
machines with multiple high-end GPUs.
Training these large models is impossible using
datasets with a detailed ground-truth costly to
produce, which can only be of moderate size.
Instead, it is done with datasets automatically
Chunk length: 226

--- Document Chunk 351 ---
Instead, it is done with datasets automatically
produced by combining data available on the
internet with minimal curation, if any. These
sets may combine multiple modalities, such as
text and images from web pages, or sound and
Chunk length: 228

--- Document Chunk 352 ---
text and images from web pages, or sound and
images from videos, which can be used for large-
scale supervised training.
As of 2024, the most powerful models are the
54
Chunk length: 168

--- Document Chunk 353 ---
2015 2020
1018
1021
1024
1KWh
1MWh
1GWh
AlexNet
ResNet
AlphaGo
AlphaZero
Transformer
GPT
BERT
GPT-2
GPT-3
ViT
PaLM
LaMDA
Whisper
VGG16
GoogLeNet
CLIP-ViT
Year
Training cost (FLOP)
Figure 3.7: Training costs in number of FLOP of some
Chunk length: 232

--- Document Chunk 354 ---
landmark models [Sevilla et al., 2023]. The colors in-
dicate the domains of application: Computer Vision
(blue), Natural Language Processing (red), or other
(black). The dashed lines correspond to the energy con-
Chunk length: 213

--- Document Chunk 355 ---
sumption using A100s SXM in 16-bit precision. For
reference, the total electricity consumption in the US in
2021 was 3920TWh.
55
Chunk length: 128

--- Document Chunk 356 ---
so-called Large Large Large Large Large Large Large Large Large Large LargeLan Lan Lan Lan Lan Lan Lan Lan Lan Lan Language guage guage guage guage guage guage guage guage guage guageMod Mod Mod Mod Mod Mod Mod Mod Mod Mod Models els els els els els
Chunk length: 249

--- Document Chunk 357 ---
Mod Mod Mod Mod Mod Models els els els els els els els els els els(LLMs LLMs LLMs LLMs LLMs LLMs LLMs LLMs LLMs LLMs LLMs), which
Chunk length: 129

--- Document Chunk 358 ---
we will see in Â§ 5.3 and Â§ 7.1, trained on ex-
tremely large text datasets (see Table 3.1).
56
Chunk length: 94

--- Document Chunk 359 ---
Part II
Deep Models
57
Chunk length: 22

--- Document Chunk 360 ---
Chapter 4
Model Components
A deep model is nothing more than a complex
tensorial computation that can ultimately be
decomposed into standard mathematical oper-
ations from linear algebra and analysis. Over
Chunk length: 205

--- Document Chunk 361 ---
ations from linear algebra and analysis. Over
the years, the field has developed a large collec-
tion of high-level modules with a clear semantic,
and complex models combining these modules,
which have proven to be effective in specific ap-
Chunk length: 240

--- Document Chunk 362 ---
which have proven to be effective in specific ap-
plication domains.
Empirical evidence and theoretical results show
that greater performance is achieved with deeper
architectures, that is, long compositions of map-
Chunk length: 215

--- Document Chunk 363 ---
architectures, that is, long compositions of map-
pings. As we saw in section Â§ 3.4, training such
Chunk length: 98

--- Document Chunk 364 ---
a model is challenging due to the van van van van van van van van van van vanish ish ish ish ish ish ish ish ish ish ishing ing ing ing ing ing ing ing ing ing inggra gra gra gra gra gra gra gra gra gra gra-
Chunk length: 207

--- Document Chunk 365 ---
di di di di di di di di di di dient ent ent ent ent ent ent ent ent ent ent, and multiple important technical contri-
butions have mitigated this issue.
58
Chunk length: 155

--- Document Chunk 366 ---
4.1 The notion of layer
We call lay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ersstandard complex compounded
tensor operations that have been designed and
empirically identified as being generic and effi-
Chunk length: 241

--- Document Chunk 367 ---
empirically identified as being generic and effi-
cient. They often incorporate trainable param-
eters and correspond to a convenient level of
granularity for designing and describing large
deep models. The term is inherited from sim-
Chunk length: 234

--- Document Chunk 368 ---
deep models. The term is inherited from sim-
ple multi-layer neural networks, even though
modern models may take the form of a complex
graph of such modules, incorporating multiple
parallel pathways.
Ã—K
X
f
g n=4
Y
32Ã—32
4Ã—4
Chunk length: 224

--- Document Chunk 369 ---
parallel pathways.
Ã—K
X
f
g n=4
Y
32Ã—32
4Ã—4
In the following pages, I try to stick to the con-
vention for model depiction illustrated above:
â€¢ operators / layers are depicted as boxes,
â€¢ darker coloring indicates that they embed
Chunk length: 229

--- Document Chunk 370 ---
â€¢ darker coloring indicates that they embed
trainable parameters,
â€¢ non-default valued hyper-parameters are
59
Chunk length: 110

--- Document Chunk 371 ---
added in blue on their right,
â€¢ a dashed outer frame with a multiplicative
factor indicates that a group of layers is repli-
cated in series, each with its own set of trainable
parameters, if any, and
â€¢ in some cases, the dimension of their output is
Chunk length: 250

--- Document Chunk 372 ---
â€¢ in some cases, the dimension of their output is
specified on the right when it differs from their
input.
Additionally, layers that have a complex internal
structure are depicted with a greater height.
60
Chunk length: 205

--- Document Chunk 373 ---
4.2 Linear layers
The most important modules in terms of compu-
tation and number of parameters are the Lin Lin Lin Lin Lin Lin Lin Lin Lin Lin Linear ear ear ear ear ear ear ear ear ear ear
Chunk length: 190

--- Document Chunk 374 ---
lay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers. They benefit from decades of research
and engineering in algorithmic and chip design
for matrix operations.
Note that the term â€œlinearâ€ in deep learning gen-
Chunk length: 245

--- Document Chunk 375 ---
erally refers improperly to an affine affine affine affine affine affine affine affine affine affine affineop op op op op op op op op op oper er er er er er er er er er era a a a a a a a a a ation tion tion tion tion tion tion tion tion tion tion,
Chunk length: 247

--- Document Chunk 376 ---
which is the sum of a linear expression and a
constant bias.
Fully connected layers
Chunk length: 83

--- Document Chunk 377 ---
Fully connected layers
The most basic linear layer is thefully fully fully fully fully fully fully fully fully fully fullycon con con con con con con con con con connected nected nected nected nected nected nected nected nected nected nected
Chunk length: 241

--- Document Chunk 378 ---
layer layer layer layer layer layer layer layer layer layer layer, parameterized by a trainable weight weight weight weight weight weight weight weight weight weight weightma ma ma ma ma ma ma ma ma ma ma-
Chunk length: 205

--- Document Chunk 379 ---
trix trix trix trix trix trix trix trix trix trix trixW of size Dâ€² Ã—D and bias bias bias bias bias bias bias bias bias bias biasvec vec vec vec vec vec vec vec vec vec vector tor tor tor tor tor tor tor tor tor torb of dimen-
Chunk length: 225

--- Document Chunk 380 ---
sion Dâ€². It implements an affine transformation
generalized to arbitrary tensor shapes, where
the supplementary dimensions are interpreted
as vector indexes. Formally, given an input X
of dimension D1 Ã—Â·Â·Â·Ã— DK Ã—D, it computes an
Chunk length: 228

--- Document Chunk 381 ---
of dimension D1 Ã—Â·Â·Â·Ã— DK Ã—D, it computes an
output Y of dimension D1 Ã—Â·Â·Â·Ã— DK Ã—Dâ€² with
âˆ€d1,...,d K,
Y [d1,...,d K] =W X[d1,...,d K]+ b.
While at first sight such an affine operation
61
Chunk length: 184

--- Document Chunk 382 ---
seems limited to geometric transformations such
as rotations, symmetries, and translations, it can
in fact do more than that. In particular, projec-
tions for dimension reduction or signal filtering,
but also, from the perspective of the dot product
Chunk length: 249

--- Document Chunk 383 ---
but also, from the perspective of the dot product
being a measure of similarity, a matrix-vector
product can be interpreted as computing match-
ing scores between the queries, as encoded by
the input vectors, and keys, as encoded by the
matrix rows.
Chunk length: 249

--- Document Chunk 384 ---
matrix rows.
As we saw in Â§ 3.3, the gradient descent starts
Chunk length: 60

--- Document Chunk 385 ---
with the pa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e etersâ€™ tersâ€™ tersâ€™ tersâ€™ tersâ€™ tersâ€™ tersâ€™ tersâ€™ tersâ€™ tersâ€™ tersâ€™ran ran ran ran ran ran ran ran ran ran random dom dom dom dom dom dom dom dom
Chunk length: 248

--- Document Chunk 386 ---
ran ran random dom dom dom dom dom dom dom dom dom domini ini ini ini ini ini ini ini ini ini initial tial tial tial tial tial tial tial tial tial tializa iza iza iza iza iza iza iza iza iza ization tion tion tion tion tion tion tion tion tion tion
Chunk length: 248

--- Document Chunk 387 ---
. If
Chunk length: 4

--- Document Chunk 388 ---
this is done too naively, as seen in Â§ 3.4, the net-
work may suffer from exploding or vanishing
activations and gradients [Glorot and Bengio,
2010]. Deep learning frameworks implement ini-
tialization methods that in particular scale the
Chunk length: 238

--- Document Chunk 389 ---
tialization methods that in particular scale the
random parameters according to the dimension
of the input to keep the variance of the activa-
tions constant and prevent pathological behav-
iors.
Convolutional layers
Chunk length: 216

--- Document Chunk 390 ---
iors.
Convolutional layers
A linear layer can take as input an arbitrarily-
shaped tensor by reshaping it into a vector, as
long as it has the correct number of coefficients.
However, such a layer is poorly adapted to deal-
62
Chunk length: 226

--- Document Chunk 391 ---
Ï•
X
Y
Ïˆ
Y
X
Ï•
X
Y
Ïˆ
Y
X
Ï•
X
Y
Ïˆ
Y
X
...
1D convolution
...
1D transposed
convolution
Figure 4.1: A 1D convolution (left) takes as input
a DÃ—T tensor X, applies the same affine mapping
Ï•(Â·;w) to every sub-tensor of shape DÃ—K, and stores
Chunk length: 235

--- Document Chunk 392 ---
the resulting Dâ€² Ã—1 tensors into Y . A 1D transposed
convolution (right) takes as input a DÃ—T tensor, ap-
plies the same affine mapping Ïˆ(Â·;w) to every sub-
tensor of shape DÃ—1, and sums the shifted resulting
Chunk length: 208

--- Document Chunk 393 ---
Dâ€² Ã—K tensors. Both can process inputs of different
sizes.
63
Chunk length: 61

--- Document Chunk 394 ---
X
Y
Ï•
2D convolution
H
W
D
Y
X
Ïˆ
2D transposed
convolution
Figure 4.2: A 2D convolution (left) takes as input a
DÃ—H Ã—W tensor X, applies the same affine mapping
Ï•(Â·;w) to every sub-tensor of shape DÃ—K Ã—L, and
Chunk length: 208

--- Document Chunk 395 ---
Ï•(Â·;w) to every sub-tensor of shape DÃ—K Ã—L, and
stores the resulting Dâ€² Ã—1Ã—1 tensors into Y . A 2D
transposed convolution (right) takes as input a DÃ—
H Ã—W tensor, applies the same affine mapping Ïˆ(Â·;w)
to every DÃ—1Ã—1 sub-tensor, and sums the shifted
Chunk length: 249

--- Document Chunk 396 ---
to every DÃ—1Ã—1 sub-tensor, and sums the shifted
resulting Dâ€² Ã—K Ã—L tensors into Y .
ing with large tensors, since the number of pa-
rameters and number of operations are propor-
tional to the product of the input and output
Chunk length: 223

--- Document Chunk 397 ---
tional to the product of the input and output
dimensions. For instance, to process an RGB
image of size 256Ã—256 as input and compute a
result of the same size, it would require approxi-
mately 4Ã—1010 parameters and multiplications.
Chunk length: 231

--- Document Chunk 398 ---
mately 4Ã—1010 parameters and multiplications.
Besides these practical issues, most of the high-
dimension signals are strongly structured. For
64
Chunk length: 145

--- Document Chunk 399 ---
Ï•
Y
X
Ï•
Y
X
s = 2 ...
Stride
Ï•
Y
X
d = 2
Dilation
Ï•
Y
X
p = 2
Padding
Figure 4.3: Beside its kernel size and number of input
/ output channels, a convolution admits three hyper-
parameters: the stride s (left) modulates the step size
Chunk length: 233

--- Document Chunk 400 ---
when going through the input tensor, the padding p
(top right) specifies how many zero entries are added
around the input tensor before processing it, and the
dilation d (bottom right) parameterizes the index count
Chunk length: 214

--- Document Chunk 401 ---
between coefficients of the filter.
65
Chunk length: 38

--- Document Chunk 402 ---
instance, images exhibit short-term correlations
and statistical stationarity with respect to trans-
lation, scaling, and certain symmetries. This
Chunk length: 146

--- Document Chunk 403 ---
is not reflected in the in in in in in in in in in in induc duc duc duc duc duc duc duc duc duc ductive tive tive tive tive tive tive tive tive tive tivebias bias bias bias bias bias bias bias bias bias biasof a fully
Chunk length: 217

--- Document Chunk 404 ---
connected layer, which completely ignores the
signal structure.
To leverage these regularities, the tool of choice
Chunk length: 114

--- Document Chunk 405 ---
is con con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay layers ers ers ers ers
Chunk length: 248

--- Document Chunk 406 ---
lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers, which are also affine, but
Chunk length: 98

--- Document Chunk 407 ---
process time-series or 2D signals locally, with
the same operator everywhere.
Chunk length: 77

--- Document Chunk 408 ---
A 1D 1D 1D 1D 1D 1D 1D 1D 1D 1D 1Dcon con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lution tion tion tion tion tion tion tion tion tion tionis mainly defined by three hy hy hy hy hy hy hy hy hy
Chunk length: 248

--- Document Chunk 409 ---
defined by three hy hy hy hy hy hy hy hy hy hy hy-
Chunk length: 50

--- Document Chunk 410 ---
per per per per per per per per per per per-pa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e eters ters ters ters ters ters ters ters ters ters ters: its ker ker ker ker ker ker ker ker ker ker kernel
Chunk length: 246

--- Document Chunk 411 ---
ker ker ker ker ker ker ker ker ker ker kernel nel nel nel nel nel nel nel nel nel nelsize size size size size size size size size size sizeK, its number of
Chunk length: 156

--- Document Chunk 412 ---
input channels D, its number of output chan-
nels Dâ€², and by the trainable parameters w of an
affine mapping Ï•(Â·;w) :RDÃ—K â†’ RDâ€²Ã—1.
It can process any tensor X of size DÃ—T with
T â‰¥ K, and applies Ï•(Â·;w) to every sub-tensor
Chunk length: 221

--- Document Chunk 413 ---
T â‰¥ K, and applies Ï•(Â·;w) to every sub-tensor
of sizeDÃ—K of X, storing the results in a tensor
Y of size Dâ€² Ã—(T âˆ’K +1), as pictured in Figure
4.1 (left).
Chunk length: 153

--- Document Chunk 414 ---
4.1 (left).
A 2D 2D 2D 2D 2D 2D 2D 2D 2D 2D 2Dcon con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lution tion tion tion tion tion tion tion tion tion tionis similar but has aK Ã—L ker-
Chunk length: 236

--- Document Chunk 415 ---
nel and takes as input a DÃ—H Ã—W tensor (see
Figure 4.2, left).
Both operators have for trainable parameters
those of Ï• that can be envisioned as Dâ€² fil fil fil fil fil fil fil fil fil fil filters ters ters ters ters ters ters ters ters ters ters
66
Chunk length: 248

--- Document Chunk 416 ---
of size DÃ—K or DÃ—K Ã—L respectively, and a
bias bias bias bias bias bias bias bias bias bias biasvec vec vec vec vec vec vec vec vec vec vector tor tor tor tor tor tor tor tor tor tor of dimensionDâ€².
Chunk length: 198

--- Document Chunk 417 ---
Such a layer is equiv equiv equiv equiv equiv equiv equiv equiv equiv equiv equivari ari ari ari ari ari ari ari ari ari ariant ant ant ant ant ant ant ant ant ant antto translation, mean-
ing that if the input signal is translated, the out-
Chunk length: 241

--- Document Chunk 418 ---
put is similarly transformed. This property re-
Chunk length: 47

--- Document Chunk 419 ---
sults in a desirable in in in in in in in in in in induc duc duc duc duc duc duc duc duc duc ductive tive tive tive tive tive tive tive tive tive tivebias bias bias bias bias bias bias bias bias bias biaswhen dealing
Chunk length: 216

--- Document Chunk 420 ---
with a signal whose distribution is invariant to
translation.
Chunk length: 61

--- Document Chunk 421 ---
translation.
They also admit three additional hy hy hy hy hy hy hy hy hy hy hyper per per per per per per per per per per-pa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e e-
Chunk length: 219

--- Document Chunk 422 ---
ters ters ters ters ters ters ters ters ters ters ters, illustrated on Figure 4.3:
â€¢ The padding padding padding padding padding padding padding padding padding padding paddingspecifies how many zero coeffi-
Chunk length: 207

--- Document Chunk 423 ---
cients should be added around the input tensor
before processing it, particularly to maintain the
tensor size when the kernel size is greater than
one. Its default value is 0.
Chunk length: 175

--- Document Chunk 424 ---
one. Its default value is 0.
â€¢ The stride stride stride stride stride stride stride stride stride stride stridespecifies the step size used when go-
ing through the input, allowing one to reduce the
output size geometrically by using large steps.
Chunk length: 246

--- Document Chunk 425 ---
output size geometrically by using large steps.
Its default value is 1.
â€¢ The di di di di di di di di di di dila la la la la la la la la la lation tion tion tion tion tion tion tion tion tion tionspecifies the index count between
Chunk length: 229

--- Document Chunk 426 ---
the filter coefficients of the local affine opera-
tor. Its default value is 1, and greater values
correspond to inserting zeros between the coef-
ficients, which increases the filter / kernel size
while keeping the number of trainable parame-
67
Chunk length: 246

--- Document Chunk 427 ---
H
W
Model depth
Figure 4.4: Given an activation in a series of convolu-
Chunk length: 71

--- Document Chunk 428 ---
tion layers, here in red, its re re re re re re re re re re recep cep cep cep cep cep cep cep cep cep ceptive tive tive tive tive tive tive tive tive tive tivefield field field field field field field field field field fieldis the area in
Chunk length: 238

--- Document Chunk 429 ---
the input signal, in blue, that modulates its value. Each
intermediate convolutional layer increases the width
and height of that area by roughly those of the kernel.
ters unchanged.
Except for the number of channels, a convo-
Chunk length: 226

--- Document Chunk 430 ---
Except for the number of channels, a convo-
lutionâ€™s output is usually smaller than its in-
put. In the 1D case without padding nor di-
lation, if the input is of size T, the kernel of
size K, and the stride is S, the output is of size
Chunk length: 235

--- Document Chunk 431 ---
Tâ€² = (T âˆ’K)/S +1.
Given an activation computed by a convolutional
layer, or the vector of values for all the channels
at a certain location, the portion of the input
68
Chunk length: 168

--- Document Chunk 432 ---
signal that it depends on is called its re re re re re re re re re re recep cep cep cep cep cep cep cep cep cep ceptive tive tive tive tive tive tive tive tive tive tive
Chunk length: 169

--- Document Chunk 433 ---
field field field field field field field field field field field(see Figure 4.4). One of the H Ã—W sub-
tensors corresponding to a single channel of a
Chunk length: 150

--- Document Chunk 434 ---
tensors corresponding to a single channel of a
DÃ—H Ã—W activation tensor is called an ac ac ac ac ac ac ac ac ac ac acti ti ti ti ti ti ti ti ti ti tiva va va va va va va va va va va-
Chunk length: 182

--- Document Chunk 435 ---
tion tion tion tion tion tion tion tion tion tion tionmap map map map map map map map map map map.
Convolutions are used to recombine information,
generally to reduce the spatial size of the rep-
resentation, in exchange for a greater number
Chunk length: 241

--- Document Chunk 436 ---
resentation, in exchange for a greater number
of channels, which translates into a richer local
representation. They can implement differential
operators such as edge-detectors, or template
matching mechanisms. A succession of such lay-
Chunk length: 236

--- Document Chunk 437 ---
matching mechanisms. A succession of such lay-
ers can also be envisioned as a compositional and
hierarchical representation [Zeiler and Fergus,
2014], or as a diffusion process in which infor-
mation can be transported by half the kernel size
Chunk length: 243

--- Document Chunk 438 ---
mation can be transported by half the kernel size
when passing through a layer.
Chunk length: 79

--- Document Chunk 439 ---
A converse operation is the trans trans trans trans trans trans trans trans trans trans transposed posed posed posed posed posed posed posed posed posed posedcon con con con con con con con con con convo vo vo vo vo vo vo vo vo vo vo-
Chunk length: 234

--- Document Chunk 440 ---
lu lu lu lu lu lu lu lu lu lu lution tion tion tion tion tion tion tion tion tion tionthat also consists of a localized affine op-
erator, defined by similar hyper and trainable
parameters as the convolution, but which, for
Chunk length: 223

--- Document Chunk 441 ---
parameters as the convolution, but which, for
instance, in the 1D case, applies an affine map-
ping Ïˆ(Â·;w) :RDÃ—1 â†’ RDâ€²Ã—K, to every DÃ—1
sub-tensor of the input, and sums the shifted
Dâ€² Ã—K resulting tensors to compute its output.
Chunk length: 227

--- Document Chunk 442 ---
Dâ€² Ã—K resulting tensors to compute its output.
Such an operator increases the size of the signal
and can be understood intuitively as a synthe-
69
Chunk length: 146

--- Document Chunk 443 ---
sis process (see Figure 4.1, right, and Figure 4.2,
right).
A series of convolutional layers is the usual ar-
chitecture for mapping a large-dimension signal,
such as an image or a sound sample, to a low-
Chunk length: 204

--- Document Chunk 444 ---
such as an image or a sound sample, to a low-
dimension tensor. This can be used, for instance,
to get class scores for classification or a com-
pressed representation. Transposed convolution
layers are used the opposite way to build a large-
Chunk length: 242

--- Document Chunk 445 ---
dimension signal from a compressed representa-
tion, either to assess that the compressed repre-
sentation contains enough information to recon-
struct the signal or for synthesis, as it is easier
to learn a density model over a low-dimension
Chunk length: 242

--- Document Chunk 446 ---
to learn a density model over a low-dimension
representation. We will revisit this in Â§ 5.2.
70
Chunk length: 95

--- Document Chunk 447 ---
4.3 Activation functions
If a network were combining only linear com-
ponents, it would itself be a linear operator,
Chunk length: 116

--- Document Chunk 448 ---
so it is essential to have non non non non non non non non non non non-lin lin lin lin lin lin lin lin lin lin linear ear ear ear ear ear ear ear ear ear earop op op op op op op op op op oper er er er er er er er er er era a a a a a a a a a ations
Chunk length: 247

--- Document Chunk 449 ---
er er er er er er er era a a a a a a a a a ations tions tions tions tions tions tions tions tions tions tions
Chunk length: 109

--- Document Chunk 450 ---
.
Chunk length: 1

--- Document Chunk 451 ---
These are implemented in particular withac ac ac ac ac ac ac ac ac ac acti ti ti ti ti ti ti ti ti ti tiva va va va va va va va va va va-
Chunk length: 137

--- Document Chunk 452 ---
tion tion tion tion tion tion tion tion tion tion tionfunc func func func func func func func func func functions tions tions tions tions tions tions tions tions tions tions, which are layers that transform
Chunk length: 206

--- Document Chunk 453 ---
each component of the input tensor individually
through a mapping, resulting in a tensor of the
same shape.
There are many different activation functions,
Chunk length: 154

--- Document Chunk 454 ---
but the most used is the Rec Rec Rec Rec Rec Rec Rec Rec Rec Rec Recti ti ti ti ti ti ti ti ti ti tified fied fied fied fied fied fied fied fied fied fiedLin Lin Lin Lin Lin Lin Lin Lin Lin Lin Linear ear ear ear ear ear ear ear ear ear earUnit Unit
Chunk length: 249

--- Document Chunk 455 ---
ear ear ear ear ear ear ear ear ear earUnit Unit Unit Unit Unit Unit Unit Unit Unit Unit Unit
Chunk length: 93

--- Document Chunk 456 ---
(ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU) [Glorot et al., 2011], which sets nega-
tive values to zero and keeps positive values
unchanged (see Figure 4.5, top right):
relu(x) =
(
0 if x <0,
x otherwise.
Chunk length: 217

--- Document Chunk 457 ---
relu(x) =
(
0 if x <0,
x otherwise.
Given that the core training strategy of deep-
learning relies on the gradient, it may seem prob-
lematic to have a mapping that is not differen-
tiable at zero and constant on half the real line.
Chunk length: 232

--- Document Chunk 458 ---
However, the main property gradient descent
requires is that the gradient is informative on
average. Parameter initialization and data nor-
malization make half of the activations positive
71
Chunk length: 191

--- Document Chunk 459 ---
Tanh ReLU
Leaky ReLU GELU
Figure 4.5: Activation functions.
when the training starts, ensuring that this is the
case.
Before the generalization of ReLU, the standard
Chunk length: 165

--- Document Chunk 460 ---
activation function was the hy hy hy hy hy hy hy hy hy hy hyper per per per per per per per per per perbolic bolic bolic bolic bolic bolic bolic bolic bolic bolic bolictan tan tan tan tan tan tan tan tan tan tangent gent gent gent gent gent gent
Chunk length: 245

--- Document Chunk 461 ---
tan tan tan tangent gent gent gent gent gent gent gent gent gent gent
Chunk length: 69

--- Document Chunk 462 ---
(Tanh Tanh Tanh Tanh Tanh Tanh Tanh Tanh Tanh Tanh Tanh, see Figure 4.5, top left) which saturates
exponentially fast on both the negative and pos-
itive sides, aggravating the vanishing gradient.
Other popular activation functions follow the
Chunk length: 242

--- Document Chunk 463 ---
Other popular activation functions follow the
same idea of keeping positive values unchanged
and squashing the negative values. Leaky Leaky Leaky Leaky Leaky Leaky Leaky Leaky Leaky Leaky LeakyReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU
Chunk length: 247

--- Document Chunk 464 ---
[Maas et al., 2013] applies a small positive multi-
72
Chunk length: 54

--- Document Chunk 465 ---
plying factor to the negative values (see Figure
4.5, bottom left):
leakyrelu(x) =
(
ax if x <0,
x otherwise.
And GELU GELU GELU GELU GELU GELU GELU GELU GELU GELU GELU[Hendrycks and Gimpel, 2016] is de-
Chunk length: 203

--- Document Chunk 466 ---
fined using the cumulative distribution function
of the Gaussian distribution, that is:
gelu(x) =xP(Z â‰¤ x),
where Z âˆ¼ ð’© (0,1). It roughly behaves like a
smooth ReLU (see Figure 4.5, bottom right).
The choice of an activation function, in partic-
Chunk length: 245

--- Document Chunk 467 ---
The choice of an activation function, in partic-
ular among the variants of ReLU, is generally
driven by empirical performance.
73
Chunk length: 130

--- Document Chunk 468 ---
4.4 Pooling
A classical strategy to reduce the signal size is to
use a pool pool pool pool pool pool pool pool pool pool pooling ing ing ing ing ing ing ing ing ing ing operation that combines multiple
activations into one that ideally summarizes the
Chunk length: 250

--- Document Chunk 469 ---
activations into one that ideally summarizes the
information. The most standard operation of this
Chunk length: 97

--- Document Chunk 470 ---
information. The most standard operation of this
class is the max max max max max max max max max max maxpool pool pool pool pool pool pool pool pool pool pooling ing ing ing ing ing ing ing ing ing inglayer, which, similarly
Chunk length: 225

--- Document Chunk 471 ---
to convolution, can operate in 1D and 2D and is
defined by a ker ker ker ker ker ker ker ker ker ker kernel nel nel nel nel nel nel nel nel nel nelsize size size size size size size size size size size.
In its standard form, this layer computes the
Chunk length: 248

--- Document Chunk 472 ---
In its standard form, this layer computes the
maximum activation per channel, over non-
overlapping sub-tensors of spatial size equal to
the kernel size. These values are stored in a re-
sult tensor with the same number of channels
Chunk length: 231

--- Document Chunk 473 ---
sult tensor with the same number of channels
as the input, and whose spatial size is divided
by the kernel size. As with the convolution, this
Chunk length: 142

--- Document Chunk 474 ---
operator has three hy hy hy hy hy hy hy hy hy hy hyper per per per per per per per per per per-pa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e eters ters ters ters ters ters ters ters ters ters ters:
Chunk length: 246

--- Document Chunk 475 ---
ters ters ters ters ters ters ters ters ters: padding padding padding padding padding padding padding padding padding padding padding,
Chunk length: 134

--- Document Chunk 476 ---
stride stride stride stride stride stride stride stride stride stride stride, and di di di di di di di di di di dila la la la la la la la la la lation tion tion tion tion tion tion tion tion tion tion, with the stride being equal
Chunk length: 229

--- Document Chunk 477 ---
to the kernel size by default. A smaller stride
results in a larger resulting tensor, following the
same formula as for convolutions (see Â§ 4.2).
The max operation can be intuitively interpreted
as a logical disjunction, or, when it follows a
Chunk length: 242

--- Document Chunk 478 ---
series of con con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay layers ers ers
Chunk length: 247

--- Document Chunk 479 ---
lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ersthat compute lo-
Chunk length: 94

--- Document Chunk 480 ---
cal scores for the presence of parts, as a way
of encoding that at least one instance of a part
is present. It loses precise location, making it
74
Chunk length: 147

--- Document Chunk 481 ---
Y
X
max
Y
X
max
Y
X
max
...
1D max pooling
Figure 4.6: A 1D max pooling takes as input a DÃ—T
tensor X, computes the max over non-overlapping 1Ã—
L sub-tensors (in blue) and stores the resulting values
(in red) in a DÃ—(T/L) tensor Y .
75
Chunk length: 235

--- Document Chunk 482 ---
in in in in in in in in in in invari vari vari vari vari vari vari vari vari vari variant ant ant ant ant ant ant ant ant ant ant to local deformations.
Chunk length: 152

--- Document Chunk 483 ---
A standard alternative is the av av av av av av av av av av aver er er er er er er er er er erage age age age age age age age age age agepool pool pool pool pool pool pool pool pool pool pooling ing ing ing ing ing ing ing ing ing ing
Chunk length: 234

--- Document Chunk 484 ---
layer that computes the average instead of the
maximum over the sub-tensors. This is a linear
operation, whereas max pooling is not.
76
Chunk length: 135

--- Document Chunk 485 ---
4.5 Dropout
Some layers have been designed to explicitly
facilitate training or improve the learned repre-
sentations.
One of the main contributions of that sort was
Chunk length: 165

--- Document Chunk 486 ---
One of the main contributions of that sort was
dropout dropout dropout dropout dropout dropout dropout dropout dropout dropout dropout[Srivastava et al., 2014]. Such a layer
has no trainable parameters, but one hyper-
Chunk length: 217

--- Document Chunk 487 ---
has no trainable parameters, but one hyper-
parameter, p, and takes as input a tensor of arbi-
trary shape.
It is usually switched off during testing, in which
case its output is equal to its input. When it is ac-
Chunk length: 213

--- Document Chunk 488 ---
tive, it has a probability p of setting to zero each
activation of the input tensor independently, and
it re-scales all the activations by a factor of 1
1âˆ’p
to maintain the expected value unchanged (see
Figure 4.7).
Chunk length: 215

--- Document Chunk 489 ---
Figure 4.7).
The motivation behind dropout is to favor
meaningful individual activation and discourage
group representation. Since the probability that
a group of k activations remains intact through
a dropout layer is (1âˆ’p)k, joint representations
Chunk length: 248

--- Document Chunk 490 ---
a dropout layer is (1âˆ’p)k, joint representations
become unreliable, making the training proce-
dure avoid them. It can also be seen as a noise
injection that makes the training more robust.
When dealing with images and 2D tensors, the
77
Chunk length: 237

--- Document Chunk 491 ---
11111
11111
11111
11111
11111
11111
11111
11111
11111
1111100
0
00
00 00
00
Ã— Ã— 1
1âˆ’p
Train Test
Y
X
Y
X
Figure 4.7: Dropout can process a tensor of arbitrary
shape. During training (left), it sets activations at ran-
Chunk length: 217

--- Document Chunk 492 ---
dom to zero with probability p and applies a multiply-
ing factor to keep the expected values unchanged. Dur-
ing test (right), it keeps all the activations unchanged.
short-term correlation of the signals and the re-
Chunk length: 217

--- Document Chunk 493 ---
short-term correlation of the signals and the re-
sulting redundancy negate the effect of dropout,
since activations set to zero can be inferred from
their neighbors. Hence, dropout for 2D tensors
sets entire channels to zero instead of individual
Chunk length: 247

--- Document Chunk 494 ---
activations (see Figure 4.8).
Although dropout is generally used to improve
training and is inactive during inference, it can
be used in certain setups as a randomization
strategy, for instance, to estimate empirically
Chunk length: 218

--- Document Chunk 495 ---
strategy, for instance, to estimate empirically
confidence scores [Gal and Ghahramani, 2015].
78
Chunk length: 96

--- Document Chunk 496 ---
B
D
H,W
1 1 0 1 0 0 1Ã— Ã— 1
1âˆ’p
Train Test
Figure 4.8: 2D signals such as images generally exhibit
strong short-term correlation and individual activa-
tions can be inferred from their neighbors. This redun-
Chunk length: 206

--- Document Chunk 497 ---
dancy nullifies the effect of the standard unstructured
dropout, so the usual dropout layer for 2D tensors drops
entire channels instead of individual values.
79
Chunk length: 161

--- Document Chunk 498 ---
4.6 Normalizing layers
An important class of operators to facilitate the
training of deep architectures are the nor nor nor nor nor nor nor nor nor nor normal mal mal mal mal mal mal mal mal mal maliz iz iz iz iz iz iz iz iz iz iz-
Chunk length: 231

--- Document Chunk 499 ---
ing ing ing ing ing ing ing ing ing ing inglay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers, which force the empirical mean and
variance of groups of activations.
Chunk length: 200

--- Document Chunk 500 ---
variance of groups of activations.
The main layer in that family is batch batch batch batch batch batch batch batch batch batch batchnor nor nor nor nor nor nor nor nor nor normal mal mal mal mal mal mal mal mal mal mal-
Chunk length: 220

--- Document Chunk 501 ---
iza iza iza iza iza iza iza iza iza iza ization tion tion tion tion tion tion tion tion tion tion[Ioffe and Szegedy, 2015], which is the
only standard layer to process batches instead
of individual samples. It is parameterized by a
Chunk length: 231

--- Document Chunk 502 ---
of individual samples. It is parameterized by a
hyper-parameter D and two series of trainable
scalar parameters Î²1,...,Î² D and Î³1,...,Î³ D.
Given a batch of B samples x1,...,x B of dimen-
sion D, it first computes for each of the D com-
Chunk length: 235

--- Document Chunk 503 ---
sion D, it first computes for each of the D com-
ponents an empirical mean Ë†md and variance Ë†vd
across the batch:
Ë†md = 1
B
BX
b=1
xb,d
Ë†vd = 1
B
BX
b=1
(xb,d âˆ’ Ë†md)2,
from which it computes for every component
Chunk length: 210

--- Document Chunk 504 ---
from which it computes for every component
xb,d a normalized value zb,d, with empirical
mean 0 and variance 1, and from it the final
result value yb,d with mean Î²d and standard de-
80
Chunk length: 183

--- Document Chunk 505 ---
B
D
H,W
(Â· âˆ’Ë†md)/âˆšË†vd +Ïµ
Î³d Â· +Î²d
(Â· âˆ’Ë†mb)/âˆšË†vb +Ïµ
Î³d,h,w Â· +Î²d,h,w
batchnorm layernorm
Figure 4.9: Batch normalization (left) normalizes in
mean and variance each group of activations for a
given d, and scales/shifts that same group of activation
Chunk length: 247

--- Document Chunk 506 ---
with learned parameters for each d. Layer normaliza-
tion (right) normalizes each group of activations for a
certain b, and scales/shifts each group of activations
for a given d,h,w with learned parameters indexed by
the same.
81
Chunk length: 229

--- Document Chunk 507 ---
viation Î³d:
âˆ€b, z b,d = xb,d âˆ’ Ë†mdâˆšË†vd +Ïµ
yb,d = Î³dzb,d +Î²d.
Because this normalization is defined across a
batch, it is done only during training. During
testing, the layer transforms individual samples
Chunk length: 203

--- Document Chunk 508 ---
testing, the layer transforms individual samples
according to the Ë†mds and Ë†vds estimated with a
moving average over the full training set, which
boils down to a fixed affine transformation per
component.
Chunk length: 204

--- Document Chunk 509 ---
component.
The motivation behind batch normalization was
to avoid that a change in scaling in an early layer
of the network during training impacts all the
layers that follow, which then have to adapt their
Chunk length: 206

--- Document Chunk 510 ---
trainable parameters accordingly. Although the
actual mode of action may be more complicated
than this initial motivation, this layer consider-
ably facilitates the training of deep models.
In the case of 2D tensors, to follow the prin-
Chunk length: 236

--- Document Chunk 511 ---
In the case of 2D tensors, to follow the prin-
ciple of convolutional layers of processing all
locations similarly, the normalization is done
per-channel across all 2D positions, and Î² and
Î³ remain vectors of dimension D so that the
Chunk length: 232

--- Document Chunk 512 ---
Î³ remain vectors of dimension D so that the
scaling/shift does not depend on the 2D posi-
tion. Hence, if the tensor to be processed is
82
Chunk length: 138

--- Document Chunk 513 ---
of shape B Ã—DÃ—H Ã—W, the layer computes
( Ë†md,Ë†vd), for d = 1,...,D from the correspond-
ing B Ã—H Ã—W slice, normalizes it accordingly,
and finally scales and shifts its components with
the trainable parameters Î²d and Î³d.
Chunk length: 219

--- Document Chunk 514 ---
the trainable parameters Î²d and Î³d.
So, given a B Ã—D tensor, batch normalization
normalizes it across b and scales/shifts it ac-
cording to d, which can be implemented as a
component-wise product by Î³ and a sum with
Chunk length: 215

--- Document Chunk 515 ---
component-wise product by Î³ and a sum with
Î². Given a B Ã—DÃ—H Ã—W tensor, it normal-
izes across b,h,w and scales/shifts according to
d (see Figure 4.9, left).
This can be generalized depending on these di-
Chunk length: 204

--- Document Chunk 516 ---
mensions
Chunk length: 8

--- Document Chunk 517 ---
. For instance, layer layer layer layer layer layer layer layer layer layer layernor nor nor nor nor nor nor nor nor nor normal mal mal mal mal mal mal mal mal mal maliza iza iza iza iza iza iza iza iza iza ization tion tion tion tion tion tion tion
Chunk length: 249

--- Document Chunk 518 ---
iza ization tion tion tion tion tion tion tion tion tion tion[Ba
Chunk length: 64

--- Document Chunk 519 ---
et al., 2016] computes moments and normalizes
across all components of individual samples, and
scales and shifts components individually (see
Figure 4.9, right). So, given a B Ã—D tensor, it
normalizes across d and scales/shifts also accord-
Chunk length: 240

--- Document Chunk 520 ---
ing to the same. Given a B Ã—DÃ—H Ã—W tensor,
it normalizes it across d,h,w and scales/shifts
according to the same.
Contrary to batch normalization, since it pro-
cesses samples individually, layer normalization
Chunk length: 209

--- Document Chunk 521 ---
cesses samples individually, layer normalization
behaves the same during training and testing.
83
Chunk length: 97

--- Document Chunk 522 ---
4.7 Skip connections
Another technique that mitigates the vanishing
gradient and allows the training of deep archi-
Chunk length: 115

--- Document Chunk 523 ---
tectures are skip skip skip skip skip skip skip skip skip skip skipcon con con con con con con con con con connec nec nec nec nec nec nec nec nec nec nections tions tions tions tions tions tions tions tions tions tions[Long et al., 2014;
Chunk length: 237

--- Document Chunk 524 ---
Ronneberger et al., 2015]. They are not layers
per se, but an architectural design in which out-
puts of some layers are transported as-is to other
layers further in the model, bypassing process-
ing in between. This unmodified signal can be
Chunk length: 241

--- Document Chunk 525 ---
ing in between. This unmodified signal can be
concatenated or added to the input of the layer
the connection branches into (see Figure 4.10). A
Chunk length: 143

--- Document Chunk 526 ---
the connection branches into (see Figure 4.10). A
particular type of skip connections are the resid resid resid resid resid resid resid resid resid resid resid-
Chunk length: 160

--- Document Chunk 527 ---
ual ual ual ual ual ual ual ual ual ual ualcon con con con con con con con con con connec nec nec nec nec nec nec nec nec nec nections tions tions tions tions tions tions tions tions tions tions which combine the signal with
Chunk length: 224

--- Document Chunk 528 ---
a sum, and usually skip only a few layers (see
Figure 4.10, right).
The most desirable property of this design is to
ensure that, even in the case of gradient-killing
processing at a certain stage, the gradient will
Chunk length: 215

--- Document Chunk 529 ---
processing at a certain stage, the gradient will
still propagate through the skip connections.
Residual connections, in particular, allow for the
building of deep models with up to several hun-
Chunk length: 193

--- Document Chunk 530 ---
building of deep models with up to several hun-
dred layers, and key models, such as theresid resid resid resid resid resid resid resid resid resid residual ual ual ual ual ual ual ual ual ual ual
Chunk length: 196

--- Document Chunk 531 ---
net net net net net net net net net net networks works works works works works works works works works works[He et al., 2015] in computer vision
Chunk length: 144

--- Document Chunk 532 ---
(see Â§ 5.2), and the Trans Trans Trans Trans Trans Trans Trans Trans Trans Trans Transform form form form form form form form form form formers ers ers ers ers ers ers ers ers ers ers[Vaswani et al.,
2017] in natural language processing (see Â§ 5.3),
Chunk length: 249

--- Document Chunk 533 ---
2017] in natural language processing (see Â§ 5.3),
are entirely composed of blocks of layers with
residual connections.
84
Chunk length: 121

--- Document Chunk 534 ---
Â·Â·Â·
f(1)
f(2)
f(3)
f(4)
f(5)
f(6)
f(7)
f(8)
Â·Â·Â·
Â·Â·Â·
f(1)
f(2)
f(3)
f(4)
f(5)
f(6)
Â·Â·Â·
Â·Â·Â·
f(1)
f(2)
+
f(3)
f(4)
+
Â·Â·Â·
Figure 4.10: Skip connections, highlighted in red on this
figure, transport the signal unchanged across multiple
Chunk length: 230

--- Document Chunk 535 ---
layers. Some architectures (center) that downscale and
re-upscale the representation size to operate at multiple
scales, have skip connections to feed outputs from the
early parts of the network to later layers operating at
Chunk length: 223

--- Document Chunk 536 ---
the same scales [Long et al., 2014; Ronneberger et al.,
2015]. The residual connections (right) are a special
type of skip connections that sum the original signal
to the transformed one, and usually bypass at most a
Chunk length: 216

--- Document Chunk 537 ---
handful of layers [He et al., 2015].
85
Chunk length: 39

--- Document Chunk 538 ---
Their role can also be to facilitate multi-scale rea-
soning in models that reduce the signal size be-
fore re-expanding it, by connecting layers with
Chunk length: 150

--- Document Chunk 539 ---
fore re-expanding it, by connecting layers with
compatible sizes, for instance for se se se se se se se se se se seman man man man man man man man man man mantic tic tic tic tic tic tic tic tic tic ticseg seg seg seg seg seg seg seg seg seg seg-
Chunk length: 245

--- Document Chunk 540 ---
men men men men men men men men men men menta ta ta ta ta ta ta ta ta ta tation tion tion tion tion tion tion tion tion tion tion(see Â§ 6.4). In the case of residual
connections, they may also facilitate learning
Chunk length: 212

--- Document Chunk 541 ---
connections, they may also facilitate learning
by simplifying the task to finding a differential
improvement instead of a full update.
86
Chunk length: 137

--- Document Chunk 542 ---
4.8 Attention layers
In many applications, there is a need for an op-
eration able to combine local information at lo-
cations far apart in a tensor. For instance, this
could be distant details for coherent and realistic
Chunk length: 220

--- Document Chunk 543 ---
im im im im im im im im im im image age age age age age age age age age agesyn syn syn syn syn syn syn syn syn syn synthe the the the the the the the the the thesis sis sis sis sis sis sis sis sis sis sis, or words at different positions
Chunk length: 237

--- Document Chunk 544 ---
in a paragraph to make a grammatical or seman-
Chunk length: 46

--- Document Chunk 545 ---
tic decision in Nat Nat Nat Nat Nat Nat Nat Nat Nat Nat Natu u u u u u u u u u ural ral ral ral ral ral ral ral ral ral ralLan Lan Lan Lan Lan Lan Lan Lan Lan Lan Language guage guage guage guage guage guage guage guage guage guagePro Pro Pro Pro
Chunk length: 246

--- Document Chunk 546 ---
guage guage guage guage guagePro Pro Pro Pro Pro Pro Pro Pro Pro Pro Process cess cess cess cess cess cess cess cess cess cessing ing ing ing ing ing ing ing ing ing ing
Chunk length: 169

--- Document Chunk 547 ---
.
Chunk length: 1

--- Document Chunk 548 ---
Fully Fully Fully Fully Fully Fully Fully Fully Fully Fully Fullycon con con con con con con con con con connected nected nected nected nected nected nected nected nected nected nectedlay lay lay lay lay lay lay lay lay lay layers ers ers ers ers
Chunk length: 246

--- Document Chunk 549 ---
lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers erscannot process large-
Chunk length: 91

--- Document Chunk 550 ---
dimension signals, nor signals of variable size,
Chunk length: 48

--- Document Chunk 551 ---
and con con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay layers ers ers ers ers
Chunk length: 249

--- Document Chunk 552 ---
lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ersare not able to prop-
Chunk length: 91

--- Document Chunk 553 ---
agate information quickly. Strategies that ag-
gregate the results of convolutions, for instance,
by averaging them over large spatial areas, suf-
fer from mixing multiple signals into a limited
number of dimensions.
Chunk length: 216

--- Document Chunk 554 ---
At At At At At At At At At At Atten ten ten ten ten ten ten ten ten ten tention tion tion tion tion tion tion tion tion tion tionlay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ersspecifically address this prob-
Chunk length: 246

--- Document Chunk 555 ---
lem by computing an attention score for each
component of the resulting tensor to each com-
ponent of the input tensor, without locality con-
straints, and averaging the features across the
full tensor accordingly [Vaswani et al., 2017].
Chunk length: 237

--- Document Chunk 556 ---
full tensor accordingly [Vaswani et al., 2017].
Even though they are substantially more com-
plicated than other layers, they have become a
standard element in many recent models. They
Chunk length: 184

--- Document Chunk 557 ---
standard element in many recent models. They
are, in particular, the key building block ofTrans Trans Trans Trans Trans Trans Trans Trans Trans Trans Trans-
87
Chunk length: 159

--- Document Chunk 558 ---
K
Q
V YA A
Computes Aq,1,...,A q,NKV Computes Yq
Figure 4.11: The attention operator can be inter-
preted as matching every query Qq with all the
keys K1,...,K NKVto get normalized attention scores
Chunk length: 197

--- Document Chunk 559 ---
Aq,1,...,A q,NKV(left, and Equation 4.1), and then av-
eraging the values V1,...,V NKVwith these scores to
compute the resulting Yq (right, and Equation 4.2).
Chunk length: 158

--- Document Chunk 560 ---
form form form form form form form form form form formers ers ers ers ers ers ers ers ers ers ers, the dominant architecture for Large Large Large Large Large Large Large Large Large Large Large
Chunk length: 194

--- Document Chunk 561 ---
Lan Lan Lan Lan Lan Lan Lan Lan Lan Lan Language guage guage guage guage guage guage guage guage guage guageMod Mod Mod Mod Mod Mod Mod Mod Mod Mod Models els els els els els els els els els els. See Â§ 5.3 and Â§ 7.1.
Attention operator
Given
Chunk length: 241

--- Document Chunk 562 ---
Attention operator
Given
â€¢ a tensor Q of queries queries queries queries queries queries queries queries queries queries queries of sizeNQÃ—DQK,
â€¢ a tensor K of keys keys keys keys keys keys keys keys keys keys keys of sizeNKVÃ—DQK, and
Chunk length: 234

--- Document Chunk 563 ---
â€¢ a tensor V of val val val val val val val val val val values ues ues ues ues ues ues ues ues ues ues of sizeNKVÃ—DV,
Chunk length: 117

--- Document Chunk 564 ---
the at at at at at at at at at at atten ten ten ten ten ten ten ten ten ten tention tion tion tion tion tion tion tion tion tion tionop op op op op op op op op op oper er er er er er er er er er era a a a a a a a a a ator tor tor tor tor tor tor tor
Chunk length: 249

--- Document Chunk 565 ---
a a a a a a a a ator tor tor tor tor tor tor tor tor tor tor computes a tensor
Chunk length: 78

--- Document Chunk 566 ---
Y = att(Q,K,V )
of dimension NQÃ—DV. To do so, it first com-
putes for every query index q and every key in-
88
Chunk length: 110

--- Document Chunk 567 ---
dex k an attention score Aq,k as the sof sof sof sof sof sof sof sof sof sof soft t t t t t t t t t targmax argmax argmax argmax argmax argmax argmax argmax argmax argmax argmax
of the dot products between the query Qq and
the keys:
Aq,k =
exp

1âˆš
Chunk length: 248

--- Document Chunk 568 ---
the keys:
Aq,k =
exp

1âˆš
DQKQqÂ·Kk

P
l exp

1âˆš
DQKQqÂ·Kl
, (4.1)
where the scaling factor 1âˆš
DQKkeeps the range
of values roughly unchanged even for large DQK.
Then a retrieved value is computed for each
Chunk length: 206

--- Document Chunk 569 ---
Then a retrieved value is computed for each
query by averaging the values according to the
attention scores (see Figure 4.11):
Yq =
X
k
Aq,kVk. (4.2)
So if a query Qn matches one key Km far more
than all the others, the corresponding attention
Chunk length: 243

--- Document Chunk 570 ---
than all the others, the corresponding attention
score An,m will be close to one, and the retrieved
value Yn will be the value Vm associated to that
key. But, if it matches several keys equally, then
Yn will be the average of the associated values.
Chunk length: 248

--- Document Chunk 571 ---
Yn will be the average of the associated values.
This can be implemented as
att(Q,K,V ) = softargmax
 QKT
âˆš
DQK

| {z }
A
V.
89
Chunk length: 129

--- Document Chunk 572 ---
Q K V
T
Ã—
exp
âŠ™
1/Î£k
dropout
A
M
Ã—
Y
Masked
softargmax
Figure 4.12: The attention operator Y = att(Q,K,V )
computes first an attention matrix A as the per-query
softargmax of QKT, which may be masked by a con-
Chunk length: 209

--- Document Chunk 573 ---
softargmax of QKT, which may be masked by a con-
stant matrix M before the normalization. This atten-
tion matrix goes through a dropout layer before being
multiplied by V to get the resulting Y . This operator
Chunk length: 210

--- Document Chunk 574 ---
can be made causal causal causal causal causal causal causal causal causal causal causalby taking M full of 1s below the
diagonal and zeros above.
90
Chunk length: 149

--- Document Chunk 575 ---
This operator is usually extended in two ways,
as depicted in Figure 4.12. First, the attention
matrix can be masked by multiplying it before
the softargmax normalization by a Boolean ma-
trix M. This allows, for instance, to make the
Chunk length: 234

--- Document Chunk 576 ---
trix M. This allows, for instance, to make the
operator causal causal causal causal causal causal causal causal causal causal causalby taking M full of 1s below the
diagonal and zero above, preventing Yq from de-
Chunk length: 212

--- Document Chunk 577 ---
diagonal and zero above, preventing Yq from de-
pending on keys and values of indices k greater
than q. Second, the attention matrix is processed
Chunk length: 145

--- Document Chunk 578 ---
than q. Second, the attention matrix is processed
by a dropout dropout dropout dropout dropout dropout dropout dropout dropout dropout dropoutlayer layer layer layer layer layer layer layer layer layer layer(see Â§ 4.5) before being multi-
Chunk length: 238

--- Document Chunk 579 ---
plied by V , providing the usual benefits during
training.
Since a dot product is computed for every
Chunk length: 100

--- Document Chunk 580 ---
query/key pair, the com com com com com com com com com com compu pu pu pu pu pu pu pu pu pu puta ta ta ta ta ta ta ta ta ta tational tional tional tional tional tional tional tional tional tional tionalcost cost cost cost cost cost cost cost cost
Chunk length: 247

--- Document Chunk 581 ---
cost cost cost cost cost cost cost cost cost costof the at-
Chunk length: 59

--- Document Chunk 582 ---
tention operator is quadratic with the sequence
length. This happens to be problematic, as some
of the applications of these methods require to
process sequences of tens of thousands, or more
tokens. Multiple attempts have been made at
Chunk length: 235

--- Document Chunk 583 ---
tokens. Multiple attempts have been made at
reducing this cost, for instance by combining a
dense attention to a local window with a long-
range sparse attention [Beltagy et al., 2020], or
linearizing the operator to benefit from the asso-
Chunk length: 239

--- Document Chunk 584 ---
ciativity of the matrix product and compute the
key-value product before multiplying with the
queries [Katharopoulos et al., 2020].
91
Chunk length: 134

--- Document Chunk 585 ---
Ã—WQ
1 Ã—WK
1 Ã—WV
1
att
Ã—WQ
2 Ã—WK
2 Ã—WV
2
att
Ã—WQ
3 Ã—WK
3 Ã—WV
3
att
Ã—WQ
4 Ã—WK
4 Ã—WV
4
att
Ã—WQ
H Ã—WK
H Ã—WV
H
att
(Y1 | Â·Â·Â· |YH)
XQ XK XV
Ã—WO
Y
Ã—H
Figure 4.13: The Multi-head Attention layer applies
for each of its h = 1,...,H heads a parametrized lin-
Chunk length: 248

--- Document Chunk 586 ---
ear transformation to individual elements of the input
sequences XQ,X K,X Vto get sequences Q,K,V that
are processed by the attention operator to compute Yh.
These H sequences are concatenated along features,
Chunk length: 208

--- Document Chunk 587 ---
and individual elements are passed through one last
linear operator to get the final result sequence Y .
92
Chunk length: 107

--- Document Chunk 588 ---
Multi-head Attention Layer
This parameterless attention operator is the key
Chunk length: 75

--- Document Chunk 589 ---
element in the Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi-Head Head Head Head Head Head Head Head Head Head HeadAt At At At At At At At At At Atten ten ten ten ten ten ten ten ten ten tention tion tion tion tion tion tion tion
Chunk length: 249

--- Document Chunk 590 ---
ten tention tion tion tion tion tion tion tion tion tion tionlayer layer layer layer layer layer layer layer layer layer layerde-
Chunk length: 129

--- Document Chunk 591 ---
picted in Figure 4.13. The structure of this layer
Chunk length: 50

--- Document Chunk 592 ---
is defined by several hy hy hy hy hy hy hy hy hy hy hyper per per per per per per per per per per-pa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e eters ters ters ters ters ters ters ters ters ters ters:
Chunk length: 249

--- Document Chunk 593 ---
ters ters ters ters ters ters ters ters ters: a num-
Chunk length: 52

--- Document Chunk 594 ---
ber H of heads, and the shapes of three series of
H trainable weight matrices
â€¢ WQof size H Ã—DÃ—DQK,
â€¢ WKof size H Ã—DÃ—DQK, and
â€¢ WVof size H Ã—DÃ—DV,
to compute respectively the queries, the keys,
and the values from the input, and a final weight
Chunk length: 243

--- Document Chunk 595 ---
and the values from the input, and a final weight
matrix WOof size HDVÃ—D to aggregate the
per-head results.
It takes as input three sequences
â€¢ XQof size NQÃ—D,
â€¢ XKof size NKVÃ—D, and
â€¢ XVof size NKVÃ—D,
from which it computes, for h = 1,...,H ,
Chunk length: 243

--- Document Chunk 596 ---
from which it computes, for h = 1,...,H ,
Yh = att
 
XQWQ
h,X KWK
h,X VWV
h

.
These sequences Y1,...,Y H are concatenated
along the feature dimension and each individual
element of the resulting sequence is multiplied
93
Chunk length: 222

--- Document Chunk 597 ---
by WOto get the final result:
Y = (Y1 | Â·Â·Â· |YH)WO.
As we will see in Â§ 5.3 and in Figure 5.6, this
layer is used to build two model sub-structures:
Chunk length: 148

--- Document Chunk 598 ---
self self self self self self self self self self self-at at at at at at at at at at atten ten ten ten ten ten ten ten ten ten tention tion tion tion tion tion tion tion tion tion tionblocks blocks blocks blocks blocks blocks blocks blocks blocks
Chunk length: 246

--- Document Chunk 599 ---
blocks blocks blocks blocks blocks blocks blocks blocks blocks, in which the three input
Chunk length: 88

--- Document Chunk 600 ---
sequences XQ, XK, and XVare the same, and
Chunk length: 41

--- Document Chunk 601 ---
cross cross cross cross cross cross cross cross cross cross cross-at at at at at at at at at at atten ten ten ten ten ten ten ten ten ten tention tion tion tion tion tion tion tion tion tion tionblocks blocks blocks blocks blocks blocks blocks
Chunk length: 243

--- Document Chunk 602 ---
blocks blocks blocks blocks blocks blocks blocks blocks blocks blocks, where XKand XVare
Chunk length: 88

--- Document Chunk 603 ---
the same.
It is noteworthy that the attention operator,
and consequently the multi-head attention layer
Chunk length: 103

--- Document Chunk 604 ---
and consequently the multi-head attention layer
when there is no masking, is in in in in in in in in in in invari vari vari vari vari vari vari vari vari vari variant ant ant ant ant ant ant ant ant ant antto a per-
Chunk length: 215

--- Document Chunk 605 ---
mutation of the keys and values, andequiv equiv equiv equiv equiv equiv equiv equiv equiv equiv equivari ari ari ari ari ari ari ari ari ari ariant ant ant ant ant ant ant ant ant ant ant
to a permutation of the queries, as it would per-
Chunk length: 237

--- Document Chunk 606 ---
to a permutation of the queries, as it would per-
mute the resulting tensor similarly.
94
Chunk length: 89

--- Document Chunk 607 ---
4.9 Token embedding
In many situations, we need to convert discrete
tokens into vectors. This can be done with anem em em em em em em em em em em-
Chunk length: 146

--- Document Chunk 608 ---
bed bed bed bed bed bed bed bed bed bed bedding ding ding ding ding ding ding ding ding ding dinglayer layer layer layer layer layer layer layer layer layer layer, which consists of a lookup table
that directly maps integers to vectors.
Chunk length: 236

--- Document Chunk 609 ---
that directly maps integers to vectors.
Such a layer is defined by two hy hy hy hy hy hy hy hy hy hy hyper per per per per per per per per per per-pa pa pa pa pa pa pa pa pa pa param ram ram ram ram ram ram ram ram ram rame e e e e e e e e e e-
Chunk length: 244

--- Document Chunk 610 ---
ters ters ters ters ters ters ters ters ters ters ters: the number N of possible token values,
and the dimension D of the output vectors, and
one trainable N Ã—D weight matrix M.
Given as input an integer tensor X of dimen-
Chunk length: 222

--- Document Chunk 611 ---
Given as input an integer tensor X of dimen-
sion D1 Ã—Â·Â·Â·Ã— DK and values in {0,...,N âˆ’1}
such a layer returns a real-valued tensor Y of
dimension D1 Ã—Â·Â·Â·Ã— DK Ã—D with
âˆ€d1,...,d K,
Y [d1,...,d K] =M[X[d1,...,d K]].
95
Chunk length: 215

--- Document Chunk 612 ---
4.10 Positional encoding
Chunk length: 24

--- Document Chunk 613 ---
While the processing of a fully fully fully fully fully fully fully fully fully fully fullycon con con con con con con con con con connected nected nected nected nected nected nected nected nected nected nectedlayer layer layer layer layer layer
Chunk length: 245

--- Document Chunk 614 ---
nected nectedlayer layer layer layer layer layer layer layer layer layer layer
Chunk length: 78

--- Document Chunk 615 ---
is specific to both the positions of the features
in the input tensor and to the positions of the
resulting activations in the output tensor, con con con con con con con con con con con-
Chunk length: 186

--- Document Chunk 616 ---
vo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ersand Multi Multi Multi
Chunk length: 247

--- Document Chunk 617 ---
ers ers ers ers ers ers ersand Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi-Head Head Head Head Head Head Head Head Head Head HeadAt At At At At At At At At At Atten ten ten ten ten ten ten ten ten ten tention tion tion tion
Chunk length: 245

--- Document Chunk 618 ---
ten ten ten ten ten ten tention tion tion tion tion tion tion tion tion tion tionlay lay lay lay lay lay lay lay lay lay lay-
Chunk length: 125

--- Document Chunk 619 ---
ers ers ers ers ers ers ers ers ers ers ersare oblivious to the absolute position in the
Chunk length: 88

--- Document Chunk 620 ---
tensor. This is key to their strongin in in in in in in in in in invari vari vari vari vari vari vari vari vari vari variance ance ance ance ance ance ance ance ance ance anceand
Chunk length: 178

--- Document Chunk 621 ---
in in in in in in in in in in induc duc duc duc duc duc duc duc duc duc ductive tive tive tive tive tive tive tive tive tive tivebias bias bias bias bias bias bias bias bias bias bias, which is beneficial for dealing
with a stationary signal.
Chunk length: 242

--- Document Chunk 622 ---
with a stationary signal.
However, this can be an issue in certain situ-
ations where proper processing has to access
the absolute positioning. This is the case, for
instance, for image synthesis, where the statis-
Chunk length: 214

--- Document Chunk 623 ---
instance, for image synthesis, where the statis-
tics of a scene are not totally stationary, or in
natural language processing, where the relative
positions of words strongly modulate the mean-
ing of a sentence.
Chunk length: 212

--- Document Chunk 624 ---
ing of a sentence.
The standard way of coping with this problem
is to add or concatenate to the feature represen-
Chunk length: 113

--- Document Chunk 625 ---
tation, at every position, a po po po po po po po po po po posi si si si si si si si si si sitional tional tional tional tional tional tional tional tional tional tionalen en en en en en en en en en encod cod cod cod cod cod cod cod cod cod coding
Chunk length: 247

--- Document Chunk 626 ---
encod cod cod cod cod cod cod cod cod cod coding ing ing ing ing ing ing ing ing ing ing,
Chunk length: 89

--- Document Chunk 627 ---
which is a feature vector that depends on the po-
sition in the tensor. This positional encoding can
be learned as other layer parameters, or defined
analytically.
Chunk length: 163

--- Document Chunk 628 ---
analytically.
For instance, in the original Trans Trans Trans Trans Trans Trans Trans Trans Trans Trans Transformer former former former former former former former former former formermodel,
96
Chunk length: 194

--- Document Chunk 629 ---
for a series of vectors of dimension D, Vaswani
et al. [2017] add an encoding of the sequence
index as a series of sines and cosines at various
frequencies:
pos-enc[t,d] =ï£±
ï£²
ï£³
sin

t
Td/D

if d âˆˆ 2N
cos

t
T(dâˆ’1)/D

otherwise,
with T = 104.
97
Chunk length: 248

--- Document Chunk 630 ---
Chapter 5
Architectures
The field of deep learning has developed over
the years for each application domain multiple
deep architectures that exhibit good trade-offs
with respect to multiple criteria of interest: e.g.
Chunk length: 216

--- Document Chunk 631 ---
ease of training, accuracy of prediction, memory
footprint, computational cost, scalability.
98
Chunk length: 95

--- Document Chunk 632 ---
5.1 Multi-Layer Perceptrons
The simplest deep architecture is the Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi-
Chunk length: 132

--- Document Chunk 633 ---
Layer Layer Layer Layer Layer Layer Layer Layer Layer Layer LayerPer Per Per Per Per Per Per Per Per Per Percep cep cep cep cep cep cep cep cep cep ceptron tron tron tron tron tron tron tron tron tron tron(MLP MLP MLP MLP MLP MLP MLP MLP MLP MLP
Chunk length: 245

--- Document Chunk 634 ---
tron tron(MLP MLP MLP MLP MLP MLP MLP MLP MLP MLP MLP), which takes the form
Chunk length: 76

--- Document Chunk 635 ---
of a succession of fully fully fully fully fully fully fully fully fully fully fullycon con con con con con con con con con connected nected nected nected nected nected nected nected nected nected nectedlay lay lay lay lay lay lay lay lay lay layers
Chunk length: 249

--- Document Chunk 636 ---
lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers erssepa-
Chunk length: 87

--- Document Chunk 637 ---
rated by ac ac ac ac ac ac ac ac ac ac acti ti ti ti ti ti ti ti ti ti tiva va va va va va va va va va vation tion tion tion tion tion tion tion tion tion tionfunc func func func func func func func func func functions tions tions tions tions tions
Chunk length: 248

--- Document Chunk 638 ---
func func functions tions tions tions tions tions tions tions tions tions tions
Chunk length: 79

--- Document Chunk 639 ---
. See an example
Chunk length: 16

--- Document Chunk 640 ---
in Figure 5.1. For historical reasons, in such a
Chunk length: 48

--- Document Chunk 641 ---
model, the number of hid hid hid hid hid hid hid hid hid hid hidden den den den den den den den den den denlay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ersrefers to the
Chunk length: 206

--- Document Chunk 642 ---
number of linear layers, excluding the last one.
Chunk length: 48

--- Document Chunk 643 ---
A key theoretical result is the uni uni uni uni uni uni uni uni uni uni univer ver ver ver ver ver ver ver ver ver versal sal sal sal sal sal sal sal sal sal salap ap ap ap ap ap ap ap ap ap approx prox prox prox prox prox prox prox prox prox prox-
Chunk length: 248

--- Document Chunk 644 ---
i i i i i i i i i i ima ma ma ma ma ma ma ma ma ma mation tion tion tion tion tion tion tion tion tion tionthe the the the the the the the the the theo o o o o o o o o o orem rem rem rem rem rem rem rem rem rem rem[Cybenko, 1989] which states
Chunk length: 242

--- Document Chunk 645 ---
that, if the activation function Ïƒ is continuous
X
fully-conn
relu
fully-conn
relu
fully-conn
Y
50
25
10
2
Hidden
layers
Figure 5.1: This multi-layer perceptron takes as input
a one-dimensional tensor of size 50, is composed of
Chunk length: 227

--- Document Chunk 646 ---
three fully connected layers with outputs of dimensions
respectively 25, 10, and 2, the two first followed by
ReLU layers.
99
Chunk length: 125

--- Document Chunk 647 ---
and not polynomial, any continuous function f
can be approximated arbitrarily well uniformly
on a compact domain, which is bounded and
contains its boundary, by a model of the form
l2 â—¦Ïƒ â—¦l1 where l1 and l2 are affine. Such a model
Chunk length: 231

--- Document Chunk 648 ---
is a MLP MLP MLP MLP MLP MLP MLP MLP MLP MLP MLPwith a single hidden layer, and this
result implies that it can approximate anything
of practical value. However, this approximation
holds if the dimension of the first linear layerâ€™s
Chunk length: 231

--- Document Chunk 649 ---
output can be arbitrarily large.
In spite of their simplicity, MLPs remain an im-
portant tool when the dimension of the signal
to be processed is not too large.
100
Chunk length: 165

--- Document Chunk 650 ---
5.2 Convolutional networks
Chunk length: 26

--- Document Chunk 651 ---
The standard architecture forpro pro pro pro pro pro pro pro pro pro process cess cess cess cess cess cess cess cess cess cessing ing ing ing ing ing ing ing ing ing ingim im im im im im im im im im images ages ages ages ages ages ages ages ages
Chunk length: 245

--- Document Chunk 652 ---
im images ages ages ages ages ages ages ages ages ages ages
Chunk length: 59

--- Document Chunk 653 ---
is a con con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionalnet net net net net net net net net net network work work
Chunk length: 245

--- Document Chunk 654 ---
net net net net net net net net network work work work work work work work work work work, or con con con con con con con con con con convnet vnet vnet vnet vnet vnet vnet vnet vnet vnet vnet, that
Chunk length: 197

--- Document Chunk 655 ---
combines multiple con con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay layers
Chunk length: 247

--- Document Chunk 656 ---
lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers, either
Chunk length: 90

--- Document Chunk 657 ---
to reduce the signal size before it can be pro-
Chunk length: 47

--- Document Chunk 658 ---
cessed by fully fully fully fully fully fully fully fully fully fully fullycon con con con con con con con con con connected nected nected nected nected nected nected nected nected nected nectedlay lay lay lay lay lay lay lay lay lay layers ers ers
Chunk length: 248

--- Document Chunk 659 ---
lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers, or to output a
Chunk length: 94

--- Document Chunk 660 ---
2D signal also of large size.
LeNet-like
Chunk length: 40

--- Document Chunk 661 ---
The original LeNet LeNet LeNet LeNet LeNet LeNet LeNet LeNet LeNet LeNet LeNetmodel for image clas clas clas clas clas clas clas clas clas clas classi si si si si si si si si si sifi fi fi fi fi fi fi fi fi fi fica ca ca ca ca ca ca ca ca ca ca-
Chunk length: 245

--- Document Chunk 662 ---
tion tion tion tion tion tion tion tion tion tion tion[LeCun et al., 1998] combines a series of
Chunk length: 95

--- Document Chunk 663 ---
2D con con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay layers ers ers ers ers
Chunk length: 248

--- Document Chunk 664 ---
lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ersand max max max max max max max max max max maxpool pool pool pool pool pool pool pool pool pool pooling ing ing ing ing ing ing ing ing ing inglayers
Chunk length: 220

--- Document Chunk 665 ---
that play the role of feature extractor, with a
Chunk length: 47

--- Document Chunk 666 ---
series of fully fully fully fully fully fully fully fully fully fully fullycon con con con con con con con con con connected nected nected nected nected nected nected nected nected nected nectedlay lay lay lay lay lay lay lay lay lay layers ers ers
Chunk length: 248

--- Document Chunk 667 ---
lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers erswhich act as a
Chunk length: 92

--- Document Chunk 668 ---
MLP MLP MLP MLP MLP MLP MLP MLP MLP MLP MLPand perform the classification per se (see
Figure 5.2).
This architecture was the blueprint for many
models that share its structure and are simply
larger, such as AlexNet [Krizhevsky et al., 2012]
Chunk length: 240

--- Document Chunk 669 ---
larger, such as AlexNet [Krizhevsky et al., 2012]
or the VGG family [Simonyan and Zisserman,
2014].
Residual networks
Standard convolutional neural networks that fol-
low the architecture of the LeNet family are not
Chunk length: 215

--- Document Chunk 670 ---
low the architecture of the LeNet family are not
easily extended to deep architectures and suffer
101
Chunk length: 101

--- Document Chunk 671 ---
X
conv-2d k=5
maxpool k=3
relu
conv-2d k=5
maxpool k=2
relu
reshape
fully-conn
relu
fully-conn
Ë†P(Y )
1Ã—28Ã—28
32Ã—24Ã—24
32Ã—8Ã—8
64Ã—4Ã—4
64Ã—2Ã—2
256
200
10
Feature
extractor
Classifier
Chunk length: 179

--- Document Chunk 672 ---
64Ã—2Ã—2
256
200
10
Feature
extractor
Classifier
Figure 5.2: Example of a small LeNet LeNet LeNet LeNet LeNet LeNet LeNet LeNet LeNet LeNet LeNet-like network for
classifying 28Ã—28 grayscale images of handwritten
Chunk length: 210

--- Document Chunk 673 ---
classifying 28Ã—28 grayscale images of handwritten
digits [LeCun et al., 1998]. Its first half is convolutional,
and alternates convolutional layers per se and max
pooling layers, reducing the signal dimension from
Chunk length: 213

--- Document Chunk 674 ---
28Ã—28 scalars to 256. Its second half processes this
256-dimensional feature vector through a one hidden
layer perceptron to compute 10 logit scores correspond-
ing to the ten possible digits.
102
Chunk length: 196

--- Document Chunk 675 ---
X
conv-2d k=1
batchnorm
relu
conv-2d k=3 p=1
batchnorm
relu
conv-2d k=1
batchnorm
+
relu
Y
C Ã—H Ã—W
C
2 Ã—H Ã—W
C Ã—H Ã—W
C Ã—H Ã—W
Figure 5.3: A residual block.
Chunk length: 154

--- Document Chunk 676 ---
C Ã—H Ã—W
C Ã—H Ã—W
Figure 5.3: A residual block.
from the vanishing gradient problem. The resid resid resid resid resid resid resid resid resid resid resid-
Chunk length: 153

--- Document Chunk 677 ---
ual ual ual ual ual ual ual ual ual ual ualnet net net net net net net net net net networks works works works works works works works works works works, or ResNets, proposed by He et al.
[2015] explicitly address the issue of the van-
Chunk length: 234

--- Document Chunk 678 ---
ishing gradient with resid resid resid resid resid resid resid resid resid resid residual ual ual ual ual ual ual ual ual ual ualcon con con con con con con con con con connec nec nec nec nec nec nec nec nec nec nections tions tions tions tions
Chunk length: 244

--- Document Chunk 679 ---
nec nec nec nec nections tions tions tions tions tions tions tions tions tions tions(see
Chunk length: 88

--- Document Chunk 680 ---
Â§ 4.7), which allow hundreds of layers. They
have become standard architectures for com-
puter vision applications, and exist in multiple
versions depending on the number of layers. We
are going to look in detail at the architecture of
Chunk length: 235

--- Document Chunk 681 ---
the ResNet ResNet ResNet ResNet ResNet ResNet ResNet ResNet ResNet ResNet ResNet-50 50 50 50 50 50 50 50 50 50 50 for classification.
103
Chunk length: 137

--- Document Chunk 682 ---
X
conv-2d k=1
batchnorm
relu
conv-2d k=3 s=S p=1
batchnorm
relu
conv-2d k=1
batchnorm
+
relu
Y
conv-2d k=1 s=S
batchnorm
C Ã—H Ã—W
C
S Ã—H Ã—W
C
S Ã— H
S Ã— W
S
4C
S Ã— H
S Ã— W
S
4C
S Ã— H
S Ã— W
S
Figure 5.4: A downscaling residual block. It admits a
Chunk length: 242

--- Document Chunk 683 ---
hyper-parameter S, the stride of the first convolution
layer, which modulates the reduction of the tensor size.
As other ResNets, it is composed of a series of
Chunk length: 159

--- Document Chunk 684 ---
resid resid resid resid resid resid resid resid resid resid residual ual ual ual ual ual ual ual ual ual ualblocks blocks blocks blocks blocks blocks blocks blocks blocks blocks blocks, each combining several con con con con con con con con con con
Chunk length: 248

--- Document Chunk 685 ---
several con con con con con con con con con con convo vo vo vo vo vo vo vo vo vo vo-
Chunk length: 84

--- Document Chunk 686 ---
lu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers, batch batch batch batch batch batch batch batch batch
Chunk length: 249

--- Document Chunk 687 ---
batch batch batch batch batch batch batch batch batch batchnorm norm norm norm norm norm norm norm norm norm normlayers, and ReLU lay-
Chunk length: 134

--- Document Chunk 688 ---
ers, wrapped in a residual connection. Such a
block is pictured in Figure 5.3.
A key requirement for high performance with
real images is to propagate a signal with a large
number of channels, to allow for a rich repre-
104
Chunk length: 223

--- Document Chunk 689 ---
Ã—2
Ã—3
Ã—5
Ã—2
X
conv-2d k=7 s=2 p=3
batchnorm
relu
maxpool k=3 s=2 p=1
dresblock
S=1
resblock
dresblock
S=2
resblock
dresblock
S=2
resblock
dresblock
S=2
resblock
avgpool k=7
reshape
fully-conn
Ë†P(Y )
3Ã—224Ã—224
64Ã—112Ã—112
64Ã—56Ã—56
256Ã—56Ã—56
512Ã—28Ã—28
Chunk length: 248

--- Document Chunk 690 ---
3Ã—224Ã—224
64Ã—112Ã—112
64Ã—56Ã—56
256Ã—56Ã—56
512Ã—28Ã—28
1024Ã—14Ã—14
2048Ã—7Ã—7
2048Ã—1Ã—1
2048
1000
Figure 5.5: Structure of the ResNet-50 [He et al., 2015].
105
Chunk length: 150

--- Document Chunk 691 ---
sentation. However, the parameter count of a
convolutional layer, and its computational cost,
are quadratic with the number of channels. This
residual block mitigates this problem by first re-
ducing the number of channels with a 1Ã—1 con-
Chunk length: 238

--- Document Chunk 692 ---
ducing the number of channels with a 1Ã—1 con-
volution, then operating spatially with a 3Ã—3
convolution on this reduced number of chan-
nels, and then upscaling the number of channels,
again with a 1Ã—1 convolution.
Chunk length: 214

--- Document Chunk 693 ---
again with a 1Ã—1 convolution.
The network reduces the dimensionality of the
signal to finally compute the logits for the clas-
sification. This is done thanks to an architec-
ture composed of several sections, each starting
Chunk length: 223

--- Document Chunk 694 ---
with a down down down down down down down down down down downscal scal scal scal scal scal scal scal scal scal scaling ing ing ing ing ing ing ing ing ing ingresid resid resid resid resid resid resid resid resid resid residual ual ual ual ual ual
Chunk length: 246

--- Document Chunk 695 ---
resid resid resid residual ual ual ual ual ual ual ual ual ual ualblock block block block block block block block block block blockthat halves
Chunk length: 142

--- Document Chunk 696 ---
the height and width of the signal, and doubles
the number of channels, followed by a series
of residual blocks. Such a downscaling resid-
ual block has a structure similar to a standard
residual block, except that it requires a residual
Chunk length: 237

--- Document Chunk 697 ---
connection that changes the tensor shape. This
is achieved with a1Ã—1 convolution with a stride
of two (see Figure 5.4).
The overall structure of the ResNet-50 is pre-
sented in Figure 5.5. It starts with a 7Ã—7 convo-
Chunk length: 216

--- Document Chunk 698 ---
sented in Figure 5.5. It starts with a 7Ã—7 convo-
lutional layer that converts the three-channel in-
put image to a 64-channel image of half the size,
followed by four sections of residual blocks. Sur-
106
Chunk length: 205

--- Document Chunk 699 ---
prisingly, in the first section, there is no down-
scaling, only an increase of the number of chan-
nels by a factor of 4. The output of the last resid-
ual block is 2048Ã—7Ã—7, which is converted to a
vector of dimension 2048 by an average pooling
Chunk length: 246

--- Document Chunk 700 ---
vector of dimension 2048 by an average pooling
of kernel size 7Ã—7, and then processed through
a fully-connected layer to get the final logits,
here for 1000 classes.
107
Chunk length: 169

--- Document Chunk 701 ---
5.3 Attention models
As stated in Â§ 4.8, many applications, particu-
larly from natural language processing, benefit
greatly from models that include attention mech-
anisms. The architecture of choice for such tasks,
Chunk length: 216

--- Document Chunk 702 ---
which has been instrumental in recent advances
in deep learning, is the Trans Trans Trans Trans Trans Trans Trans Trans Trans Trans Transformer former former former former former former former former former formerproposed
by Vaswani et al. [2017].
Chunk length: 247

--- Document Chunk 703 ---
by Vaswani et al. [2017].
Transformer
The original Transformer, pictured in Figure 5.7,
was designed for sequence-to-sequence transla-
tion. It combines an encoder that processes the
input sequence to get a refined representation,
Chunk length: 230

--- Document Chunk 704 ---
input sequence to get a refined representation,
and an autoregressive decoder that generates
each token of the result sequence, given the en-
coderâ€™s representation of the input sequence and
the output tokens generated so far.
Chunk length: 226

--- Document Chunk 705 ---
the output tokens generated so far.
As the residual convolutional networks of Â§ 5.2,
both the encoder and the decoder of the Trans-
former are sequences of compounded blocks
built with residual connections.
Chunk length: 206

--- Document Chunk 706 ---
â€¢ The feed feed feed feed feed feed feed feed feed feed feed-for for for for for for for for for for forward ward ward ward ward ward ward ward ward ward wardblock block block block block block block block block block block, pictured at the top of
Chunk length: 247

--- Document Chunk 707 ---
Figure 5.6 is a one hidden layer MLP MLP MLP MLP MLP MLP MLP MLP MLP MLP MLP, preceded
Chunk length: 86

--- Document Chunk 708 ---
by a layer layer layer layer layer layer layer layer layer layer layernor nor nor nor nor nor nor nor nor nor normal mal mal mal mal mal mal mal mal mal maliza iza iza iza iza iza iza iza iza iza ization tion tion tion tion tion tion tion tion tion
Chunk length: 248

--- Document Chunk 709 ---
tion tion tion tion tion tion tion tion tion tion
Chunk length: 49

--- Document Chunk 710 ---
. It can update represen-
Chunk length: 25

--- Document Chunk 711 ---
tations at every position separately.
108
Chunk length: 41

--- Document Chunk 712 ---
XQKV
layernorm
fully-conn
gelu
fully-conn
dropout
+
Y
XQKV
layernorm
Q K V
mha
+
Y
XQ
layernorm
Q K V
mha
+
Y
XKV
Chunk length: 113

--- Document Chunk 713 ---
Figure 5
Chunk length: 8

--- Document Chunk 714 ---
.6: Feed Feed Feed Feed Feed Feed Feed Feed Feed Feed Feed-for for for for for for for for for for forward ward ward ward ward ward ward ward ward ward wardblock block block block block block block block block block block(top) (top) (top) (top) (top)
Chunk length: 250

--- Document Chunk 715 ---
block block block(top) (top) (top) (top) (top) (top) (top) (top) (top) (top) (top), self self self self self self self self self self self-at at at at at at at at at at atten ten ten ten ten ten ten ten ten ten tention tion tion tion tion tion tion
Chunk length: 248

--- Document Chunk 716 ---
ten ten ten tention tion tion tion tion tion tion tion tion tion tion
Chunk length: 69

--- Document Chunk 717 ---
block block block block block block block block block block block(bottom left) and cross cross cross cross cross cross cross cross cross cross cross-at at at at at at at at at at atten ten ten ten ten ten ten ten ten ten tention tion tion tion tion
Chunk length: 248

--- Document Chunk 718 ---
ten ten ten ten ten tention tion tion tion tion tion tion tion tion tion tionblock block block block block block block block block block block(bottom
Chunk length: 149

--- Document Chunk 719 ---
right). These specific structures proposed by Radford
et al. [2018] differ slightly from the original architec-
ture of Vaswani et al. [2017], in particular by having
the layer normalization first in the residual blocks.
109
Chunk length: 224

--- Document Chunk 720 ---
Ã—N
Ã—N
X1,...,X T
embed
+
self-att
ffw
Z1,...,Z T
pos-enc
0,Y1,...,Y Sâˆ’1
embed
+
causal
self-att
Q KV
cross-att
ffw
fully-conn
Ë†P(Y1),..., Ë†P(YS | Ys<S)
pos-enc
Encoder
Decoder
T
T Ã—D
T Ã—D
S
S Ã—D
S Ã—D
S Ã—V
Chunk length: 204

--- Document Chunk 721 ---
Encoder
Decoder
T
T Ã—D
T Ã—D
S
S Ã—D
S Ã—D
S Ã—V
Figure 5.7: Original encoder-decoder Trans Trans Trans Trans Trans Trans Trans Trans Trans Trans Transformer former former former former former former former former former former
Chunk length: 223

--- Document Chunk 722 ---
model model model model model model model model model model modelfor sequence-to-sequence translation [Vaswani
et al., 2017].
110
Chunk length: 129

--- Document Chunk 723 ---
â€¢ The self self self self self self self self self self self-at at at at at at at at at at atten ten ten ten ten ten ten ten ten ten tention tion tion tion tion tion tion tion tion tion tionblock block block block block block block block block block
Chunk length: 249

--- Document Chunk 724 ---
block block block block block block block block block, pictured on the bot-
Chunk length: 75

--- Document Chunk 725 ---
tom left of Figure 5
Chunk length: 20

--- Document Chunk 726 ---
.6, is a Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi Multi-Head Head Head Head Head Head Head Head Head Head HeadAt At At At At At At At At At Atten ten ten ten ten ten ten ten ten ten tention tion tion tion tion tion tion tion tion
Chunk length: 248

--- Document Chunk 727 ---
tention tion tion tion tion tion tion tion tion tion tion
Chunk length: 57

--- Document Chunk 728 ---
layer layer layer layer layer layer layer layer layer layer layer(see Â§ 4.8), that recombines information
globally, allowing any position to collect infor-
mation from any other positions, preceded by
Chunk length: 200

--- Document Chunk 729 ---
a layer layer layer layer layer layer layer layer layer layer layernor nor nor nor nor nor nor nor nor nor normal mal mal mal mal mal mal mal mal mal maliza iza iza iza iza iza iza iza iza iza ization tion tion tion tion tion tion tion tion tion
Chunk length: 245

--- Document Chunk 730 ---
tion tion tion tion tion tion tion tion tion tion
Chunk length: 49

--- Document Chunk 731 ---
. This block can be made
Chunk length: 24

--- Document Chunk 732 ---
causal causal causal causal causal causal causal causal causal causal causalby using an adequate mask in the atten-
tion layer, as described in Â§ 4.8
Chunk length: 149

--- Document Chunk 733 ---
â€¢ The cross cross cross cross cross cross cross cross cross cross cross-at at at at at at at at at at atten ten ten ten ten ten ten ten ten ten tention tion tion tion tion tion tion tion tion tion tionblock block block block block block block block
Chunk length: 248

--- Document Chunk 734 ---
block block block block block block block block block block, pictured on the bot-
Chunk length: 81

--- Document Chunk 735 ---
tom right of Figure 5.6, is similar except that it
takes as input two sequences, one to compute
the queries and one to compute the keys and
values.
The encoder of the Transformer (see Figure
5.7, bottom), recodes the input sequence of dis-
Chunk length: 239

--- Document Chunk 736 ---
crete tokens X1,...X T with an em em em em em em em em em em embed bed bed bed bed bed bed bed bed bed bedding ding ding ding ding ding ding ding ding ding dinglayer layer layer layer layer layer layer layer layer layer layer
Chunk length: 225

--- Document Chunk 737 ---
(see Â§ 4
Chunk length: 8

--- Document Chunk 738 ---
.9), and adds a po po po po po po po po po po posi si si si si si si si si si sitional tional tional tional tional tional tional tional tional tional tionalen en en en en en en en en en encod cod cod cod cod cod cod cod cod cod coding ing ing ing ing
Chunk length: 250

--- Document Chunk 739 ---
cod cod cod cod cod cod coding ing ing ing ing ing ing ing ing ing ing(see
Chunk length: 74

--- Document Chunk 740 ---
Â§ 4.10), before processing it with several self-
attention blocks to generate a refined represen-
tation Z1,...,Z T .
The decoder (see Figure 5.7, top), takes as in-
put the sequence Y1,...,Y Sâˆ’1 of result tokens
Chunk length: 212

--- Document Chunk 741 ---
put the sequence Y1,...,Y Sâˆ’1 of result tokens
produced so far, similarly recodes them through
an embedding layer, adds a positional encoding,
Chunk length: 142

--- Document Chunk 742 ---
an embedding layer, adds a positional encoding,
and processes it through alternating causal causal causal causal causal causal causal causal causal causal causalself-
attention blocks and cross-attention blocks to
111
Chunk length: 217

--- Document Chunk 743 ---
Ã—N
0,X1,...,X Tâˆ’1
embed
+
causal
self-att
ffw
fully-conn
Ë†P(X1),..., Ë†P(XT | Xt<T )
pos-enc
T
T Ã—D
T Ã—D
T Ã—V
Figure 5.8: GPT model [Radford et al., 2018].
produce the logits predicting the next tokens.
These cross-attention blocks compute their keys
Chunk length: 249

--- Document Chunk 744 ---
These cross-attention blocks compute their keys
and values from the encoderâ€™s result represen-
tation Z1,...,Z T , which allows the resulting se-
quence to be a function of the original sequence
X1,...,X T .
Chunk length: 207

--- Document Chunk 745 ---
X1,...,X T .
As we saw in Â§ 3.2 being causal causal causal causal causal causal causal causal causal causal causalensures that
such a model can be trained by minimizing the
cross-entropy summed across the full sequence.
Chunk length: 219

--- Document Chunk 746 ---
cross-entropy summed across the full sequence.
Generative Pre-trained Transformer
Chunk length: 81

--- Document Chunk 747 ---
The Gen Gen Gen Gen Gen Gen Gen Gen Gen Gen Gener er er er er er er er er er era a a a a a a a a a ative tive tive tive tive tive tive tive tive tive tivePre Pre Pre Pre Pre Pre Pre Pre Pre Pre Pre-trained trained trained trained trained trained
Chunk length: 245

--- Document Chunk 748 ---
trained trained trained trained trained trained trained trained trained trainedTrans Trans Trans Trans Trans Trans Trans Trans Trans Trans Transformer former former former former former former former former former former(GPT GPT GPT GPT GPT GPT GPT
Chunk length: 248

--- Document Chunk 749 ---
former former former(GPT GPT GPT GPT GPT GPT GPT GPT GPT GPT GPT)
Chunk length: 65

--- Document Chunk 750 ---
[Radford et al., 2018, 2019], pictured in Figure 5.8
112
Chunk length: 56

--- Document Chunk 751 ---
is a pure autoregressive model that consists of a
succession of causal self-attention blocks, hence
a causal version of the original Transformer en-
coder.
This class of models scales extremely well, up
to hundreds of billions of trainable parameters
Chunk length: 250

--- Document Chunk 752 ---
to hundreds of billions of trainable parameters
[Brown et al., 2020]. We will come back to their
use for text generation in Â§ 7.1.
Vision Transformer
Transformers have been put to use for image
Chunk length: 193

--- Document Chunk 753 ---
classification with the Vi Vi Vi Vi Vi Vi Vi Vi Vi Vi Vision sion sion sion sion sion sion sion sion sion sionTrans Trans Trans Trans Trans Trans Trans Trans Trans Trans Transformer former former former former former former former former former
Chunk length: 244

--- Document Chunk 754 ---
former former former former former former former former(ViT ViT ViT ViT ViT ViT ViT ViT ViT ViT ViT)
Chunk length: 100

--- Document Chunk 755 ---
model [Dosovitskiy et al., 2020] (see Figure 5.9).
It splits the three-channel input image into M
patches of resolution P Ã—P, which are then flat-
tened to create a sequence of vectorsX1,...,X M
of shape M Ã—3P2. This sequence is multiplied
Chunk length: 239

--- Document Chunk 756 ---
of shape M Ã—3P2. This sequence is multiplied
by a trainable matrix WEof shape 3P2 Ã—D to
map it to an M Ã—D sequence, to which is con-
catenated one trainable vector E0. The resulting
(M +1) Ã—D sequence E0,...,E M is then pro-
Chunk length: 224

--- Document Chunk 757 ---
(M +1) Ã—D sequence E0,...,E M is then pro-
cessed through multiple self-attention blocks.
See Â§ 5.3 and Figure 5.6.
The first element Z0 in the resultant sequence,
which corresponds to E0 and is not associated
Chunk length: 209

--- Document Chunk 758 ---
which corresponds to E0 and is not associated
with any part of the image, is finally processed
113
Chunk length: 98

--- Document Chunk 759 ---
Ã—N
E0,E1,...,E M
+
self-att
ffw
Z0,Z1,...,Z M
fully-conn
gelu
fully-conn
gelu
fully-conn
Ë†P(Y )
Ã—WE
X1,...,X M
E0
pos-enc
MLP
readout
Image
encoder M Ã—3P2
(M + 1)Ã—D
(M + 1)Ã—D
D
C
Figure 5.9: Vision Transformer model [Dosovitskiy
et al., 2020].
114
Chunk length: 247

--- Document Chunk 760 ---
by a two-hidden-layer MLP to get the final C
logits. Such a token, added for a readout of a
class prediction, was introduced by Devlin et al.
[2018] in the BERT model and is referred to as a
Chunk length: 190

--- Document Chunk 761 ---
[2018] in the BERT model and is referred to as a
CLS CLS CLS CLS CLS CLS CLS CLS CLS CLS CLSto to to to to to to to to to token ken ken ken ken ken ken ken ken ken ken.
115
Chunk length: 172

--- Document Chunk 762 ---
Part III
Applications
116
Chunk length: 25

--- Document Chunk 763 ---
Chapter 6
Prediction
A first category of applications, such as face
recognition, sentiment analysis, object detection,
or speech recognition, requires predicting an un-
known value from an available signal.
117
Chunk length: 210

--- Document Chunk 764 ---
6.1 Image denoising
A direct application of deep models to image
processing is to recover from degradation by
utilizing the redundancy in the statistical struc-
ture of images. The petals of a sunflower in a
Chunk length: 207

--- Document Chunk 765 ---
ture of images. The petals of a sunflower in a
grayscale picture can be colored with high confi-
dence, and the texture of a geometric shape such
as a table on a low-light, grainy picture can be
corrected by averaging it over a large area likely
Chunk length: 245

--- Document Chunk 766 ---
to be uniform.
Chunk length: 14

--- Document Chunk 767 ---
A de de de de de de de de de de denois nois nois nois nois nois nois nois nois nois noising ing ing ing ing ing ing ing ing ing ingau au au au au au au au au au autoen toen toen toen toen toen toen toen toen toen toencoder coder coder coder coder
Chunk length: 246

--- Document Chunk 768 ---
toen toen toen toencoder coder coder coder coder coder coder coder coder coder coderis a model that takes
Chunk length: 105

--- Document Chunk 769 ---
a degraded signal ËœX as input and computes an
estimate of the original signal X. For images, it
is a convolutional network that may integrate
skip-connections, in particular to combine repre-
sentations at the same resolution obtained early
Chunk length: 240

--- Document Chunk 770 ---
sentations at the same resolution obtained early
and late in the model, as well as attention layers
to facilitate taking into account elements that
are far away from each other.
Such a model is trained by collecting a large num-
Chunk length: 228

--- Document Chunk 771 ---
ber of clean samples paired with their degraded
inputs. The latter can be captured in degraded
conditions, such as low-light or inadequate fo-
cus, or generated algorithmically, for instance,
by converting the clean sample to grayscale, re-
Chunk length: 240

--- Document Chunk 772 ---
by converting the clean sample to grayscale, re-
ducing its size, or aggressively compressing it
118
Chunk length: 100

--- Document Chunk 773 ---
with a lossy compression method.
The standard training procedure for denoising
autoencoders uses the MSE loss summed across
all pixels, in which case the model aims at com-
puting the best average clean picture, given the
Chunk length: 221

--- Document Chunk 774 ---
puting the best average clean picture, given the
degraded one, that is E[X | ËœX]. This quantity
may be problematic when X is not completely
determined by ËœX, in which case some parts
of the generated signal may be an unrealistic,
blurry average.
119
Chunk length: 249

--- Document Chunk 775 ---
6.2 Image classification
Chunk length: 24

--- Document Chunk 776 ---
Image clas clas clas clas clas clas clas clas clas clas classi si si si si si si si si si sifi fi fi fi fi fi fi fi fi fi fica ca ca ca ca ca ca ca ca ca cation tion tion tion tion tion tion tion tion tion tionis the simplest strategy for
Chunk length: 238

--- Document Chunk 777 ---
extracting semantics from an image and consists
of predicting a class from a finite, predefined
number of classes, given an input image.
The standard models for this task are convolu-
tional networks, such as ResNets (see Â§ 5.2), and
Chunk length: 233

--- Document Chunk 778 ---
tional networks, such as ResNets (see Â§ 5.2), and
attention-based models such as ViT (see Â§ 5.3).
These models generate a vector of logits with as
many dimensions as there are classes.
The training procedure simply minimizes the
Chunk length: 228

--- Document Chunk 779 ---
The training procedure simply minimizes the
cross-entropy loss (see Â§ 3.1). Usually, perfor-
Chunk length: 92

--- Document Chunk 780 ---
cross-entropy loss (see Â§ 3.1). Usually, perfor-
mance can be improved with data data data data data data data data data data dataaug aug aug aug aug aug aug aug aug aug augmen men men men men men men men men men menta ta ta ta ta ta ta ta ta ta ta-
Chunk length: 249

--- Document Chunk 781 ---
tion tion tion tion tion tion tion tion tion tion tion, which consists of modifying the training
samples with hand-designed random transfor-
mations that do not change the semantic content
of the image, such as cropping, scaling, mirror-
Chunk length: 237

--- Document Chunk 782 ---
of the image, such as cropping, scaling, mirror-
ing, or color changes.
120
Chunk length: 75

--- Document Chunk 783 ---
6.3 Object detection
A more complex task for image understanding is
Chunk length: 67

--- Document Chunk 784 ---
ob ob ob ob ob ob ob ob ob ob object ject ject ject ject ject ject ject ject ject jectde de de de de de de de de de detec tec tec tec tec tec tec tec tec tec tection tion tion tion tion tion tion tion tion tion tion, in which the objective is, given
Chunk length: 249

--- Document Chunk 785 ---
an input image, to predict the classes and posi-
tions of objects of interest.
An object position is formalized as the four co-
ordinates (x1,y1,x2,y2) of a rectangular bound-
ing box, and the ground truth associated with
Chunk length: 221

--- Document Chunk 786 ---
ing box, and the ground truth associated with
each training image is a list of such bounding
boxes, each labeled with the class of the object
contained therein.
The standard approach to solve this task, for in-
Chunk length: 210

--- Document Chunk 787 ---
stance, by the Sin Sin Sin Sin Sin Sin Sin Sin Sin Sin Single gle gle gle gle gle gle gle gle gle gleShot Shot Shot Shot Shot Shot Shot Shot Shot Shot ShotDe De De De De De De De De De Detec tec tec tec tec tec tec tec tec tec tector tor tor tor tor
Chunk length: 249

--- Document Chunk 788 ---
tec tec tec tec tec tec tector tor tor tor tor tor tor tor tor tor tor(SSD SSD SSD SSD SSD SSD SSD SSD SSD SSD SSD) [Liu
Chunk length: 120

--- Document Chunk 789 ---
et al., 2015]), is to use a convolutional neural
network that produces a sequence of image
representations Zs of size Ds Ã—Hs Ã—Ws, s=
1,...,S , with decreasing spatial resolution Hs Ã—
Ws down to1Ã—1 for s = S (see Figure 6.1). Each
Chunk length: 229

--- Document Chunk 790 ---
Ws down to1Ã—1 for s = S (see Figure 6.1). Each
of these tensors covers the input image in full, so
the h,w indices correspond to a partitioning of
the image lattice into regular squares that gets
coarser when s increases.
Chunk length: 221

--- Document Chunk 791 ---
coarser when s increases.
As seen in Â§ 4.2, and illustrated in Figure 4.4,
Chunk length: 74

--- Document Chunk 792 ---
due to the succession of con con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay
Chunk length: 247

--- Document Chunk 793 ---
tionallay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers, a
Chunk length: 95

--- Document Chunk 794 ---
feature vector (Zs[0,h,w ],...,Z s[Ds âˆ’1,h,w ])
is a descriptor of an area of the image, called its
121
Chunk length: 103

--- Document Chunk 795 ---
X
Z1
Z2
ZSâˆ’1 ZS
...
...
Figure 6.1: A convolutional object detector processes the
input image to generate a sequence of representations
of decreasing resolutions. It computes for every h,w, at
every scale s, a pre-defined number of bounding boxes
Chunk length: 246

--- Document Chunk 796 ---
whose centers are in the image area corresponding to
that cell, and whose sizes are such that they fit in its
receptive field. Each prediction takes the form of the
estimates (Ë†x1, Ë†x2, Ë†y1, Ë†y2), represented by the red boxes
Chunk length: 225

--- Document Chunk 797 ---
above, and a vector of C +1 logits for the C classes of
interest, and an additional â€œno objectâ€ class.
122
Chunk length: 106

--- Document Chunk 798 ---
Figure 6.2: Examples of object detection with the Single-
Shot Detector [Liu et al., 2015].
123
Chunk length: 95

--- Document Chunk 799 ---
re re re re re re re re re re recep cep cep cep cep cep cep cep cep cep ceptive tive tive tive tive tive tive tive tive tive tivefield field field field field field field field field field field, that is larger than this square but
Chunk length: 231

--- Document Chunk 800 ---
centered on it. This results in a non-ambiguous
matching of any bounding box (x1,x2,y1,y2) to
a s,h,w , determined respectively by max(x2 âˆ’
x1,y2 âˆ’y1), y1+y2
2 , and x1+x2
2 .
Detection is achieved by addingS convolutional
Chunk length: 222

--- Document Chunk 801 ---
Detection is achieved by addingS convolutional
layers, each processing a Zs and computing, for
every tensor indices h,w, the coordinates of a
bounding box and the associated logits. If there
are C object classes, there are C +1 logits, the
Chunk length: 239

--- Document Chunk 802 ---
are C object classes, there are C +1 logits, the
additional one standing for â€œno object. â€ Hence,
each additional convolution layer has 4+ C +1
output channels. The SSD algorithm in particu-
lar generates several bounding boxes per s,h,w ,
Chunk length: 239

--- Document Chunk 803 ---
lar generates several bounding boxes per s,h,w ,
each dedicated to a hard-coded range of aspect
ratios.
Training sets for object detection are costly to
create, since the labeling with bounding boxes
requires a slow human intervention. To mitigate
Chunk length: 247

--- Document Chunk 804 ---
requires a slow human intervention. To mitigate
this issue, the standard approach is to fine fine fine fine fine fine fine fine fine fine fine-tune tune tune tune tune tune tune tune tune tune tune
Chunk length: 197

--- Document Chunk 805 ---
a convolutional model that has been pre pre pre pre pre pre pre pre pre pre pre-trained trained trained trained trained trained trained trained trained trained trained
on a large classification dataset such as VGG-16
Chunk length: 216

--- Document Chunk 806 ---
on a large classification dataset such as VGG-16
for the original SSD, and to replace its final fully-
connected layers with additional convolutional
ones. Surprisingly, models trained for classifica-
tion only learn feature representations that can
Chunk length: 249

--- Document Chunk 807 ---
tion only learn feature representations that can
be repurposed for object detection, even though
124
Chunk length: 100

--- Document Chunk 808 ---
that task involves the regression of geometric
quantities.
During training, every ground-truth bounding
box is associated with its s,h,w , and induces a
loss term composed of a cross-entropy loss for
the logits, and a regression loss such as MSE
Chunk length: 245

--- Document Chunk 809 ---
the logits, and a regression loss such as MSE
for the bounding box coordinates. Every other
s,h,w free of bounding-box match induces a
cross-entropy only penalty to predict the class
â€œno objectâ€.
125
Chunk length: 199

--- Document Chunk 810 ---
6.4 Semantic segmentation
The finest-grain prediction task for image under-
Chunk length: 75

--- Document Chunk 811 ---
standing is se se se se se se se se se se seman man man man man man man man man man mantic tic tic tic tic tic tic tic tic tic ticseg seg seg seg seg seg seg seg seg seg segmen men men men men men men men men men menta ta ta ta ta ta ta ta ta ta
Chunk length: 245

--- Document Chunk 812 ---
men men men men menta ta ta ta ta ta ta ta ta ta tation tion tion tion tion tion tion tion tion tion tion, which con-
Chunk length: 117

--- Document Chunk 813 ---
sists of predicting, for each pixel, the class of the
object to which it belongs. This can be achieved
with a standard convolutional neural network
that outputs a convolutional map with as many
channels as classes, carrying the estimated logits
Chunk length: 244

--- Document Chunk 814 ---
for every pixel.
While a standard residual network, for instance,
can generate a dense output of the same reso-
lution as its input, as for object detection, this
task requires operating at multiple scales. This
Chunk length: 211

--- Document Chunk 815 ---
task requires operating at multiple scales. This
is necessary so that any object, or sufficiently
informative sub-part, regardless of its size, is
captured somewhere in the model by the feature
representation at a single tensor position. Hence,
Chunk length: 244

--- Document Chunk 816 ---
standard architectures for this task downscale
Chunk length: 46

--- Document Chunk 817 ---
the image with a series of con con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay
Chunk length: 249

--- Document Chunk 818 ---
tionallay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers
Chunk length: 92

--- Document Chunk 819 ---
to increase the receptive field of the activations,
Chunk length: 51

--- Document Chunk 820 ---
and re-upscale it with a series oftrans trans trans trans trans trans trans trans trans trans transposed posed posed posed posed posed posed posed posed posed posedcon con con con con con con con con con con-
Chunk length: 208

--- Document Chunk 821 ---
vo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers, or other upscaling
Chunk length: 246

--- Document Chunk 822 ---
ers ers ers ers ers ers ers, or other upscaling methods
Chunk length: 55

--- Document Chunk 823 ---
such as bilinear interpolation, to make the pre-
diction at high resolution.
However, a strict downscaling-upscaling archi-
tecture does not allow for operating at a fine
126
Chunk length: 174

--- Document Chunk 824 ---
Figure 6.3: Semantic segmentation results with the
Pyramid Scene Parsing Network [Zhao et al., 2016].
grain when making the final prediction, since all
the signal has been transmitted through a low-
resolution representation at some point. Models
Chunk length: 246

--- Document Chunk 825 ---
resolution representation at some point. Models
that apply such downscaling-upscaling serially
Chunk length: 94

--- Document Chunk 826 ---
mitigate these issues withskip skip skip skip skip skip skip skip skip skip skipcon con con con con con con con con con connec nec nec nec nec nec nec nec nec nec nections tions tions tions tions tions tions tions tions tions tionsfrom
Chunk length: 235

--- Document Chunk 827 ---
layers at a certain resolution, before downscal-
ing, to layers at the same resolution, after upscal-
ing [Long et al., 2014; Ronneberger et al., 2015].
Models that do it in parallel, after a convolutional
127
Chunk length: 209

--- Document Chunk 828 ---
backbone, concatenate the resulting multi-scale
representation after upscaling, before making
the final per-pixel prediction [Zhao et al., 2016].
Training is achieved with a standard cross-
entropy summed over all the pixels. As for ob-
Chunk length: 236

--- Document Chunk 829 ---
entropy summed over all the pixels. As for ob-
ject detection, training can start from a net net net net net net net net net net network work work work work work work work work work work
Chunk length: 186

--- Document Chunk 830 ---
pre pre pre pre pre pre pre pre pre pre pre-trained trained trained trained trained trained trained trained trained trained trainedon a large-scale image classification
dataset to compensate for the limited availability
of segmentation ground truth.
Chunk length: 249

--- Document Chunk 831 ---
of segmentation ground truth.
128
Chunk length: 33

--- Document Chunk 832 ---
6.5 Speech recognition
Chunk length: 22

--- Document Chunk 833 ---
Speech Speech Speech Speech Speech Speech Speech Speech Speech Speech Speechrecog recog recog recog recog recog recog recog recog recog recogni ni ni ni ni ni ni ni ni ni nition tion tion tion tion tion tion tion tion tion tionconsists of converting
Chunk length: 249

--- Document Chunk 834 ---
tion tion tion tion tionconsists of converting a
Chunk length: 48

--- Document Chunk 835 ---
sound sample into a sequence of words. There
have been plenty of approaches to this problem
historically, but a conceptually simple and recent
one proposed by Radford et al. [2022] consists of
casting it as a sequence-to-sequence translation
Chunk length: 241

--- Document Chunk 836 ---
casting it as a sequence-to-sequence translation
and then solving it with a standard attention-
Chunk length: 95

--- Document Chunk 837 ---
and then solving it with a standard attention-
based Trans Trans Trans Trans Trans Trans Trans Trans Trans Trans Transformer former former former former former former former former former former, as described in Â§ 5.3.
Chunk length: 218

--- Document Chunk 838 ---
Their model first converts the sound signal into a
spectrogram, which is a one-dimensional series
T Ã—D, that encodes at every time step a vector
of energies in D frequency bands. The associ-
Chunk length: 190

--- Document Chunk 839 ---
ated text is encoded with the BPE BPE BPE BPE BPE BPE BPE BPE BPE BPE BPEto to to to to to to to to to tok k k k k k k k k k kenizer enizer enizer enizer enizer enizer enizer enizer enizer enizer enizer(see
Â§ 3.2).
Chunk length: 214

--- Document Chunk 840 ---
Â§ 3.2).
The spectrogram is processed through a few
Chunk length: 50

--- Document Chunk 841 ---
1D con con con con con con con con con con convo vo vo vo vo vo vo vo vo vo volu lu lu lu lu lu lu lu lu lu lutional tional tional tional tional tional tional tional tional tional tionallay lay lay lay lay lay lay lay lay lay layers ers ers ers ers
Chunk length: 248

--- Document Chunk 842 ---
lay lay lay lay lay lay layers ers ers ers ers ers ers ers ers ers ers, and the resulting rep-
Chunk length: 94

--- Document Chunk 843 ---
resentation is fed into the encoder of the Trans-
former. The decoder directly generates a discrete
sequence of tokens, that correspond to one of
the possible tasks considered during training.
Multiple objectives are considered: transcription
Chunk length: 242

--- Document Chunk 844 ---
Multiple objectives are considered: transcription
of English or non-English text, translation from
any language to English, or detection of non-
speech sequences, such as background music or
ambient noise.
129
Chunk length: 209

--- Document Chunk 845 ---
This approach allows leveraging extremely large
datasets that combine multiple types of sound
sources with diverse ground truths.
It is noteworthy that even though the ultimate
goal of this approach is to produce a transla-
Chunk length: 223

--- Document Chunk 846 ---
goal of this approach is to produce a transla-
tion as deterministic as possible given the input
signal, it is formally the sampling of a text dis-
tribution conditioned on a sound sample, hence
a synthesis process. The decoder is, in fact, ex-
Chunk length: 244

--- Document Chunk 847 ---
a synthesis process. The decoder is, in fact, ex-
tremely similar to the generative model of Â§ 7.1.
130
Chunk length: 103

--- Document Chunk 848 ---
6.6 Text-image representations
A powerful approach to image understanding
consists of learning consistent image and text
representations, such that an image, or a textual
description of it, would be mapped to the same
feature vector.
Chunk length: 233

--- Document Chunk 849 ---
The Con Con Con Con Con Con Con Con Con Con Contrastive trastive trastive trastive trastive trastive trastive trastive trastive trastive trastiveLan Lan Lan Lan Lan Lan Lan Lan Lan Lan Language guage guage guage guage guage guage guage guage guage
Chunk length: 247

--- Document Chunk 850 ---
guage guage guage guage guage guage guage guage guage-Im Im Im Im Im Im Im Im Im Im Image age age age age age age age age age agePre Pre Pre Pre Pre Pre Pre Pre Pre Pre Pre-train train train train train train train train train train training ing ing
Chunk length: 249

--- Document Chunk 851 ---
train train train train train training ing ing ing ing ing ing ing ing ing ing
Chunk length: 78

--- Document Chunk 852 ---
(CLIP CLIP CLIP CLIP CLIP CLIP CLIP CLIP CLIP CLIP CLIP) proposed by Radford et al. [2021] com-
bines an image encoder f, which is a ViT ViT ViT ViT ViT ViT ViT ViT ViT ViT ViT, and
Chunk length: 181

--- Document Chunk 853 ---
a text encoder g, which is a GPT GPT GPT GPT GPT GPT GPT GPT GPT GPT GPT. See Â§ 5.3 for
both.
To repurpose a GPT as a text encoder, instead of a
standard autoregressive model, they add an â€œend
of sentenceâ€ token to the input sequence, and use
Chunk length: 242

--- Document Chunk 854 ---
of sentenceâ€ token to the input sequence, and use
the representation of this token in the last layer
as the embedding. Its dimension is between 512
and 1024, depending on the configuration.
Those two models are trained from scratch using
Chunk length: 237

--- Document Chunk 855 ---
Those two models are trained from scratch using
a dataset of 400 million image-text pairs (ik,tk)
collected from the internet. The training proce-
dure follows the standard mini-batch stochastic
Chunk length: 194

--- Document Chunk 856 ---
dure follows the standard mini-batch stochastic
gradient descent approach but relies on a con con con con con con con con con con con-
Chunk length: 134

--- Document Chunk 857 ---
trastive trastive trastive trastive trastive trastive trastive trastive trastive trastive trastiveloss loss loss loss loss loss loss loss loss loss loss. The embeddings are computed for
every image and every text of the N pairs in the
Chunk length: 234

--- Document Chunk 858 ---
every image and every text of the N pairs in the
mini-batch, and a cosine similarity measure is
computed not only between text and image em-
131
Chunk length: 144

--- Document Chunk 859 ---
beddings from each pair, but also across pairs, re-
sulting in an N Ã—N matrix of similarity scores:
lm,n = f(im)Â·g(tn), m= 1,...,N,n = 1,...,N.
The model is trained with cross-entropy so that,
âˆ€n the values l1,n,...,l N,n interpreted as logit
Chunk length: 242

--- Document Chunk 860 ---
âˆ€n the values l1,n,...,l N,n interpreted as logit
scores predict n, and similarly for ln,1,...,l n,N .
This means that âˆ€n,m, s.t. n Ì¸= m the similarity
ln,n is unambiguously greater than bothln,m and
lm,n.
Chunk length: 205

--- Document Chunk 861 ---
lm,n.
When it has been trained, this model can be used
Chunk length: 54

--- Document Chunk 862 ---
to do zero zero zero zero zero zero zero zero zero zero zero-shot shot shot shot shot shot shot shot shot shot shotpre pre pre pre pre pre pre pre pre pre predic dic dic dic dic dic dic dic dic dic diction tion tion tion tion tion tion tion tion
Chunk length: 245

--- Document Chunk 863 ---
diction tion tion tion tion tion tion tion tion tion tion, that is, classifying a
Chunk length: 81

--- Document Chunk 864 ---
signal in the absence of training examples by
defining a series of candidate classes with text
descriptions, and computing the similarity of the
embedding of an image with the embedding of
each of those descriptions (see Figure 6.4).
Chunk length: 233

--- Document Chunk 865 ---
each of those descriptions (see Figure 6.4).
Additionally, since the textual descriptions are
often detailed, such a model has to capture a
richer representation of images and pick up cues
beyond what is necessary for instance for classifi-
Chunk length: 240

--- Document Chunk 866 ---
cation. This translates to excellent performance
on challenging datasets such as ImageNet Adver-
sarial [Hendrycks et al., 2019] which was specifi-
cally designed to degrade or erase cues on which
standard predictors rely.
132
Chunk length: 226

--- Document Chunk 867 ---
Figure 6.4: The CLIP text-image embedding [Radford
et al., 2021] allows for zero-shot prediction by predicting
which class description embedding is the most consis-
tent with the image embedding.
133
Chunk length: 199

--- Document Chunk 868 ---
6.7 Reinforcement learning
Many problems, such as strategy games or
robotic control, can be formalized with a discrete-
time state process St and reward process Rt that
can be modulated by choosing actions At. If
Chunk length: 212

--- Document Chunk 869 ---
can be modulated by choosing actions At. If
St is Marko Marko Marko Marko Marko Marko Marko Marko Marko Marko Markovian vian vian vian vian vian vian vian vian vian vian, meaning that it carries alone
as much information about the future as all the
Chunk length: 248

--- Document Chunk 870 ---
as much information about the future as all the
past states until that instant, such an object is a
Chunk length: 99

--- Document Chunk 871 ---
Marko Marko Marko Marko Marko Marko Marko Marko Marko Marko Markovian vian vian vian vian vian vian vian vian vian vianDe De De De De De De De De De Deci ci ci ci ci ci ci ci ci ci cision sion sion sion sion sion sion sion sion sion sionPro Pro Pro
Chunk length: 248

--- Document Chunk 872 ---
sion sion sion sion sion sion sionPro Pro Pro Pro Pro Pro Pro Pro Pro Pro Process cess cess cess cess cess cess cess cess cess cess (MDP MDP MDP MDP MDP MDP MDP MDP MDP MDP MDP)
Chunk length: 177

--- Document Chunk 873 ---
.
Chunk length: 1

--- Document Chunk 874 ---
Given an MDP, the objective is classically to find
a pol pol pol pol pol pol pol pol pol pol policy icy icy icy icy icy icy icy icy icy icyÏ€ such that At = Ï€(St) maximizes the
Chunk length: 175

--- Document Chunk 875 ---
expectation of the re re re re re re re re re re return turn turn turn turn turn turn turn turn turn turn, which is an accumu-
lated discounted reward:
E
ï£®
ï£°X
tâ‰¥0
Î³tRt
ï£¹
ï£»,
for a discount factor 0 < Î³ <1.
Chunk length: 204

--- Document Chunk 876 ---
ï£»,
for a discount factor 0 < Î³ <1.
This is the standard setup of Re Re Re Re Re Re Re Re Re Re Rein in in in in in in in in in inforce force force force force force force force force force forcement ment ment ment ment ment ment ment ment ment ment
Chunk length: 248

--- Document Chunk 877 ---
Learn Learn Learn Learn Learn Learn Learn Learn Learn Learn Learning ing ing ing ing ing ing ing ing ing ing(RL RL RL RL RL RL RL RL RL RL RL), and it can be worked out by intro-
ducing the optimal state-action value function
Chunk length: 225

--- Document Chunk 878 ---
ducing the optimal state-action value function
Q(s,a) which is the expected return if we exe-
cute action a in state s, and then follow the op op op op op op op op op op opti ti ti ti ti ti ti ti ti ti ti-
Chunk length: 205

--- Document Chunk 879 ---
mal mal mal mal mal mal mal mal mal mal malpol pol pol pol pol pol pol pol pol pol policy icy icy icy icy icy icy icy icy icy icy. It provides a means to compute the
optimal policy as Ï€(s) = argmaxa Q(s,a), and,
Chunk length: 211

--- Document Chunk 880 ---
optimal policy as Ï€(s) = argmaxa Q(s,a), and,
thanks to the Markovian assumption, it verifies
134
Chunk length: 97

--- Document Chunk 881 ---
the Bell Bell Bell Bell Bell Bell Bell Bell Bell Bell Bellman man man man man man man man man man manequa equa equa equa equa equa equa equa equa equa equation tion tion tion tion tion tion tion tion tion tion:
Q(s,a) = (6.1)
E

Rt +Î³max
aâ€²
Chunk length: 241

--- Document Chunk 882 ---
Q(s,a) = (6.1)
E

Rt +Î³max
aâ€²
Q(St+1,aâ€²)
St = s,At = a

,
from which we can design a procedure to train
a parametric model Q(Â·, Â·;w).
To apply this framework to play classical Atari
video games, Mnih et al. [2015] use forSt the con-
Chunk length: 238

--- Document Chunk 883 ---
catenation of the frame at time t and the three
that precede, so that the Markovian assumption
is reasonable, and use for Q a model dubbed the
Chunk length: 142

--- Document Chunk 884 ---
Deep Deep Deep Deep Deep Deep Deep Deep Deep Deep DeepQ Q Q Q Q Q Q Q Q Q Q-Net Net Net Net Net Net Net Net Net Net Network work work work work work work work work work work(DQN DQN DQN DQN DQN DQN DQN DQN DQN DQN DQN), composed of two con-
Chunk length: 240

--- Document Chunk 885 ---
volutional layers and one fully connected layer
with one output value per action, following the
classical structure of a LeNet (see Â§ 5.2).
Training is achieved by alternatively playing and
recording episodes, and building mini-batches of
Chunk length: 238

--- Document Chunk 886 ---
recording episodes, and building mini-batches of
tuples (sn,an,rn,sâ€²n) âˆ¼ (St,At,Rt,St+1) taken
across stored episodes and time steps, and mini-
mizing
â„’(w) = 1
N
NX
n=1
(Q(sn,an;w)âˆ’yn)2 (6.2)
with one iteration of SGD, where yn = rn if this
Chunk length: 240

--- Document Chunk 887 ---
with one iteration of SGD, where yn = rn if this
tuple is the end of the episode, and yn = rn +
Î³maxa Q(sâ€²n,a; Â¯w) otherwise.
135
Chunk length: 129

--- Document Chunk 888 ---
Frame number
Value
Figure 6.5: This graph shows the evolution of the state
value V (St) = maxa Q(St,a) during a game of Break-
out. The spikes at time points (1) and (2) correspond to
clearing a brick, at time point (3) it is about to break
Chunk length: 240

--- Document Chunk 889 ---
through to the top line, and at (4) it does, which ensures
a high future reward [Mnih et al., 2015].
Here Â¯w is a constant copy of w, i.e. the gradient
does not propagate through it to w. This is nec-
essary since the target value in Equation 6.1 is
Chunk length: 249

--- Document Chunk 890 ---
essary since the target value in Equation 6.1 is
the expectation of yn, while it is yn itself which
is used in Equation 6.2. Fixing w in yn results in
a better approximation of the desirable gradient.
Chunk length: 200

--- Document Chunk 891 ---
a better approximation of the desirable gradient.
A key issue is the policy used to collect episodes.
Mnih et al. [2015] simply use the Ïµ-greedy strat-
egy, which consists of taking an action com-
pletely at random with probability Ïµ, and the
Chunk length: 242

--- Document Chunk 892 ---
pletely at random with probability Ïµ, and the
optimal action argmaxa Q(s,a) otherwise. In-
jecting a bit of randomness is necessary to favor
136
Chunk length: 144

--- Document Chunk 893 ---
exploration.
Training is done with ten million frames corre-
sponding to a bit less than eight days of game-
play. The trained network computes accurate
estimates of the state values (see Figure 6.5), and
Chunk length: 204

--- Document Chunk 894 ---
reaches human performance on a majority of the
49 games used in the experimental validation.
137
Chunk length: 96

--- Document Chunk 895 ---
Chapter 7
Synthesis
A second category of applications distinct from
prediction is synthesis. It consists of fitting a
density model to training samples and providing
means to sample from this model.
138
Chunk length: 202

--- Document Chunk 896 ---
7.1 Text generation
The standard approach to text text text text text text text text text text textsyn syn syn syn syn syn syn syn syn syn synthe the the the the the the the the the thesis sis sis sis sis sis sis sis sis sis sisis to
Chunk length: 233

--- Document Chunk 897 ---
use an attention-based, au au au au au au au au au au autore tore tore tore tore tore tore tore tore tore toregres gres gres gres gres gres gres gres gres gres gressive sive sive sive sive sive sive sive sive sive sivemodel model model model model
Chunk length: 247

--- Document Chunk 898 ---
sive sive sive sivemodel model model model model model model model model model model
Chunk length: 84

--- Document Chunk 899 ---
. A
Chunk length: 3

--- Document Chunk 900 ---
very successful model proposed by Radford et al.
[2018], is the GPT GPT GPT GPT GPT GPT GPT GPT GPT GPT GPT which we described in Â§ 5.3.
This architecture has been used for very large
models, such as OpenAIâ€™s 175-billion-parameter
Chunk length: 230

--- Document Chunk 901 ---
models, such as OpenAIâ€™s 175-billion-parameter
GPT-3 [Brown et al., 2020]. It is composed of 96
self-attention blocks, each with 96 heads, and
processes tokens of dimension 12,288, with a
hidden dimension of 49,512 in the MLPs of the
Chunk length: 233

--- Document Chunk 902 ---
hidden dimension of 49,512 in the MLPs of the
attention blocks.
When such a model is trained on a very large
Chunk length: 108

--- Document Chunk 903 ---
dataset, it results in a Large Large Large Large Large Large Large Large Large Large LargeLan Lan Lan Lan Lan Lan Lan Lan Lan Lan Language guage guage guage guage guage guage guage guage guage guageModel Model Model Model Model Model Model Model
Chunk length: 245

--- Document Chunk 904 ---
Model Model Model Model Model Model Model Model Model Model
Chunk length: 59

--- Document Chunk 905 ---
(LLM LLM LLM LLM LLM LLM LLM LLM LLM LLM LLM), which exhibits extremely powerful prop-
erties. Besides the syntactic and grammatical
structure of the language, it has to integrate
very diverse knowledge, e.g. to predict the word
Chunk length: 228

--- Document Chunk 906 ---
very diverse knowledge, e.g. to predict the word
following â€œThe capital of Japan isâ€, â€œif water is
heated to 100 Celsius degrees it turns intoâ€, or
â€œbecause her puppy was sick, Jane wasâ€.
This results in particular in the ability to solve
Chunk length: 238

--- Document Chunk 907 ---
few few few few few few few few few few few-shot shot shot shot shot shot shot shot shot shot shotpre pre pre pre pre pre pre pre pre pre predic dic dic dic dic dic dic dic dic dic diction tion tion tion tion tion tion tion tion tion tion, where
Chunk length: 245

--- Document Chunk 908 ---
tion tion tion tion tion tion tion tion, where only a handful of
Chunk length: 64

--- Document Chunk 909 ---
training examples are available, as illustrated
in Figure 7.1. More surprisingly, when given
a carefully crafted prompt prompt prompt prompt prompt prompt prompt prompt prompt prompt prompt, it can exhibit abil-
139
Chunk length: 215

--- Document Chunk 910 ---
I: I love apples, O: positive, I: music is my passion, O:
positive, I: my job is boring, O: negative, I: frozen pizzas
are awesome, O: positive,
I: I love apples, O: positive, I: music is my passion, O:
Chunk length: 202

--- Document Chunk 911 ---
positive, I: my job is boring, O: negative, I: frozen pizzas
taste like cardboard, O: negative,
I: water boils at 100 degrees, O: physics, I: the square
root of two is irrational, O: mathematics, I: the set of
Chunk length: 209

--- Document Chunk 912 ---
prime numbers is infinite, O: mathematics, I: gravity is
proportional to the mass, O: physics,
I: water boils at 100 degrees, O: physics, I: the square
root of two is irrational, O: mathematics, I: the set of
Chunk length: 208

--- Document Chunk 913 ---
prime numbers is infinite, O: mathematics, I: squares
are rectangles, O: mathematics,
Figure 7.1: Examples of few-shot prediction with a 120
million parameter GPT model from Hugging Face. In
each example, the beginning of the sentence was given
Chunk length: 244

--- Document Chunk 914 ---
as a prompt prompt prompt prompt prompt prompt prompt prompt prompt prompt prompt, and the model generated the part in bold.
ities for question answering, problem solving,
Chunk length: 171

--- Document Chunk 915 ---
and chain chain chain chain chain chain chain chain chain chain chain-of of of of of of of of of of of-thought thought thought thought thought thought thought thought thought thought thoughtthat appear eerily close to
Chunk length: 217

--- Document Chunk 916 ---
high-level reasoning [Chowdhery et al., 2022;
Bubeck et al., 2023].
Due to these remarkable capabilities, these mod-
Chunk length: 116

--- Document Chunk 917 ---
els are sometimes called foun foun foun foun foun foun foun foun foun foun founda da da da da da da da da da dation tion tion tion tion tion tion tion tion tion tionmod mod mod mod mod mod mod mod mod mod models els els els els els els els els els
Chunk length: 247

--- Document Chunk 918 ---
mod models els els els els els els els els els els
Chunk length: 50

--- Document Chunk 919 ---
[Bommasani et al., 2021].
However, even though it integrates a very large
body of knowledge, such a model may be inad-
140
Chunk length: 122

--- Document Chunk 920 ---
equate for practical applications, in particular
when interacting with human users. In many
situations, one needs responses that follow the
statistics of a helpful dialog with an assistant.
This differs from the statistics of available large
Chunk length: 241

--- Document Chunk 921 ---
training sets, which combine novels, encyclope-
dias, forum messages, and blog posts.
Chunk length: 85

--- Document Chunk 922 ---
dias, forum messages, and blog posts.
This discrepancy is addressed by fine fine fine fine fine fine fine fine fine fine fine-tun tun tun tun tun tun tun tun tun tun tuning ing ing ing ing ing ing ing ing ing ing
Chunk length: 212

--- Document Chunk 923 ---
such a language model (see Â§ 3.6). The current
Chunk length: 46

--- Document Chunk 924 ---
dominant strategy is Re Re Re Re Re Re Re Re Re Re Rein in in in in in in in in in inforce force force force force force force force force force forcement ment ment ment ment ment ment ment ment ment mentLearn Learn Learn Learn Learn Learn Learn
Chunk length: 245

--- Document Chunk 925 ---
mentLearn Learn Learn Learn Learn Learn Learn Learn Learn Learn Learning ing ing ing ing ing ing ing ing ing ing
Chunk length: 112

--- Document Chunk 926 ---
from from from from from from from from from from fromHu Hu Hu Hu Hu Hu Hu Hu Hu Hu Human man man man man man man man man man manFeed Feed Feed Feed Feed Feed Feed Feed Feed Feed Feedback back back back back back back back back back back(RLHF RLHF
Chunk length: 247

--- Document Chunk 927 ---
back back back back back back back back(RLHF RLHF RLHF RLHF RLHF RLHF RLHF RLHF RLHF RLHF RLHF) [Ouyang et al
Chunk length: 109

--- Document Chunk 928 ---
.,
Chunk length: 2

--- Document Chunk 929 ---
2022], which consists of creating small labeled
training sets by asking users to either write
responses or provide ratings of generated re-
sponses. The former can be used as-is to fine-
tune the language model, and the latter can be
Chunk length: 233

--- Document Chunk 930 ---
tune the language model, and the latter can be
used to train a reward network that predicts
the rating and use it as a target to fine-tune the
Chunk length: 142

--- Document Chunk 931 ---
language model with a standard Re Re Re Re Re Re Re Re Re Re Rein in in in in in in in in in inforce force force force force force force force force force forcement ment ment ment ment ment ment ment ment ment ment
Chunk length: 214

--- Document Chunk 932 ---
Learn Learn Learn Learn Learn Learn Learn Learn Learn Learn Learning ing ing ing ing ing ing ing ing ing ing approach.
141
Chunk length: 122

--- Document Chunk 933 ---
7.2 Image generation
Multiple deep methods have been developed to
model and sample from a high-dimensional den-
Chunk length: 111

--- Document Chunk 934 ---
sity. A powerful approach for im im im im im im im im im im image age age age age age age age age age agesyn syn syn syn syn syn syn syn syn syn synthe the the the the the the the the the thesis sis sis sis sis sis sis sis sis sis sis
Chunk length: 234

--- Document Chunk 935 ---
relies on inverting a diffusion process. Such a
generative model is referred to, somehow incor-
Chunk length: 95

--- Document Chunk 936 ---
rectly, as a dif dif dif dif dif dif dif dif dif dif diffu fu fu fu fu fu fu fu fu fu fusion sion sion sion sion sion sion sion sion sion sionmodel model model model model model model model model model model.
Chunk length: 208

--- Document Chunk 937 ---
The principle consists of defining analytically
a process that gradually degrades any sample,
and consequently transforms the complex and
unknown density of the data into a simple and
well-known density such as a normal, and train-
Chunk length: 231

--- Document Chunk 938 ---
well-known density such as a normal, and train-
ing a deep architecture to invert this degradation
process [Ho et al., 2020].
Given a fixed T, the diffusion process defines a
probability distribution over series of T +1 im-
Chunk length: 223

--- Document Chunk 939 ---
probability distribution over series of T +1 im-
ages as follows: sample x0 uniformly from the
dataset, and then sequentially sample xt+1 âˆ¼
p(xt+1 | xt),t = 0,...,T âˆ’1, where the condi-
tional distribution p is defined analytically and
Chunk length: 235

--- Document Chunk 940 ---
tional distribution p is defined analytically and
such that it gradually erases the structure that
was in x0. The setup should degrade the signal
so much that the distributionp(xT ) has a known
analytical form which can be sampled.
Chunk length: 231

--- Document Chunk 941 ---
analytical form which can be sampled.
For instance, Ho et al. [2020] normalize the data
to have a mean of0 and a variance of1, and their
142
Chunk length: 140

--- Document Chunk 942 ---
xT
x0
Figure 7.2: Image synthesis with denoising diffusion
[Ho et al., 2020]. Each sample starts as a white noise
xT (top), and is gradually de-noised by sampling iter-
atively xtâˆ’1 | xt âˆ¼ ð’© (xt +f(xt,t;w),Ïƒt).
143
Chunk length: 214

--- Document Chunk 943 ---
diffusion process consists of adding a bit of white
noise and re-normalizing the variance to 1. This
process exponentially reduces the importance of
x0, and xtâ€™s density can rapidly be approximated
with a normal.
Chunk length: 212

--- Document Chunk 944 ---
with a normal.
The denoiser f is a deep architecture that
should model and allow sampling from
f(xtâˆ’1,xt,t;w) â‰ƒ p(xtâˆ’1 | xt). It can be shown,
Chunk length: 142

--- Document Chunk 945 ---
thanks to a vari vari vari vari vari vari vari vari vari vari varia a a a a a a a a a ational tional tional tional tional tional tional tional tional tional tionalbound bound bound bound bound bound bound bound bound bound bound, that if this
Chunk length: 242

--- Document Chunk 946 ---
one-step reverse process is accurate enough,
sampling xT âˆ¼ p(xT ) and denoising T steps
with f results in x0 that follows p(x0).
Training f can be achieved by generating a large
number of sequences x(n)
0 ,...,x (n)
T , picking a tn
Chunk length: 232

--- Document Chunk 947 ---
0 ,...,x (n)
T , picking a tn
in each, and maximizing
X
n
logf

x(n)
tnâˆ’1,x(n)
tn ,tn;w

.
Given their diffusion process, Ho et al. [2020]
have a denoising of the form:
xtâˆ’1 | xt âˆ¼ ð’© (xt +f(xt,t;w);Ïƒt), (7.1)
where Ïƒt is defined analytically.
Chunk length: 244

--- Document Chunk 948 ---
where Ïƒt is defined analytically.
In practice, such a model initially hallucinates
structures by pure luck in the random noise, and
144
Chunk length: 135

--- Document Chunk 949 ---
then gradually builds more elements that emerge
from the noise by reinforcing the most likely
continuation of the image obtained thus far.
This approach can be extended to text-
conditioned synthesis, to generate images
Chunk length: 219

--- Document Chunk 950 ---
conditioned synthesis, to generate images
that match a description. For instance, Nichol
et al. [2021] add to the mean of the denoising
distribution of Equation 7.1 a bias that goes in
the direction of increasing the CLIP matching
Chunk length: 230

--- Document Chunk 951 ---
the direction of increasing the CLIP matching
score (see Â§ 6.6) between the produced image
and the conditioning text description.
145
Chunk length: 133

--- Document Chunk 952 ---
Chapter 8
The Compute Schism
The scale of deep architectures is critical to their
performance and, as we saw in Â§ 3.7, Large Large Large Large Large Large Large Large Large Large LargeLan Lan Lan Lan Lan Lan Lan Lan Lan Lan Lan-
Chunk length: 228

--- Document Chunk 953 ---
guage guage guage guage guage guage guage guage guage guage guageMod Mod Mod Mod Mod Mod Mod Mod Mod Mod Models els els els els els els els els els elsin particular may require amounts
of memory and computation that greatly exceed
Chunk length: 230

--- Document Chunk 954 ---
of memory and computation that greatly exceed
those of consumer hardware.
While training such a model from scratch re-
quires resources available only to large corpora-
tions or public bodies, techniques have been de-
Chunk length: 217

--- Document Chunk 955 ---
tions or public bodies, techniques have been de-
veloped to allow inference and adaptation to spe-
cific tasks under strong resource constraints. Al-
lowing to run models locally instead of through
a provider may be highly desirable for cost or
Chunk length: 244

--- Document Chunk 956 ---
a provider may be highly desirable for cost or
confidentiality reasons.
146
Chunk length: 75

--- Document Chunk 957 ---
8.1 Prompt Engineering
The simplest strategy to specialize or improve a
Large Language Model with a limited computa-
Chunk length: 116

--- Document Chunk 958 ---
tional budget is to use prompt prompt prompt prompt prompt prompt prompt prompt prompt prompt prompten en en en en en en en en en engi gi gi gi gi gi gi gi gi gi gineer neer neer neer neer neer neer neer neer neer neering ing ing ing ing ing ing ing
Chunk length: 249

--- Document Chunk 959 ---
neer neer neering ing ing ing ing ing ing ing ing ing ing, that
Chunk length: 63

--- Document Chunk 960 ---
is, to carefully craft the beginning of the text se-
quence to bias the autoregressive process [Sahoo
et al., 2024]. This approach moves a part of the
information traditionally encoded in the modelâ€™s
parameters to the input.
Chunk length: 224

--- Document Chunk 961 ---
parameters to the input.
We saw in Â§ 7.1 a simple example of few-shot
prediction, to use an LLM for a text classification
task without fine-tuning. A long and sophisti-
cated prompt allows generalizing this strategy
to complex tasks.
Chunk length: 233

--- Document Chunk 962 ---
to complex tasks.
Since the promptâ€™s role is to leverage the â€œgoodâ€
biases that were present in the training set, it
benefits from surprising strategies such as stat-
ing that the response is generated by a skilled
professional [Xu et al., 2023].
Chunk length: 246

--- Document Chunk 963 ---
professional [Xu et al., 2023].
The con con con con con con con con con con context text text text text text text text text text textsize size size size size size size size size size sizeof a language model, that is,
Chunk length: 216

--- Document Chunk 964 ---
the number of tokens it can operate on, directly
modulates the quantity of information that can
be provided in the prompt. This is mostly con-
strained by the computational cost of standard
attention models, which is quadratic with the
Chunk length: 235

--- Document Chunk 965 ---
attention models, which is quadratic with the
context size (see Â§ 4.8).
147
Chunk length: 75

--- Document Chunk 966 ---
Q: Gina has 105 beans, she gives 23 beans to Bob, and
prepares a soup with 53 beans. How many beans are
left? A: There are 29 beans left.
Q: I prepare 53 pancakes, eat 5 of them and give 7 to
Gina. I then prepare 26 more. How many pancakes are
Chunk length: 243

--- Document Chunk 967 ---
left? A: 27 pancakes are left.
Q: Gina has 105 beans, she gives 23 beans to Bob, and
prepares a soup with 53 beans. How many beans are
left? A: Letâ€™s proceed step by step: Gina has 105 beans,
she gives 23 beans to Bob (82 left), and prepares a soup
Chunk length: 248

--- Document Chunk 968 ---
with 53 beans (29 left). So there are 29 beans left.
Q: I prepare 53 pancakes, eat 5 of them and give 7 to
Gina. I then prepare 26 more. How many pancakes are
left? A: Letâ€™s proceed step by step: 53 pancakes, eat 5
Chunk length: 214

--- Document Chunk 969 ---
of them (48 left), give 7 to Gina (41 left), prepare
26 more (67 left). So there are 67 pancakes left.
Figure 8.1: Example of a chain-of-thought to improve
the response of the Llama-3-8B base model. In the
Chunk length: 205

--- Document Chunk 970 ---
the response of the Llama-3-8B base model. In the
two examples, the beginning of the text in normal font
is the prompt, and the generated part is indicated in
bold. The generation without chain-of-thought (top)
Chunk length: 210

--- Document Chunk 971 ---
leads to an incorrect answer, while the generation with
it (bottom) generates a correct answer, by explicitly
producing multiple simple arithmetic operations.
148
Chunk length: 162

--- Document Chunk 972 ---
Chain of Thought
A remarkable type of prompting aims at making
the model generate intermediate steps before
generating the response itself.
Chunk length: 139

--- Document Chunk 973 ---
generating the response itself.
Such a chain chain chain chain chain chain chain chain chain chain chain-of of of of of of of of of of of-thought thought thought thought thought thought thought thought thought thought thought is composed of succes-
Chunk length: 248

--- Document Chunk 974 ---
sive steps that are simpler, hence have been bet-
ter modeled during training, and are predicted
more deterministically [Wei et al., 2022; Kojima
et al., 2022]. See Figure 8.1 for an example.
Retrieval-Augmented Generation
Chunk length: 222

--- Document Chunk 975 ---
Retrieval-Augmented Generation
Prompt engineering can also be put to use to
connect a language model to an external knowl-
edge base. It plays the role of a smart interface
that allows the end user to formulate questions
Chunk length: 220

--- Document Chunk 976 ---
that allows the end user to formulate questions
in natural language and get back a response that
combines information that is not encoded in the
modelâ€™s parameters [Lewis et al., 2020].
Chunk length: 185

--- Document Chunk 977 ---
For such Re Re Re Re Re Re Re Re Re Re Retrieval trieval trieval trieval trieval trieval trieval trieval trieval trieval trieval-Aug Aug Aug Aug Aug Aug Aug Aug Aug Aug Augmented mented mented mented mented mented mented mented mented mented
Chunk length: 241

--- Document Chunk 978 ---
mented mented mented mented mented mented mented mentedGen Gen Gen Gen Gen Gen Gen Gen Gen Gen Gener er er er er er er er er er era a a a a a a a a a ation tion tion tion tion tion tion tion tion tion tion
Chunk length: 205

--- Document Chunk 979 ---
(RAG RAG RAG RAG RAG RAG RAG RAG RAG RAG RAG), an embedding model is used to retrieve
documents whose embedding is correlated to
that of the userâ€™s query. Then, a prompt is con-
structed by joining these retrieved documents
Chunk length: 223

--- Document Chunk 980 ---
structed by joining these retrieved documents
with instructions to combine them, and the
generative model produces the response to the
user.
149
Chunk length: 144

--- Document Chunk 981 ---
8.2 Quantization
Although training or generating multiple
streams can benefit from high-end parallel
computing devices, deployment of a Large
Language Model for individual use requires
generally single-stream inference, which is
Chunk length: 228

--- Document Chunk 982 ---
generally single-stream inference, which is
bounded by memory size and speed far more
than by computation.
As stated in Â§ 2.1, parameters, activations, and
gradients are usually encoded with 32 or 16 bits.
Chunk length: 205

--- Document Chunk 983 ---
gradients are usually encoded with 32 or 16 bits.
The precision it provides is necessary for train-
ing, to allow gradual changes to accumulate.
However, since activations are the sums of many
Chunk length: 192

--- Document Chunk 984 ---
terms, quan quan quan quan quan quan quan quan quan quan quanti ti ti ti ti ti ti ti ti ti tiza za za za za za za za za za zation tion tion tion tion tion tion tion tion tion tionduring inference is miti-
Chunk length: 204

--- Document Chunk 985 ---
gated by an averaging effect. This is even more
true with large architectures, and models quan-
tized down to 6 or 4 bits per parameter exhibit
remarkable performance. Additionally to reduc-
ing the memory footprint, quantization also im-
Chunk length: 238

--- Document Chunk 986 ---
ing the memory footprint, quantization also im-
proves inference speed significantly.
This has motivated the development of soft-
ware to quantize existing models with Post Post Post Post Post Post Post Post Post Post Post-
Chunk length: 223

--- Document Chunk 987 ---
Train Train Train Train Train Train Train Train Train Train Training ing ing ing ing ing ing ing ing ing ingQuan Quan Quan Quan Quan Quan Quan Quan Quan Quan Quanti ti ti ti ti ti ti ti ti ti tiza za za za za za za za za za zation tion tion tion
Chunk length: 245

--- Document Chunk 988 ---
za za za za za za za za za zation tion tion tion tion tion tion tion tion tion tion, and run them in single-
Chunk length: 108

--- Document Chunk 989 ---
stream inference on consumer hardware, such
as llama.cpp [Llama.cpp, 2023]. This framework
implements multiple formats, that apply specific
150
Chunk length: 143

--- Document Chunk 990 ---
2 4 8 16 32
5.5
6
6.5
Size (Gigabytes)
Perplexity
Figure 8.2: Per Per Per Per Per Per Per Per Per Per Perplex plex plex plex plex plex plex plex plex plex plexity ity ity ity ity ity ity ity ity ity ityof quantized versions of the lan-
Chunk length: 235

--- Document Chunk 991 ---
guage models Llama-7B (blue) and 13B (red) [Touvron
et al., 2023] on the wikitext corpus, as a function of
the parametersâ€™ memory footprint. The crosses are the
original FP16 models and the dots correspond to differ-
Chunk length: 216

--- Document Chunk 992 ---
ent levels of quantization with llama.cpp [Llama.cpp,
2023].
quantization levels for the different weight matri-
ces of a language model. For instance the quan-
tization may use more bits for the WVweights
Chunk length: 205

--- Document Chunk 993 ---
tization may use more bits for the WVweights
of the attention blocks, and for the weights of
the feed-forward blocks.
An example of llama.cppâ€™s quantization is Q4_1.
151
Chunk length: 169

--- Document Chunk 994 ---
It quantizes individually sub-blocks of32 entries
of the original weight matrix by storing for each
a scaling factor d and a bias m in the original
FP16 encoding, and encoding each entryx with 4
bits as a value q âˆˆ {0,..., 24 âˆ’1}. The resulting
Chunk length: 244

--- Document Chunk 995 ---
bits as a value q âˆˆ {0,..., 24 âˆ’1}. The resulting
de-quantized value being Ëœx = dq +m.
Such a block was encoded originally as32 values
in FP16, hence 64 bytes, while the quantized
version needs 4 bytes for q and m and 32Â·4 bits
Chunk length: 227

--- Document Chunk 996 ---
version needs 4 bytes for q and m and 32Â·4 bits
= 16 bytes for the entries, hence a total of 20
bytes.
Such an aggressive quantization surprisingly de-
grades only marginally the performance of the
models, as illustrated on Figure 8.2.
Chunk length: 235

--- Document Chunk 997 ---
models, as illustrated on Figure 8.2.
An alternative to Post-Training Quantization is
Chunk length: 85

--- Document Chunk 998 ---
Quan Quan Quan Quan Quan Quan Quan Quan Quan Quan Quanti ti ti ti ti ti ti ti ti ti tiza za za za za za za za za za zation tion tion tion tion tion tion tion tion tion tion-Aware Aware Aware Aware Aware Aware Aware Aware Aware Aware AwareTrain Train
Chunk length: 249

--- Document Chunk 999 ---
Aware Aware Aware Aware Aware AwareTrain Train Train Train Train Train Train Train Train Train Training ing ing ing ing ing ing ing ing ing ingthat applies quan-
Chunk length: 161

--- Document Chunk 1000 ---
tization during the forward pass but keeps high-
precision encoding of parameters and gradients,
and propagates the gradients during the back-
ward pass as if there was no quantization [Ma
et al., 2024].
152
Chunk length: 207

--- Document Chunk 1001 ---
8.3 Adapters
As we saw in Â§ 3.6, fine-tuning is a key strat-
egy to reuse pre-trained models. Since it aims
at making only minor changes to an existing
model, techniques have been developed that add
components with few parameters, referred to
Chunk length: 242

--- Document Chunk 1002 ---
components with few parameters, referred to
as adapters adapters adapters adapters adapters adapters adapters adapters adapters adapters adapters, to the pre-trained architecture, and
freeze all the original parameters [Houlsby et al.,
2019].
Chunk length: 242

--- Document Chunk 1003 ---
2019].
The current dominant method is the Low Low Low Low Low Low Low Low Low Low Low-Rank Rank Rank Rank Rank Rank Rank Rank Rank Rank Rank
Chunk length: 140

--- Document Chunk 1004 ---
Adap Adap Adap Adap Adap Adap Adap Adap Adap Adap Adapta ta ta ta ta ta ta ta ta ta tation tion tion tion tion tion tion tion tion tion tion(LoRA LoRA LoRA LoRA LoRA LoRA LoRA LoRA LoRA LoRA LoRA), which adds low-rank cor-
Chunk length: 222

--- Document Chunk 1005 ---
rections to some of the modelâ€™s weight matrices
[Hu et al., 2021].
Formally, given a linear operation of the form
XW T, where X is a N Ã—D tensor of activations
for a batch of N samples, and W is a C Ã—D
weight matrix, the LoRA adapter replaces this
Chunk length: 247

--- Document Chunk 1006 ---
weight matrix, the LoRA adapter replaces this
operation with X(W +BA)T, where A and B
are two trainable matrices of size RÃ—D and
C Ã—R respectively, with R â‰ª min(C,D), and
the matrix W is removed from the trainable pa-
Chunk length: 217

--- Document Chunk 1007 ---
the matrix W is removed from the trainable pa-
rameters. The matrix A is initialized with ran-
dom Gaussian values, and B is set to zero, so
that the fine-tuning starts with a model that com-
putes an output identical to that of the original
one.
Chunk length: 246

--- Document Chunk 1008 ---
one.
153
Chunk length: 8

--- Document Chunk 1009 ---
The total number of parameters to optimize with
this approach is generally a few percent of the
number of parameters in the original model.
The standard procedure to fine-tune a trans trans trans trans trans trans trans trans trans trans trans-
Chunk length: 244

--- Document Chunk 1010 ---
former former former former former former former former former former formerwith such adapters is to change only the
weight matrices in the attention blocks, and to
keep the MLP MLP MLP MLP MLP MLP MLP MLP MLP MLP MLPof the feed-forward blocks un-
Chunk length: 247

--- Document Chunk 1011 ---
changed. The same strategy has been used suc-
cessfully to tune diffusion denoising models by
fine-tuning the attention blocks responsible for
the text-based conditioning.
Since fine-tuning with LoRA adapters drastically
Chunk length: 220

--- Document Chunk 1012 ---
Since fine-tuning with LoRA adapters drastically
reduces the number of trainable parameters, it
reduces the memory footprint required by op-
timizers such as Adam Adam Adam Adam Adam Adam Adam Adam Adam Adam Adam, which generally store
Chunk length: 235

--- Document Chunk 1013 ---
two running average per parameter to optimize.
Also, it reduces slightly the computation during
Chunk length: 95

--- Document Chunk 1014 ---
Also, it reduces slightly the computation during
the back back back back back back back back back back backward ward ward ward ward ward ward ward ward ward wardpass pass pass pass pass pass pass pass pass pass pass.
Chunk length: 216

--- Document Chunk 1015 ---
For commercial applications that require a large
number of fine-tuned models, the AB pairs can
be stored separately from the original model,
which has to be stored only once. And finally,
contrary to other type of adapters, the modifica-
Chunk length: 237

--- Document Chunk 1016 ---
contrary to other type of adapters, the modifica-
tions can be integrated into the original architec-
ture, simply by adding AB to W, resulting in an
architecture and parameter count for inference
154
Chunk length: 200

--- Document Chunk 1017 ---
strictly identical to that of the base model.
We saw that quantization degrade modelsâ€™ ac-
curacy only marginally. However, gradient de-
scent requires high precision in both the gra-
dient and the trained parameters, to allow the
Chunk length: 230

--- Document Chunk 1018 ---
dient and the trained parameters, to allow the
accumulation of small changes. The QLoRA ap-
proach combines a quantized base model and un-
Chunk length: 138

--- Document Chunk 1019 ---
quantized Low Low Low Low Low Low Low Low Low Low Low-Rank Rank Rank Rank Rank Rank Rank Rank Rank Rank RankAdap Adap Adap Adap Adap Adap Adap Adap Adap Adap Adapta ta ta ta ta ta ta ta ta ta tation tion tion tion tion tion tion tion tion tion
Chunk length: 243

--- Document Chunk 1020 ---
tion tion tion tion tion tion tion tion tion tionto reduce the
Chunk length: 62

--- Document Chunk 1021 ---
memory requirement even more [Dettmers et al.,
2023].
155
Chunk length: 57

--- Document Chunk 1022 ---
8.4 Model merging
An alternative to the fine-tuning and prompting
methods seen in the previous sections consists
of combining multiple models with diverse ca-
pabilities into a single one, without additional
training.
Chunk length: 217

--- Document Chunk 1023 ---
training.
Model Model Model Model Model Model Model Model Model Model Modelmerg merg merg merg merg merg merg merg merg merg merging ing ing ing ing ing ing ing ing ing ingrelies on the compatibility be-
tween multiple fine-tuned versions of a base
Chunk length: 248

--- Document Chunk 1024 ---
tween multiple fine-tuned versions of a base
model.
Ilharco et al. [2022] showed that models obtained
by fine-tuning a CLIP CLIP CLIP CLIP CLIP CLIP CLIP CLIP CLIP CLIP CLIPbase model on several im-
age classification data-sets can be combined in
Chunk length: 246

--- Document Chunk 1025 ---
age classification data-sets can be combined in
the parameter space, where they exhibit Task Task Task Task Task Task Task Task Task Task Task
Chunk length: 142

--- Document Chunk 1026 ---
Arith Arith Arith Arith Arith Arith Arith Arith Arith Arith Arithmetic metic metic metic metic metic metic metic metic metic metic properties.
Formally, let Î¸ be the parameter vector of a pre-
trained model, and for t = 1,...,T , let Î¸t and
Chunk length: 240

--- Document Chunk 1027 ---
trained model, and for t = 1,...,T , let Î¸t and
Ï„t = Î¸t âˆ’Î¸ be respectively the parameters af-
ter fine-tuning on task t and the corresponding
residual. Experiments show that the model with
parameters Î¸+Ï„1 +Â·Â·Â· +Ï„T exhibits multi-task
Chunk length: 233

--- Document Chunk 1028 ---
parameters Î¸+Ï„1 +Â·Â·Â· +Ï„T exhibits multi-task
capabilities. Similarly, subtracting a Ï„t degrades
the performance on the corresponding task.
Methods have been developed to reduce the in-
terference between the different residuals and
Chunk length: 231

--- Document Chunk 1029 ---
terference between the different residuals and
improve the performance when the number of
156
Chunk length: 93

--- Document Chunk 1030 ---
tasks increases [Yadav et al., 2023; Yu et al., 2023].
An alternative to merging models in parame-
ter space is to recombine their layers. Akiba
et al. [2024] combine merging the parameters
and re-combining layers, and rely on a stochas-
Chunk length: 237

--- Document Chunk 1031 ---
and re-combining layers, and rely on a stochas-
tic optimization to deal with the combinatorial
explosion. Experiments with three fine-tuned
versions of Mistral-7B [Jiang et al., 2023] show
that combining these two merging strategies out-
Chunk length: 238

--- Document Chunk 1032 ---
that combining these two merging strategies out-
performs both of them.
157
Chunk length: 75

--- Document Chunk 1033 ---
The Missing Bits
For the sake of concision, this volume skips many
important topics, in particular:
Recurrent Neural Networks
Before attention models showed greater perfor-
Chunk length: 172

--- Document Chunk 1034 ---
mance, Re Re Re Re Re Re Re Re Re Re Recur cur cur cur cur cur cur cur cur cur current rent rent rent rent rent rent rent rent rent rentNeu Neu Neu Neu Neu Neu Neu Neu Neu Neu Neural ral ral ral ral ral ral ral ral ral ralNet Net Net Net Net Net Net
Chunk length: 249

--- Document Chunk 1035 ---
ral ral ral ral ralNet Net Net Net Net Net Net Net Net Net Networks works works works works works works works works works works(RNN RNN RNN RNN RNN RNN RNN RNN RNN RNN RNN) were
Chunk length: 177

--- Document Chunk 1036 ---
the standard approach for dealing with temporal
sequences such as text or sound samples. These
Chunk length: 94

--- Document Chunk 1037 ---
sequences such as text or sound samples. These
architectures possess an internal hid hid hid hid hid hid hid hid hid hid hidden den den den den den den den den den denstate state state state state state state state state state state
Chunk length: 232

--- Document Chunk 1038 ---
that gets updated each time a component of the
sequence is processed. Their main components
are layers such as LSTM [Hochreiter and Schmid-
huber, 1997] or GRU [Cho et al., 2014].
Training a recurrent architecture amounts to
Chunk length: 224

--- Document Chunk 1039 ---
Training a recurrent architecture amounts to
unfolding it in time, which results in a long
composition of operators. This has historically
prompted the design of key techniques now used
Chunk length: 185

--- Document Chunk 1040 ---
prompted the design of key techniques now used
for deep architectures such as rec rec rec rec rec rec rec rec rec rec recti ti ti ti ti ti ti ti ti ti tifiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiersand gat-
Chunk length: 226

--- Document Chunk 1041 ---
ing, a form of skip skip skip skip skip skip skip skip skip skip skipcon con con con con con con con con con connec nec nec nec nec nec nec nec nec nec nections tions tions tions tions tions tions tions tions tions tionswhich are mod-
158
Chunk length: 238

--- Document Chunk 1042 ---
ulated dynamically.
One of the key drawbacks of traditional recur-
rent architectures is that the structure of the
computation xt+1 = f(xt) imposes to process
the input sequence serially, which takes a time
Chunk length: 206

--- Document Chunk 1043 ---
the input sequence serially, which takes a time
proportional to T. In contrast, transformers, for
instance, can take advantage of parallel compu-
tation, resulting in a constant time if enough
computing units are available.
Chunk length: 223

--- Document Chunk 1044 ---
computing units are available.
This is addressed by architectures such as QRNN
[Bradbury et al., 2016], S4 [Gu et al., 2021], or
Mamba [Gu and Dao, 2023], whose recurrent op-
erations are affine so that theft themselves, and
Chunk length: 224

--- Document Chunk 1045 ---
erations are affine so that theft themselves, and
consequently the xt = ft(x0), can be computed
in parallel, resulting in a constant time if f does
not depend on t and logT otherwise, again if
enough parallel computing units are available.
Chunk length: 239

--- Document Chunk 1046 ---
enough parallel computing units are available.
Autoencoder
An au au au au au au au au au au autoen toen toen toen toen toen toen toen toen toen toencoder coder coder coder coder coder coder coder coder coder coderis a model that maps an input
Chunk length: 242

--- Document Chunk 1047 ---
signal, possibly of high dimension, to a low-
dimension latent representation, and then maps
it back to the original signal, ensuring that infor-
mation has been preserved. We saw it in Â§ 6.1
for denoising, but it can also be used to auto-
Chunk length: 239

--- Document Chunk 1048 ---
for denoising, but it can also be used to auto-
matically discover a meaningful low-dimension
159
Chunk length: 97

--- Document Chunk 1049 ---
parameterization of the data manifold.
Chunk length: 38

--- Document Chunk 1050 ---
The Vari Vari Vari Vari Vari Vari Vari Vari Vari Vari Varia a a a a a a a a a ational tional tional tional tional tional tional tional tional tional tionalAu Au Au Au Au Au Au Au Au Au Autoen toen toen toen toen toen toen toen toen toen toencoder
Chunk length: 246

--- Document Chunk 1051 ---
toen toen toen toen toen toen toen toen toencoder coder coder coder coder coder coder coder coder coder coder(VAE VAE VAE VAE VAE VAE VAE VAE VAE VAE VAE) proposed by
Chunk length: 166

--- Document Chunk 1052 ---
Kingma and Welling [2013] is a generative model
with a similar structure. It imposes, through
the loss, a pre-defined distribution on the latent
representation. This allows, after training, the
generation of new samples by sampling the la-
Chunk length: 239

--- Document Chunk 1053 ---
generation of new samples by sampling the la-
tent representation according to this imposed
distribution and then mapping back through the
decoder.
Generative Adversarial Networks
Another approach to density modeling is the
Chunk length: 223

--- Document Chunk 1054 ---
Gen Gen Gen Gen Gen Gen Gen Gen Gen Gen Gener er er er er er er er er er era a a a a a a a a a ative tive tive tive tive tive tive tive tive tive tiveAd Ad Ad Ad Ad Ad Ad Ad Ad Ad Adver ver ver ver ver ver ver ver ver ver versar sar sar sar sar sar
Chunk length: 248

--- Document Chunk 1055 ---
ver ver ver ver ver versar sar sar sar sar sar sar sar sar sar sarial ial ial ial ial ial ial ial ial ial ialNet Net Net Net Net Net Net Net Net Net Networks works works works works works works works works works works(GAN GAN GAN GAN GAN GAN GAN GAN
Chunk length: 249

--- Document Chunk 1056 ---
works works works(GAN GAN GAN GAN GAN GAN GAN GAN GAN GAN GAN) intro-
Chunk length: 69

--- Document Chunk 1057 ---
duced by Goodfellow et al. [2014]. This method
combines a gen gen gen gen gen gen gen gen gen gen gener er er er er er er er er er era a a a a a a a a a ator tor tor tor tor tor tor tor tor tor tor, which takes a random in-
Chunk length: 223

--- Document Chunk 1058 ---
put following a fixed distribution as input and
produces a structured signal such as an image,
Chunk length: 94

--- Document Chunk 1059 ---
and a dis dis dis dis dis dis dis dis dis dis discrim crim crim crim crim crim crim crim crim crim crimi i i i i i i i i i ina na na na na na na na na na nator tor tor tor tor tor tor tor tor tor tor, which takes a sample as
Chunk length: 224

--- Document Chunk 1060 ---
input and predicts whether it comes from the
training set or if it was generated by the genera-
tor.
Training optimizes the discriminator to mini-
mize a standard cross-entropy loss, and the gen-
erator to maximize the discriminatorâ€™s loss. It
Chunk length: 243

--- Document Chunk 1061 ---
erator to maximize the discriminatorâ€™s loss. It
can be shown that, at equilibrium, the gener-
160
Chunk length: 97

--- Document Chunk 1062 ---
ator produces samples indistinguishable from
real data. In practice, when the gradient flows
through the discriminator to the generator, it
informs the latter about the cues that the dis-
criminator uses that need to be addressed.
Chunk length: 230

--- Document Chunk 1063 ---
criminator uses that need to be addressed.
Graph Neural Networks
Many applications require processing signals
which are not organized regularly on a grid. For
instance, proteins, 3D meshes, geographic loca-
Chunk length: 206

--- Document Chunk 1064 ---
instance, proteins, 3D meshes, geographic loca-
tions, or social interactions are more naturally
structured as graphs. Standard convolutional
networks or even attention models are poorly
adapted to process such data, and the tool of
Chunk length: 232

--- Document Chunk 1065 ---
choice for such a task isGraph Graph Graph Graph Graph Graph Graph Graph Graph Graph GraphNeu Neu Neu Neu Neu Neu Neu Neu Neu Neu Neural ral ral ral ral ral ral ral ral ral ralNet Net Net Net Net Net Net Net Net Net Networks works works works works
Chunk length: 248

--- Document Chunk 1066 ---
Net Net Net Net Networks works works works works works works works works works works
Chunk length: 84

--- Document Chunk 1067 ---
(GNN GNN GNN GNN GNN GNN GNN GNN GNN GNN GNN) [Scarselli et al., 2009].
These models are composed of layers that com-
pute activations at each vertex by combining
linearly the activations located at its immediate
Chunk length: 212

--- Document Chunk 1068 ---
linearly the activations located at its immediate
neighboring vertices. This operation is very sim-
ilar to a standard convolution, except that the
data structure does not reflect any geometrical
information associated with the feature vectors
Chunk length: 243

--- Document Chunk 1069 ---
information associated with the feature vectors
they carry.
161
Chunk length: 63

--- Document Chunk 1070 ---
Self-supervised training
As stated in Â§ 7.1, even though they are trained
Chunk length: 73

--- Document Chunk 1071 ---
only to predict the next word, Large Large Large Large Large Large Large Large Large Large LargeLan Lan Lan Lan Lan Lan Lan Lan Lan Lan Language guage guage guage guage guage guage guage guage guage guage
Chunk length: 204

--- Document Chunk 1072 ---
Mod Mod Mod Mod Mod Mod Mod Mod Mod Mod Models els els els els els els els els els elstrained on large unlabeled datasets such
as GPT GPT GPT GPT GPT GPT GPT GPT GPT GPT GPT(see Â§ 5.3) are able to solve various tasks,
Chunk length: 217

--- Document Chunk 1073 ---
such as identifying the grammatical role of a
word, answering questions, or even translating
from one language to another [Radford et al.,
2019].
Such models constitute one category of a larger
Chunk length: 193

--- Document Chunk 1074 ---
Such models constitute one category of a larger
class of methods that fall under the name of self self self self self self self self self self self-
Chunk length: 148

--- Document Chunk 1075 ---
su su su su su su su su su su super per per per per per per per per per pervised vised vised vised vised vised vised vised vised vised visedlearn learn learn learn learn learn learn learn learn learn learning ing ing ing ing ing ing ing ing ing ing,
Chunk length: 249

--- Document Chunk 1076 ---
learning ing ing ing ing ing ing ing ing ing ing, and try to take advantage
Chunk length: 75

--- Document Chunk 1077 ---
of unlabeled datasets [Balestriero et al., 2023].
The key principle of these methods is to define a
task that does not require labels but necessitates
feature representations which are useful for the
real task of interest, for which a small labeled
Chunk length: 248

--- Document Chunk 1078 ---
real task of interest, for which a small labeled
dataset exists. In computer vision, for instance,
image features can be optimized so that they
Chunk length: 143

--- Document Chunk 1079 ---
image features can be optimized so that they
are in in in in in in in in in in invari vari vari vari vari vari vari vari vari vari variant ant ant ant ant ant ant ant ant ant antto data transformations that do not
Chunk length: 213

--- Document Chunk 1080 ---
change the semantic content of the image, while
being statistically uncorrelated [Zbontar et al.,
2021].
In both NLP and computer vision, a powerful
generic strategy is to train a model to recover
parts of the signal that have been masked [Devlin
Chunk length: 246

--- Document Chunk 1081 ---
parts of the signal that have been masked [Devlin
162
Chunk length: 53

--- Document Chunk 1082 ---
et al., 2018; Zhou et al., 2021].
163
Chunk length: 37

--- Document Chunk 1083 ---
Bibliography
T. Akiba, M. Shing, Y. Tang, et al. Evolution-
ary Optimization of Model Merging Recipes.
CoRR, abs/2403.13187, 2024. [pdf]. 157
J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer
Normalization. CoRR, abs/1607.06450, 2016.
[pdf]. 83
Chunk length: 241

--- Document Chunk 1084 ---
[pdf]. 83
R. Balestriero, M. Ibrahim, V. Sobal, et al. A
Cookbook of Self-Supervised Learning. CoRR,
abs/2304.12210, 2023. [pdf]. 162
A. Baydin, B. Pearlmutter, A. Radul, and
J. Siskind. Automatic differentiation in
machine learning: a survey. CoRR,
Chunk length: 249

--- Document Chunk 1085 ---
machine learning: a survey. CoRR,
abs/1502.05767, 2015. [pdf]. 42
M. Belkin, D. Hsu, S. Ma, and S. Mandal. Rec-
onciling modern machine learning and the
bias-variance trade-off. CoRR, abs/1812.11118,
2018. [pdf]. 50
164
Chunk length: 219

--- Document Chunk 1086 ---
I. Beltagy, M. Peters, and A. Cohan. Long-
former: The Long-Document Transformer.
CoRR, abs/2004.05150, 2020. [pdf]. 91
R. Bommasani, D. Hudson, E. Adeli, et al. On
the Opportunities and Risks of Foundation
Models. CoRR, abs/2108.07258, 2021. [pdf].
Chunk length: 249

--- Document Chunk 1087 ---
Models. CoRR, abs/2108.07258, 2021. [pdf].
140
J. Bradbury, S. Merity, C. Xiong, and R. Socher.
Quasi-Recurrent Neural Networks. CoRR,
abs/1611.01576, 2016. [pdf]. 159
T. Brown, B. Mann, N. Ryder, et al. Lan-
Chunk length: 208

--- Document Chunk 1088 ---
T. Brown, B. Mann, N. Ryder, et al. Lan-
guage Models are Few-Shot Learners. CoRR,
abs/2005.14165, 2020. [pdf]. 54, 113, 139
S. Bubeck, V. Chandrasekaran, R. Eldan, et al.
Sparks of Artificial General Intelligence:
Chunk length: 214

--- Document Chunk 1089 ---
Sparks of Artificial General Intelligence:
Early experiments with GPT-4. CoRR,
abs/2303.12712, 2023. [pdf]. 140
T. Chen, B. Xu, C. Zhang, and C. Guestrin. Train-
ing Deep Nets with Sublinear Memory Cost.
CoRR, abs/1604.06174, 2016. [pdf]. 43
Chunk length: 241

--- Document Chunk 1090 ---
CoRR, abs/1604.06174, 2016. [pdf]. 43
K. Cho, B. van Merrienboer, Ã‡. GÃ¼lÃ§ehre,
et al. Learning Phrase Representations using
RNN Encoder-Decoder for Statistical Machine
Translation. CoRR, abs/1406.1078, 2014. [pdf].
158
165
Chunk length: 222

--- Document Chunk 1091 ---
A. Chowdhery, S. Narang, J. Devlin, et al. PaLM:
Scaling Language Modeling with Pathways.
CoRR, abs/2204.02311, 2022. [pdf]. 9, 54, 140
G. Cybenko. Approximation by superpositions
of a sigmoidal function. Mathematics of Con-
Chunk length: 224

--- Document Chunk 1092 ---
of a sigmoidal function. Mathematics of Con-
trol, Signals, and Systems , 2(4):303â€“314, De-
cember 1989. [pdf]. 99
J. Deng, W. Dong, R. Socher, et al. ImageNet:
A Large-Scale Hierarchical Image Database.
In Conference on Computer Vision and Pattern
Chunk length: 248

--- Document Chunk 1093 ---
In Conference on Computer Vision and Pattern
Recognition (CVPR) , 2009. [pdf]. 51
T. Dettmers, A. Pagnoni, A. Holtzman, and
L. Zettlemoyer. QLoRA: Efficient Finetuning
of Quantized LLMs. CoRR, abs/2305.14314,
2023. [pdf]. 155
Chunk length: 225

--- Document Chunk 1094 ---
2023. [pdf]. 155
J. Devlin, M. Chang, K. Lee, and K. Toutanova.
BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding.
CoRR, abs/1810.04805, 2018. [pdf]. 54, 115,
162
A. Dosovitskiy, L. Beyer, A. Kolesnikov, et al.
Chunk length: 241

--- Document Chunk 1095 ---
A. Dosovitskiy, L. Beyer, A. Kolesnikov, et al.
An Image is Worth 16x16 Words: Transform-
ers for Image Recognition at Scale. CoRR,
abs/2010.11929, 2020. [pdf]. 113, 114
166
Chunk length: 173

--- Document Chunk 1096 ---
K. Fukushima. Neocognitron: A self-organizing
neural network model for a mechanism of
pattern recognition unaffected by shift in po-
sition. Biological Cybernetics , 36(4):193â€“202,
April 1980. [pdf]. 2
Y. Gal and Z. Ghahramani. Dropout as
Chunk length: 238

--- Document Chunk 1097 ---
Y. Gal and Z. Ghahramani. Dropout as
a Bayesian Approximation: Representing
Model Uncertainty in Deep Learning. CoRR,
abs/1506.02142, 2015. [pdf]. 78
X. Glorot and Y. Bengio. Understanding the dif-
ficulty of training deep feedforward neural
Chunk length: 241

--- Document Chunk 1098 ---
ficulty of training deep feedforward neural
networks. In International Conference on Arti-
ficial Intelligence and Statistics (AISTATS) , 2010.
[pdf]. 44, 62
X. Glorot, A. Bordes, and Y. Bengio. Deep Sparse
Chunk length: 206

--- Document Chunk 1099 ---
X. Glorot, A. Bordes, and Y. Bengio. Deep Sparse
Rectifier Neural Networks. In International
Conference on Artificial Intelligence and Statis-
tics (AISTATS), 2011. [pdf]. 71
A. Gomez, M. Ren, R. Urtasun, and R. Grosse.
Chunk length: 219

--- Document Chunk 1100 ---
A. Gomez, M. Ren, R. Urtasun, and R. Grosse.
The Reversible Residual Network: Backprop-
agation Without Storing Activations. CoRR,
abs/1707.04585, 2017. [pdf]. 43
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza,
Chunk length: 208

--- Document Chunk 1101 ---
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza,
et al. Generative Adversarial Networks. CoRR,
abs/1406.2661, 2014. [pdf]. 160
167
Chunk length: 127

--- Document Chunk 1102 ---
A. Gu and T. Dao. Mamba: Linear-Time Se-
quence Modeling with Selective State Spaces.
CoRR, abs/2312.00752, 2023. [pdf]. 159
A. Gu, K. Goel, and C. RÃ©. Efficiently Modeling
Long Sequences with Structured State Spaces.
Chunk length: 217

--- Document Chunk 1103 ---
Long Sequences with Structured State Spaces.
CoRR, abs/2111.00396, 2021. [pdf]. 159
K. He, X. Zhang, S. Ren, and J. Sun. Deep Resid-
ual Learning for Image Recognition. CoRR,
abs/1512.03385, 2015. [pdf]. 52, 84, 85, 103,
105
Chunk length: 224

--- Document Chunk 1104 ---
abs/1512.03385, 2015. [pdf]. 52, 84, 85, 103,
105
D. Hendrycks and K. Gimpel. Gaussian Error
Linear Units (GELUs). CoRR, abs/1606.08415,
2016. [pdf]. 73
D. Hendrycks, K. Zhao, S. Basart, et al. Natural
Adversarial Examples. CoRR, abs/1907.07174,
Chunk length: 245

--- Document Chunk 1105 ---
Adversarial Examples. CoRR, abs/1907.07174,
2019. [pdf]. 132
J. Ho, A. Jain, and P. Abbeel. Denoising Diffusion
Probabilistic Models. CoRR, abs/2006.11239,
2020. [pdf]. 142, 143, 144
S. Hochreiter and J. Schmidhuber. Long Short-
Chunk length: 228

--- Document Chunk 1106 ---
S. Hochreiter and J. Schmidhuber. Long Short-
Term Memory.Neural Computation, 9(8):1735â€“
1780, 1997. [pdf]. 158
N. Houlsby, A. Giurgiu, S. Jastrzebski, et al.
Parameter-Efficient Transfer Learning for
NLP. CoRR, abs/1902.00751, 2019. [pdf]. 153
168
Chunk length: 248

--- Document Chunk 1107 ---
E. Hu, Y. Shen, P. Wallis, et al. LoRA: Low-Rank
Adaptation of Large Language Models. CoRR,
abs/2106.09685, 2021. [pdf]. 153
G. Ilharco, M. Ribeiro, M. Wortsman, et al. Edit-
ing Models with Task Arithmetic. CoRR,
abs/2212.04089, 2022. [pdf]. 156
Chunk length: 246

--- Document Chunk 1108 ---
abs/2212.04089, 2022. [pdf]. 156
S. Ioffe and C. Szegedy. Batch Normalization: Ac-
celerating Deep Network Training by Reduc-
ing Internal Covariate Shift. In International
Conference on Machine Learning (ICML) , 2015.
[pdf]. 80
Chunk length: 228

--- Document Chunk 1109 ---
[pdf]. 80
A. Jiang, A. Sablayrolles, A. Mensch, et al. Mistral
7B. CoRR, abs/2310.06825, 2023. [pdf]. 157
J. Kaplan, S. McCandlish, T. Henighan, et al. Scal-
ing Laws for Neural Language Models. CoRR,
abs/2001.08361, 2020. [pdf]. 52, 53
Chunk length: 236

--- Document Chunk 1110 ---
abs/2001.08361, 2020. [pdf]. 52, 53
A. Katharopoulos, A. Vyas, N. Pappas, and
F. Fleuret. Transformers are RNNs: Fast Au-
toregressive Transformers with Linear Atten-
tion. In Proceedings of the International Confer-
Chunk length: 216

--- Document Chunk 1111 ---
tion. In Proceedings of the International Confer-
ence on Machine Learning (ICML) , pages 5294â€“
5303, 2020. [pdf]. 91
D. Kingma and J. Ba. Adam: A Method for
Stochastic Optimization. CoRR, abs/1412.6980,
2014. [pdf]. 39
169
Chunk length: 223

--- Document Chunk 1112 ---
D. P. Kingma and M. Welling. Auto-Encoding
Variational Bayes. CoRR, abs/1312.6114, 2013.
[pdf]. 160
T. Kojima, S. Gu, M. Reid, et al. Large Lan-
guage Models are Zero-Shot Reasoners. CoRR,
abs/2205.11916, 2022. [pdf]. 149
Chunk length: 221

--- Document Chunk 1113 ---
abs/2205.11916, 2022. [pdf]. 149
A. Krizhevsky, I. Sutskever, and G. Hinton. Ima-
geNet Classification with Deep Convolutional
Neural Networks. In Neural Information Pro-
cessing Systems (NIPS) , 2012. [pdf]. 8, 101
Chunk length: 215

--- Document Chunk 1114 ---
cessing Systems (NIPS) , 2012. [pdf]. 8, 101
Y. LeCun, B. Boser, J. S. Denker, et al. Back-
propagation applied to handwritten zip code
recognition. Neural Computation , 1(4):541â€“
551, 1989. [pdf]. 8
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Chunk length: 247

--- Document Chunk 1115 ---
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):
2278â€“2324, 1998. [pdf]. 101, 102
P. Lewis, E. Perez, A. Piktus, et al. Retrieval-
Chunk length: 220

--- Document Chunk 1116 ---
P. Lewis, E. Perez, A. Piktus, et al. Retrieval-
Augmented Generation for Knowledge-
Intensive NLP Tasks. CoRR, abs/2005.11401,
2020. [pdf]. 149
W. Liu, D. Anguelov, D. Erhan, et al. SSD: Single
Shot MultiBox Detector.CoRR, abs/1512.02325,
Chunk length: 239

--- Document Chunk 1117 ---
Shot MultiBox Detector.CoRR, abs/1512.02325,
2015. [pdf]. 121, 123
170
Chunk length: 70

--- Document Chunk 1118 ---
Llama.cpp. Llama.cpp git repository, June 2023.
[web]. 150, 151
J. Long, E. Shelhamer, and T. Darrell. Fully Con-
volutional Networks for Semantic Segmenta-
tion. CoRR, abs/1411.4038, 2014. [pdf]. 84, 85,
127
Chunk length: 208

--- Document Chunk 1119 ---
127
S. Ma, H. Wang, L. Ma, et al. The Era of 1-bit
LLMs: All Large Language Models are in 1.58
Bits. CoRR, abs/2402.17764, 2024. [pdf]. 152
A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rec-
tifier nonlinearities improve neural network
Chunk length: 229

--- Document Chunk 1120 ---
tifier nonlinearities improve neural network
acoustic models. In proceedings of the ICML
Workshop on Deep Learning for Audio, Speech
and Language Processing , 2013. [pdf]. 72
V. Mnih, K. Kavukcuoglu, D. Silver, et al. Human-
Chunk length: 224

--- Document Chunk 1121 ---
V. Mnih, K. Kavukcuoglu, D. Silver, et al. Human-
level control through deep reinforcement
learning. Nature, 518(7540):529â€“533, February
2015. [pdf]. 135, 136
A. Nichol, P. Dhariwal, A. Ramesh, et al. GLIDE:
Chunk length: 207

--- Document Chunk 1122 ---
A. Nichol, P. Dhariwal, A. Ramesh, et al. GLIDE:
Towards Photorealistic Image Generation and
Editing with Text-Guided Diffusion Models.
CoRR, abs/2112.10741, 2021. [pdf]. 145
L. Ouyang, J. Wu, X. Jiang, et al. Training lan-
Chunk length: 223

--- Document Chunk 1123 ---
L. Ouyang, J. Wu, X. Jiang, et al. Training lan-
guage models to follow instructions with hu-
171
Chunk length: 97

--- Document Chunk 1124 ---
man feedback. CoRR, abs/2203.02155, 2022.
[pdf]. 141
R. Pascanu, T. Mikolov, and Y. Bengio. On the dif-
ficulty of training recurrent neural networks.
In International Conference on Machine Learn-
ing (ICML) , 2013. [pdf]. 44
Chunk length: 225

--- Document Chunk 1125 ---
ing (ICML) , 2013. [pdf]. 44
A. Radford, J. Kim, C. Hallacy, et al. Learn-
ing Transferable Visual Models From Natural
Language Supervision. CoRR, abs/2103.00020,
2021. [pdf]. 131, 133
A. Radford, J. Kim, T. Xu, et al. Robust Speech
Chunk length: 232

--- Document Chunk 1126 ---
A. Radford, J. Kim, T. Xu, et al. Robust Speech
Recognition via Large-Scale Weak Supervi-
sion. CoRR, abs/2212.04356, 2022. [pdf]. 129
A. Radford, K. Narasimhan, T. Salimans, and
I. Sutskever. Improving Language Understand-
Chunk length: 223

--- Document Chunk 1127 ---
I. Sutskever. Improving Language Understand-
ing by Generative Pre-Training, 2018. [pdf].
109, 112, 139
A. Radford, J. Wu, R. Child, et al. Language
Models are Unsupervised Multitask Learners,
2019. [pdf]. 112, 162
Chunk length: 214

--- Document Chunk 1128 ---
2019. [pdf]. 112, 162
O. Ronneberger, P. Fischer, and T. Brox. U-Net:
Convolutional Networks for Biomedical Im-
age Segmentation. In Medical Image Comput-
ing and Computer-Assisted Intervention , 2015.
[pdf]. 84, 85, 127
172
Chunk length: 224

--- Document Chunk 1129 ---
P. Sahoo, A. Singh, S. Saha, et al. A Systematic
Survey of Prompt Engineering in Large Lan-
guage Models: Techniques and Applications.
CoRR, abs/2402.07927, 2024. [pdf]. 147
F. Scarselli, M. Gori, A. C. Tsoi, et al. The Graph
Chunk length: 225

--- Document Chunk 1130 ---
Neural Network Model. IEEE Transactions
on Neural Networks (TNN) , 20(1):61â€“80, 2009.
[pdf]. 161
R. Sennrich, B. Haddow, and A. Birch. Neural
Machine Translation of Rare Words with Sub-
word Units. CoRR, abs/1508.07909, 2015. [pdf].
34
Chunk length: 235

--- Document Chunk 1131 ---
word Units. CoRR, abs/1508.07909, 2015. [pdf].
34
J. Sevilla, L. Heim, A. Ho, et al. Compute Trends
Across Three Eras of Machine Learning.CoRR,
abs/2202.05924, 2022. [pdf]. 8, 52, 54
J. Sevilla, P. Villalobos, J. F. CerÃ³n, et al. Param-
Chunk length: 236

--- Document Chunk 1132 ---
eter, Compute and Data Trends in Machine
Learning, May 2023. [web]. 55
K. Simonyan and A. Zisserman. Very Deep Con-
volutional Networks for Large-Scale Image
Recognition. CoRR, abs/1409.1556, 2014. [pdf].
101
Chunk length: 208

--- Document Chunk 1133 ---
101
N. Srivastava, G. Hinton, A. Krizhevsky, et al.
Dropout: A Simple Way to Prevent Neural
173
Chunk length: 95

--- Document Chunk 1134 ---
Networks from Overfitting. Journal of Ma-
chine Learning Research (JMLR) , 15:1929â€“1958,
2014. [pdf]. 77
M. Telgarsky. Benefits of depth in neural net-
works. CoRR, abs/1602.04485, 2016. [pdf]. 47
H. Touvron, T. Lavril, G. Izacard, et al. LLaMA:
Chunk length: 245

--- Document Chunk 1135 ---
H. Touvron, T. Lavril, G. Izacard, et al. LLaMA:
Open and Efficient Foundation Language Mod-
els. CoRR, abs/2302.13971, 2023. [pdf]. 151
A. Vaswani, N. Shazeer, N. Parmar, et al. Atten-
tion Is All You Need. CoRR, abs/1706.03762,
Chunk length: 229

--- Document Chunk 1136 ---
tion Is All You Need. CoRR, abs/1706.03762,
2017. [pdf]. 84, 87, 97, 108, 109, 110
J. Wei, X. Wang, D. Schuurmans, et al. Chain of
Thought Prompting Elicits Reasoning in Large
Language Models. CoRR, abs/2201.11903, 2022.
[pdf]. 149
Chunk length: 231

--- Document Chunk 1137 ---
[pdf]. 149
B. Xu, A. Yang, J. Lin, et al. ExpertPrompting: In-
structing Large Language Models to be Distin-
guished Experts. CoRR, abs/2305.14688, 2023.
[pdf]. 147
P. Yadav, D. Tam, L. Choshen, et al. TIES-
Chunk length: 207

--- Document Chunk 1138 ---
P. Yadav, D. Tam, L. Choshen, et al. TIES-
Merging: Resolving Interference When Merg-
ing Models. CoRR, abs/2306.01708, 2023. [pdf].
157
L. Yu, B. Yu, H. Yu, et al. Language Models
are Super Mario: Absorbing Abilities from
174
Chunk length: 226

--- Document Chunk 1139 ---
Homologous Models as a Free Lunch. CoRR,
abs/2311.03099, 2023. [pdf]. 157
J. Zbontar, L. Jing, I. Misra, et al. Barlow Twins:
Self-Supervised Learning via Redundancy Re-
duction. CoRR, abs/2103.03230, 2021. [pdf].
162
Chunk length: 217

--- Document Chunk 1140 ---
duction. CoRR, abs/2103.03230, 2021. [pdf].
162
M. D. Zeiler and R. Fergus. Visualizing and Un-
derstanding Convolutional Networks. In Eu-
ropean Conference on Computer Vision (ECCV) ,
2014. [pdf]. 69
H. Zhao, J. Shi, X. Qi, et al. Pyramid Scene
Chunk length: 245

--- Document Chunk 1141 ---
H. Zhao, J. Shi, X. Qi, et al. Pyramid Scene
Parsing Network. CoRR, abs/1612.01105, 2016.
[pdf]. 127, 128
J. Zhou, C. Wei, H. Wang, et al. iBOT: Im-
age BERT Pre-Training with Online Tokenizer.
CoRR, abs/2111.07832, 2021. [pdf]. 163
175
Chunk length: 236

--- Document Chunk 1142 ---
Index
1D convolution, 66
2D convolution, 66
activation, 23, 41
function, 71, 99
map, 69
Adam, 39, 154
adapter, 153
affine operation, 61
artificial neural network, 8, 11
attention operator, 88
autoencoder, 159
denoising, 118
Autograd, 42
Chunk length: 236

--- Document Chunk 1143 ---
autoencoder, 159
denoising, 118
Autograd, 42
autoregressive model, see model, autoregressive
average pooling, 76
backpropagation, 42
backward pass, 42, 154
basis function regression, 14
batch, 21, 38
batch normalization, 80, 104
176
Chunk length: 232

--- Document Chunk 1144 ---
Bellman equation, 135
bias vector, 61, 67
BPE, see Byte Pair Encoding
Byte Pair Encoding, 34, 129
cache memory, 21
capacity, 16
causal, 32, 90, 111
model, see model, causal
chain rule (derivative), 40
chain rule (probability), 30
Chunk length: 229

--- Document Chunk 1145 ---
chain rule (probability), 30
chain-of-thought, 140, 149
channel, 23
checkpointing, 43
classification, 18, 26, 101, 120
CLIP, see Contrastive Language-Image
Pre-training
CLS token, 115
computational cost, 43, 91
context size, 147
Chunk length: 228

--- Document Chunk 1146 ---
computational cost, 43, 91
context size, 147
Contrastive Language-Image Pre-training, 131,
156
contrastive loss, 27, 131
convnet, see convolutional network
convolution, 66
convolutional layer, see layer, convolutional
convolutional network, 101
Chunk length: 244

--- Document Chunk 1147 ---
convolutional network, 101
cross-attention block, 94, 109, 111
cross-entropy, 27, 31, 45
177
Chunk length: 92

--- Document Chunk 1148 ---
data augmentation, 120
deep learning, 8, 11
Deep Q-Network, 135
denoising autoencoder, see autoencoder,
denoising
density modeling, 18
depth, 41
diffusion model, 142
dilation, 67, 74
discriminator, 160
downscaling residual block, 106
Chunk length: 233

--- Document Chunk 1149 ---
downscaling residual block, 106
downstream task, 50
DQN, see Deep Q-Network
dropout, 77, 91
embedding layer, see layer, embedding
epoch, 48
equivariance, 67, 94
feed-forward block, 108, 109
few-shot prediction, 139
filter, 66
fine-tune, 124
Chunk length: 240

--- Document Chunk 1150 ---
filter, 66
fine-tune, 124
fine-tuning, 51, 141
flops, 22
forward pass, 41
foundation model, 140
FP32, 22
framework, 23
178
Chunk length: 122

--- Document Chunk 1151 ---
GAN, see Generative Adversarial Networks
GELU, 73
Generative Adversarial Networks, 160
Generative Pre-trained Transformer, 112, 131,
139, 162
generator, 160
GNN, see Graph Neural Network
GPT, see Generative Pre-trained Transformer
Chunk length: 230

--- Document Chunk 1152 ---
GPT, see Generative Pre-trained Transformer
GPU, see Graphical Processing Unit
gradient descent, 35, 37, 40, 45
gradient norm clipping, 44
gradient step, 35
Graph Neural Network, 161
Graphical Processing Unit, 8, 20
ground truth, 18
Chunk length: 232

--- Document Chunk 1153 ---
Graphical Processing Unit, 8, 20
ground truth, 18
hidden layer, see layer, hidden
hidden state, 158
hyper parameter, see parameter, hyper
hyperbolic tangent, 72
image processing, 101
image synthesis, 87, 142
inductive bias, 17, 49, 66, 67, 96
Chunk length: 242

--- Document Chunk 1154 ---
inductive bias, 17, 49, 66, 67, 96
invariance, 76, 94, 96, 162
kernel size, 66, 74
key, 88
Large Language Model, 51, 56, 88, 139, 146, 162
179
Chunk length: 142

--- Document Chunk 1155 ---
layer, 41, 59
attention, 87
convolutional, 66, 74, 87, 96, 101, 104, 121,
126, 129
embedding, 95, 111
fully connected, 61, 87, 96, 99, 101
hidden, 99
linear, 61
Multi-Head Attention, 93, 96, 111
normalizing, 80
reversible, 43
Chunk length: 225

--- Document Chunk 1156 ---
normalizing, 80
reversible, 43
layer normalization, 83, 108, 111
Leaky ReLU, 72
learning rate, 35, 50
learning rate schedule, 50
LeNet, 101, 102
linear layer, see layer, linear
LLM, see Large Language Model
local minimum, 35
logit, 26, 31
Chunk length: 238

--- Document Chunk 1157 ---
local minimum, 35
logit, 26, 31
LoRA, see Low-Rank Adaptation
loss, 12
Low-Rank Adaptation, 153, 155
machine learning, 11, 17, 18
Markovian Decision Process, 134
Markovian property, 134
max pooling, 74, 101
MDP, see Markovian, Decision Process
180
Chunk length: 247

--- Document Chunk 1158 ---
mean squared error, 14, 26
memory requirement, 43
memory speed, 21
metric learning, 27
MLP, see multi-layer perceptron, 154
model, 12
autoregressive, 30, 31, 139
causal, 33, 91, 111, 112
parametric, 12
pre-trained, 51, 124, 128
model merging, 156
Chunk length: 246

--- Document Chunk 1159 ---
pre-trained, 51, 124, 128
model merging, 156
multi-layer perceptron, 45, 99â€“101, 108
Natural Language Processing, 87
NLP, see Natural Language Processing
non-linearity, 71
normalizing layer, see layer, normalizing
object detection, 121
Chunk length: 235

--- Document Chunk 1160 ---
object detection, 121
overfitting, 17, 48
padding, 67, 74
parameter, 12
hyper, 13, 35, 48, 66, 67, 74, 93, 95
parametric model, see model, parametric
peak performance, 22
Perplexity, 151
perplexity, 31
policy, 134
optimal, 134
181
Chunk length: 230

--- Document Chunk 1161 ---
pooling, 74
positional encoding, 96, 111
Post-Training Quantization, 150
posterior probability, 26
pre-trained model, see model, pre-trained
prompt, 139, 140
engineering, 147
quantization, 150
Quantization-Aware Training, 152
query, 88
Chunk length: 235

--- Document Chunk 1162 ---
Quantization-Aware Training, 152
query, 88
RAG, see Retrieval-Augmented Generation
random initialization, 62
receptive field, 68, 69, 124
rectified linear unit, 71, 158
recurrent neural network, 158
regression, 18
Reinforcement Learning, 134, 141
Chunk length: 246

